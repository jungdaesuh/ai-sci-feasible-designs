/Users/suhjungdae/code/software/proxima_fusion/RL-feasible-designs/2511.08217v1.md
. Zhidkovskaya1, Anastasia Orlova1, Nina Gubina1,
Anastasia Vepreva1, Rodion Golovinskii1, Ilya Tonkii1, Ivan Dubrovsky1, Ivan Gurev1,
Dmitry Gilemkhanov1, Denis Chistiakov1, Timur A. Aliev1, Ivan Poddiakov2, Galina Zubkova2,
Ekaterina V. Skorb1, Vladimir Vinogradov1, Alexander Boukhanovsky1, Nikolay O. Nikitin1,
Andrei Dmitrenko †1,3, Anna V. Kalyuzhnaya ‡1, and Andrey Savchenko2,4
1ITMO University, Saint Petersburg, Russia
2Sber AI Lab, Moscow, Russia
3D ONE AG, Zurich, Switzerland
4HSE University, Moscow, Russia
Abstract
Hit identification is a central challenge in early
drug discovery, traditionally requiring substan-
tial experimental resources. Recent advances
in artificial intelligence, particularly large lan-
guage models (LLMs), have enabled virtual
screening methods that reduce costs and im-
prove efficiency. However, the growing com-
plexity of these tools has limited their accessi-
bility to wet-lab researchers. Multi-agent sys-
tems offer a promising solution by combining
the interpretability of LLMs with the precision
of specialized models and tools. In this work,
we present MADD, a multi-agent system that
builds and executes customized hit identifica-
tion pipelines from natural language queries.
MADD employs four coordinated agents to
handle key subtasks in de novo compound gen-
eration and screening. We evaluate MADD
across seven drug discovery cases and demon-
strate its superior performance compared to ex-
isting LLM-based solutions. Using MADD, we
pioneer the application of AI-first drug design
to five biological targets and release the identi-
fied hit molecules. Finally, we introduce a new
benchmark of query-molecule pairs and dock-
ing scores for over three million compounds to
contribute to the agentic future of drug design.
1
Introduction
Identifying hit molecules is at the core of drug dis-
covery (Ashraf et al., 2024). Hits are commonly
defined as chemical compounds with several de-
sired properties, such as biological activity against
a particular target associated with a disease, sta-
bility, synthetic accessibility, and others (Walters
and Namchuk, 2003). With significant advances
and widespread of artificial intelligence (AI) in the
*glebsolo46@gmail.com
†dmitrenko@pish.itmo.ru
‡anna.kalyuzhnaya@itmo.ru
last decade, virtual screening has become an essen-
tial part of drug discovery (Maya Ramírez et al.,
2024; Chikhale et al., 2024). AI methods for hit
identification aim to reduce the resources required
for high-throughput wet-lab experimentation and
improve their success rate. Significant progress has
been made in the adoption of such methods in the
pharmaceutical industry all around the world (Yang
et al., 2021; Zeng et al., 2022). However, the com-
plexity of AI methods has also grown dramatically,
resulting in limited accessibility of these tools for
wet-lab researchers to pursue their drug discovery
hypotheses (Bettanti et al., 2024).
LLMs have emerged as powerful tools for bridg-
ing this gap, offering impressive capabilities for
interpreting natural language descriptions of de-
sired molecular properties and generating candi-
date molecules (Guan and Wang, 2024; Vert, 2023).
Several promising LLM-based tools have been pro-
posed recently, including DrugLLM (Liu et al.,
2024b) for zero-shot molecular generation, Chem-
LLM (Zhang et al., 2024a) for chemistry-related
Q&A, LlasMol (Yu et al., 2024a) for molecular
property prediction, and X-LoRA-Gemma (Stew-
art and Buehler, 2024) for molecular optimiza-
tion. However, LLMs alone are not competitive
in molecule generation and molecular property pre-
diction specific to early drug discovery.
Combining an LLM with highly specialized
tools gave rise to agentic architectures. The first
examples, such as ChemAgent (Yu et al., 2024b),
ChemCrow (M. Bran et al., 2024), and CACTUS
(McNaughton et al., 2024), demonstrated a high
potential of this approach, but also highlighted the
challenges of integrating numerous AI-based tools
and automatic assembly of effective processing
pipelines. Finally, multi-agent systems have been
introduced (Skarlinski et al., 2024a; Odobesku
et al.; Skarlinski et al., 2025).
arXiv:2511.08217v1  [cs.AI]  11 Nov 2025


## Page 2

We were wondering whether a multi-agent sys-
tem could, in principle, be an effective end-to-end
solution for hit identification in early drug discov-
ery and overcome limitations of existing solutions
by providing comprehensive execution of all stages:
semantic query analysis, target-adaptive molecule
generation, property calculation (binding affinity,
synthetic accessibility), which is particularly cru-
cial for drug discovery tasks. More specifically, we
formulated the following research hypothesis:
Multi-agent architecture, by distributing com-
plex tasks across specialized agents with distinct
roles, can significantly outperform both single-
agent approaches and pure LLM generators in au-
tomating the complete drug discovery workflow
through improved task coordination and domain-
specific expertise (through domain pipelines and
tools) integration.
To pursue this hypothesis, we developed MADD,
a Multi-Agent Drug Discovery orchestra that em-
ploys four specialized agents to coordinate and
execute hit identification subtasks. To inform the
architecture design choices and validate the sys-
tem’s overall performance, we formulated six drug
discovery cases related to the following conditions:
Alzheimer’s disease, Parkinson’s disease, multi-
ple sclerosis, lung cancer, dyslipidemia, and drug
resistance. For each condition, we assembled a
unique dataset, trained machine learning models,
and implemented logic to validate the generated
molecules. We compared MADD to existing LLM-
based solutions and demonstrated its superior per-
formance. As additional validation case studies,
we compared the molecules produced by MADD
with experimentally validated GSK-3β inhibitors
and applied MADD to a recently published (and,
therefore, unseen) case of thrombocytopenia. Our
empirical results demonstrate that MADD is one
of the most effective multi-agent solutions for hit
identification in drug discovery to date.
In summary, the contributions of this paper are:
1. We present MADD, an effective end-to-end
agentic solution for hit identification, and
demonstrate its effectiveness in 7 drug dis-
covery cases, including the case of thrombo-
cytopenia, which was not available during the
system’s design.
2. We produce a novel benchmark for future
drug design agents, consisting of two synchro-
nized datasets: the first, with pairs of user
queries (inputs) and generated molecules (out-
puts) related to drug discovery, and the second,
with docking scores calculated for over 3M
molecules related to the 6 conditions consid-
ered in this work.
3. We pioneer the application of AI-first drug
design to the following biological targets:
STAT3, ABL, COMT, ACL, and PCSK9. As
a result, we identified several molecule hits
with favorable bioactivity, binding affinity,
and other physicochemical properties.
The results of our study are open source (code
and data):
https://github.com/ITMO-NSS-team/MADD
https://huggingface.co/datasets/
ITMO-NSS/MADD_Benchmark_and_results
2
Related work
Recent advances highlight the growing role of
LLMs in chemistry, including predictive model-
ing, compound generation, and drug discovery.
ChemDFM (Zhao et al., 2024), LlasMol (Yu et al.,
2024a), X-Lora-Gemma (Stewart and Buehler,
2024), and CancerGPT (Li et al., 2024b) exem-
plify applications in data analysis, molecular opti-
mization, and synergy prediction. Ye (Ye, 2024)
proposed a de novo molecule generation method,
and M. Bran et al. (M. Bran et al., 2024) showed
improved molecular property predictions by inte-
grating LLMs with chemical tools. Agent-based
LLM frameworks such as DrugAgent (Liu et al.,
2024a), ChemCrow (M. Bran et al., 2024), CAC-
TUS (McNaughton et al., 2024), and ChemAgent
(Yu et al., 2024b) automate various steps in chemi-
cal research, from synthesis planning to molecular
property analysis. Broader surveys (Zhang et al.,
2024b; Jablonka et al., 2023) confirm the efficiency
and scalability of these systems.
These studies highlight that LLMs can accelerate
scientific progress in chemistry and related fields,
unlocking new material and drug development op-
portunities. Nevertheless, none demonstrate suc-
cessful automation of the whole drug discovery
pipeline. Possible reasons are (1) the absence of a
readily accessible dataset with a complete pipeline
for training and validating new models and ap-
proaches, and (2) the weakness of existing sepa-
rated models and agent architectures for solving the
whole task at a high-quality level. Based on these
conclusions, we propose our vision of a stronger


## Page 3

approach and a valuable newly farmed dataset with
state-of-the-art pharmaceutical research tasks and
their solutions.
3
MADD orchestra
Summarizer
Orchestrator
Chat Agent
Decomposer
LLM Agents
User interface
Tools
chat-bot
dataset
answer
Generate GSK-3beta
inhibitors with high
activity
N
N
H
HO
N
Train
Train
GAN
Inference
Inference
Transformer
DScore
Initialize
Initialize
RDKit
Data
Processing
Properties
Prediction
Molecule
Generation
Actions:
Actions:
Actions:
Algs:
Algs:
Algs:
ML / AutoML
ChEMBL
BindingDB
Pandas
Download
Create DS
Figure 1: Overview of MADD architecture.
We set out to develop a system that harnesses
advanced drug design methodologies and enhances
their accessibility to domain experts by integrat-
ing LLM-based agents.
These agents can au-
tonomously operate molecular generation and
property prediction tools, enabling more efficient
and systematic hypothesis testing for identifying
promising drug candidates. This strategy aims to
significantly reduce research costs while allowing
human experts to focus on more complex and value-
creating aspects of the pharmaceutical development
process. To this end, we introduce the MADD ap-
proach, which integrates various generative and
predictive tools into a coherent, end-to-end drug
discovery pipeline. The system is built upon a
multi-agent architecture (Figure 1) specifically de-
signed to manage the complexity of automating
early drug discovery workflows.
The system’s architecture isolates key functions
in specialized agents, each optimized for specific
tasks in the overall workflow. We define four agents
and two LLM-tools to achieve high operational ac-
curacy and ground their roles in modular software
design principles, reducing the space of possible
actions for each agent. Specialization of agents has
at least two advantages:
• Mitigates the accumulation of errors in com-
plex pipelines by improving the accuracy of
individual steps.
• Provides more flexibility in constructing
multi-step pipelines, which is essential for a
scalable system.
3.1
Agents
Four specialized agents were developed to address
complex, multi-step queries that require coordina-
tion across various tools. By distributing cogni-
tive responsibilities among these agents, the sys-
tem solves intricate tasks efficiently, resolving user
queries in a single processing attempt.
The pipeline receives a text query from the user
and passes it to the Decomposer agent. In the
case of complex tasks, this agent decomposes the
original query into simpler subtasks. If the query is
ambiguous, a chat agent is called to clarify it and
enrich the context with additional data provided by
the user.
Each task from the Decomposer agent is sent to
the Orchestrator in sequence. The Orchestrator
builds an action plan and calls tools to achieve a
complete response. It interacts with our genera-
tive tools to generate molecules (using pre-trained
models or training from scratch) that match the de-
sired properties and uses the property prediction
tools to determine the properties of the molecules.
The Summarizer compiles the results into a coher-
ent response and presents the generated molecules
with their calculated properties in a structured for-
mat. The Chat Agent helps the user formulate a
query, request missing data, and consult on system
capabilities.
More details about the proposed agents are de-
scribed in Appendix E.1, E.2, E.4.1.
3.2
Integrated tools
Our research addresses the complex challenge of
real-world drug discovery, which demands a so-
phisticated integration of multiple specialized tools
beyond basic chemical analysis. To meet these
requirements, we developed a toolset combining
deep generative models for creating molecules, ML
models for property prediction, and methods for
evaluating synthetic accessibility, drug similarity,
and other structural properties of molecules.


## Page 4

As illustrated in Figure 1, the tools are organized
into three logical categories: Molecular Genera-
tion Algorithms, Property Prediction Algorithms,
and Data Processing. In automated learning exper-
iments, tools within these groups were controlled
via two specialized LLM-based tools, i.e. AutoML-
DL and DatasetBuilder, rather than directly by the
Orchestrator. These components directly call the
necessary specialized tools to execute the tasks
specified by the Orchestrator. A more detailed
account of these experiments can be found in the
Appendix C.1.
The DatasetBuilder tool prepares datasets for
model training. Its functionality includes remov-
ing irrelevant data and table columns, filtering data
based on specified conditions, and loading data con-
taining molecules and their binding affinity proper-
ties. It has access to the BindingDB and ChEMBL
databases. The AutoML-DL tool is capable of gen-
erating molecules, predicting molecular properties,
initiating training procedures, and monitoring the
training status of both predictive and generative
models.
Molecule generation algorithms. This toolset
includes pre-trained generative models and algo-
rithms for automatic fine-tuning for new disease
cases. Currently, MADD supports two generative
models: LSTM-based GAN and transformer-based
CVAE. These models were pre-trained on datasets
of ∼500k ChEMBL molecules for each disease
(Appendix C.4.4), for which we calculated Dock-
ing scores and other desired properties.
These generative approaches were selected by
their proven efficacy in the previous study (Gu-
bina et al., 2024). In addition, we treat GAN as
a fast, lightweight generator compared to a trans-
former, which is beneficial for real-time processing.
Both implementations are detailed in the Appendix
C.9.1.
Property prediction algorithms. This toolset
includes pre-trained machine learning models and
methods for predicting and calculating the required
disease properties. Furthermore, this toolkit fea-
tures an AutoML framework FEDOT (Nikitin et al.,
2022) designed to automatically train machine
learning pipelines using custom datasets to predict
molecular properties for new disease cases.
MADD can use the tool to predict the half-
maximal inhibitory concentration (IC50), which
is the concentration of a drug required to inhibit a
biological process or response by 50% and is the
most widely used and informative measure of a
drug’s efficacy. Appendices C.6, D.4 describe the
training and selection of the best model for each
case study in more detail.
One of the target properties we used as training
data was the binding energy of the target protein
to a ligand. This energy can be estimated using
molecular docking, typically called the docking
score. Details on docking score estimation are
presented in the Appendix D.4.2.
The Orchestrator can use several RDKit-based
functions:
synthetic accessibility (SA), drug-
likeness estimation (QED), and structural filters
such as Brenk, SurehEMBL, Glaxo, and PAINS.
Data Processing. Tools for data retrieval via
API from the ChEMBL and BindingDB databases
are presented. Data acquisition requires specifying
a target protein (or its respective database ID) and
an affinity measurement type (Ki, Kd, or IC50).
The retrieved data can be preprocessed before
training, including filtering by affinity value and
removing missing values.
4
Experimental studies
In this section, we present a series of experimental
results that informed the design of MADD archi-
tecture and proved the overall effectiveness of our
approach.
4.1
Benchmark creation
Existing chemical datasets possess significant lim-
itations to benchmarking LLM-based agents for
drug design. While Chembench (Mirza et al., 2024)
and PharmaBench (Niu et al., 2024) cover text-
based tasks and property prediction, respectively,
they lack the complexity of real-world screening
scenarios, which often involve multiple subtasks
such as identifying biological targets, generating
molecules, and evaluating their properties.
To fill this gap, we introduce a new benchmark
featuring linked datasets of user queries, target
molecular structures, and their properties, based on
the six drug discovery cases related to Alzheimer’s
and Parkinson’s diseases, multiple sclerosis, lung
cancer, dyslipidemia, and drug resistance (Fig-
ure 2). The datasets of user queries are of particular
importance for natural language processing, while
the other datasets are essential for evaluating hit
identification capabilities.
The datasets of user queries were created as fol-
lows. First, 30 realistic queries were manually
composed to reflect inputs from both experts and


## Page 5

Generated
SMILES
Molecule
properties
Queries
>5000 molecules
for 6 diseases
Molecular properties dataset (DS, IC50,
SA, QED) for >3M molecules
Dataset of >300
queries for 6 diseases
Example for Alzheimer:
Generate
GSK-3beta
inhibitors
with
high
docking score and low brain-blood barrier
permeability
Generated Molecules
IC50-based Activity
SA Score Distribution
QED Distribution
DS Distribution
Filters
Selected Molecules
Example for Dislipidemia:
Can
you
suggest
molecules
that
inhibit
Proprotein Convertase Subtilisin/Kexin Type 9
with enhanced bioavailability and the ability to
cross the BBB?
DS <= -7
Alzheimer
Dyslipidemia
Sclerosis
Cancer
Parkinson
Drug resistance
QED = 0.60
SA = 2.44
DS = -11.14
QED = 0.74
SA = 2.08
DS =-10.49
QED = 0.83
SA = 2.68
DS =-12.50
QED = 0.74
SA = 2.08
DS = -10.2
QED = 0.70
SA = 2.57
DS =-10.60
QED = 0.70
SA = 2.51
DS =-9.84
SureChEMBL = 0
IC50 = 1
Glaxo = 0
SA Score <= 3
Brenk = 0
PAINS = 0
QED > 0.6
QED
DS
Count
SA Score
User queries
0
50
50
100
100
150
150
200
200
0.00
0.25
0.50
0.75
1.00
User queries
100
1
4
7
0
0.0
-14
-12
-10
-8
-6
-4
-2
0
1.75
1.50
1.25
1.00
0.75
0.50
0.25
0.00
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
0.5
0.4
0.3
0.2
0.1
0.0
0.2
0.6
0.4
0.8
1.0
200
2
5
8
3
6
9
300
400
500
×103
Density
Density
Density
Alzheimer
Dyslipid.
Sclerosis
Cancer
Parkinson
Drug
resistance
Inactive
Active
Alzheimer
Drug Resist.
Dyslipid.
Cancer
Parkinson
Sclerosis
Figure 2: Overview of the proposed benchmark
non-experts. The latter included vague or imprecise
instructions mimicking real user behavior. Next,
the set of queries was augmented with few-shot
prompting of GPT-4o, o1-mini, Claude Sonnet 3.5,
and Gemini 1.5 Pro to increase semantic diversity.
Highly similar queries were filtered out using sen-
tence embeddings (Stankeviˇcius and Lukoševiˇcius,
2024; Devika et al., 2021). Finally, human experts
validated the outputs, resulting in a curated set of
high-quality queries. These queries were divided
into three datasets according to the complexity of
the underlying scenario: Dataset S (single-task
queries), Dataset M (1–3 tasks per query), and
Dataset L (4–5 tasks). More details, examples,
and visualizations are provided in Appendix C.4.
4.2
Comparison of underlying LLMs
Orchestration is a critical component of a multi-
agent system.
Therefore, we evaluated several
LLMs’ ability to play an orchestrator role. For
that, we defined the orchestrator accuracy (OA) as
the percentage of the correctly selected tools in the
query (see Appendix D.8), and used Dataset S to
compare the models.
Additionally, we included two prompting strate-
gies for an objective comparison: using a unified
system prompt that was identical for all the mod-
els and using optimized individual prompts, where
model-specific prompts were engineered to ensure
top performance, using function-specific keywords
and more precise instructions.
We found that Llama-3.1-70b with the opti-
mized system prompt was the best model, achiev-
ing an OA metric of 92.3%.
Notably, this
model outperformed o1-mini (67.5%), DeepSeek-
Table 1: Comparison of MADD with other candidate
architectures for multi-agent systems by TS, SSA, and
FA metrics.
Metric
TS
SSA
FA
MADD
83.7
95.3
79.8
MADD-v1
42.5
70.0
29.8
MADD-v2A
35.0
65.1
22.8
MADD-v2B
76.7
95.3
73.0
MADD-v2C
81.4
53.7
43.7
MADD-v3
46.5
75.6
35.2
R1 (88.11%), and even the next generation Llama-
3.2-90b (90.7%), while also being cheaper. Fig-
ure 9 in Appendix C.5 presents more experimental
results on orchestration accuracy. Based on this
empirical evidence, we selected Llama-3.1-70b as
the default LLM behind agents in MADD.
4.3
Ablation study
To motivate the design of the multi-agent archi-
tecture and demonstrate its ability to deliver sensi-
ble responses to user requests, MADD was com-
pared with five alternative versions, in which one
to several agents were removed. Those included:
MADD-v1 (single-agent with CoT-based reason-
ing), MADD-v2A (two-agent system with De-
composer and Orchestrator but no summariza-
tion), MADD-v2B (two-agent system where Or-
chestrator handles both tool selection and sum-
marization), MADD-v2C (unified Orchestrator-
Decomposer with a separate Summarizer), and
MADD-v3 (RAG-based three-agent system with-
out answer revision). More detailed descriptions
are in Appendix C.2, and schematics are in Ap-


## Page 6

pendix D.9
We used Dataset L and evaluated the following
metrics: tool selection (TS) accuracy, system re-
sponse summarization (SSA) accuracy, and final
system accuracy (FA), that accounts for both TS
and SSA metrics (see formulas in Appendix D.8.2).
The results are given in Table 1.
A key observation across all systems that did not
incorporate a dedicated Summarizer was the issue
of inconsistent response delivery. This challenge
manifested itself in the inability of the systems
to effectively consolidate information and provide
coherent, comprehensive answers to the user. Con-
sequently, this limitation significantly impacted the
final metric (FA) for most alternative versions. An-
other conclusion is that the more roles the Orches-
trator agent takes on, the worse it performs. This
observation underscores the need for a clear di-
vision of roles between agents to ensure optimal
performance and successful task completion. Over-
all, MADD achieves almost 80% in FA, evaluated
on the most complex scenarios (Dataset L).
Another conclusion from our ablation study is
that the more roles the Orchestrator agent takes
on, the worse it performs. This observation further
underscores the need for a clear division of roles
between agents to ensure optimal performance and
successful task completion. Overall, the current
implementation of MADD achieves almost 80%
in FA, evaluated on the most complex scenarios
among all tested (Dataset L).
4.4
Analysis of generative models
There are many strong generative models in drug
design that are capable of producing molecules in
SMILES notation. A comprehensive evaluation
of those is beyond the scope of this study. Never-
theless, we included a limited number of recently
published generative models for comparison with
MADD to showcase its generative capabilities, ex-
ecuted by the integrated GAN and Transformer
models. More specifically, we compared MADD
against an evolutionary optimization MTDD-EF
(Cerveira et al., 2024), a Monte Carlo tree search
ChatChemTSv2 (Ishida et al., 2024), as well as the
previously discussed X-LoRA-Gemma, LlaSMoL,
ChemDFM, and ChemAgent.
As a performance metric, we used the percent-
age of “hit” molecules—those meeting the desired
properties—out of the total number of generated
molecules. The set of desired properties is defined
by five filters corresponding to bioactivity, syn-
thetic accessibility, druglikeness, and other proper-
ties (see descriptions in Figure 3). Each successive
filtering criterion includes all the previous ones,
imposing progressively stricter property require-
ments. More details on filter groups are presented
in Appendix D.5.
The most stringent criterion corresponds to filter
group 5 (GR5). Table 8 indicates that the Trans-
former used in MADD achieved top performance in
three of six tested cases corresponding to different
conditions. On the other hand, GAN never deliv-
ered the top scores. Still, it was the second most
consistent generative model after Transformer, pro-
ducing a non-zero percent of hit molecules in five
of six tested cases. As follows from Tables 9 and
8, the other generative approaches occasionally
showed better results on individual cases, but the
overall performance of MADD remained superior.
More details are available in Appendices C.9 and
C.10.
4.5
Comparison with existing solutions
We probed several existing LLM-based solu-
tions on three query datasets from our bench-
mark and compared their performance with
MADD. We selected three state-of-the-art chem-
istry LLMs, namely, X-LoRA-Gemma, LlaSMoL,
and ChemDFM, as well as the recently proposed
ChemAgent for comparison. The results are sum-
marized in Table 2.
In contrast to agent-based systems, standalone
LLMs (LlasMol, ChemDFM, X-Lora-Gemma)
practically failed to produce structured answers
with valid chemical compounds in all datasets. This
is most likely due to insufficient domain knowledge
in the training set and missing access to specialized
molecular generation and screening tools.
In turn, ChemAgent suffers from the problems
associated with the limitations of generation tools,
predicting no more than 2.5% hit molecules pass-
ing the first group of filters (GR1), as shown in
Figure 3. Moreover, ChemAgent makes errors in
summarizing and structuring the output to the user
(19.1% of SSA), reducing the final accuracy of the
system to 16.4%, as indicated in Table 3. ChemA-
gent often fails to present key results (e.g., miss-
ing SMILES representations), likely due to inac-
curate agent prompting, highlighting orchestration
struggles in complex tasks. In contrast, the multi-
agent design of MADD allows for avoiding these
shortcomings and demonstrates at least 80% of the
final system accuracy on multi-task benchmark-


## Page 7

Table 2: Comparison of the Final Accuracy (%) of
MADD and baseline methods on datasets of different
complexity.
Dataset
S
M
L
MADD
86.9
84.3
79.8
ChemAgent
12.4
15.3
16.4
LlasMol
0.46
0.24
0
X-Lora-Gemma
0.44
0.12
0
ChemDFM
5.31
0.33
0
ing datasets. Examples of the resulting question-
answer pairs for different systems are presented in
the Appendix E.4.
MADD utilizes a comparable set of tools to
ChemAgent; however, as shown in Table 2, it out-
performs ChemAgent by up to 29.1% in tool se-
lection accuracy when applied to the Dataset S
and Dataset M. However, both approaches demon-
strate similar accuracy in tool selection evaluated
on Dataset L.
In addition, we briefly compared two fresh so-
lutions for chemical problems: TxGemma (Wang
et al., 2025) and Phoenix from the FutureHouse
platform. We found that Phoenix managed to create
a single molecule that satisfied the GR2 filter, but
no more. The molecules produced by TxGemma
did not pass any filters. Detailed results for these
methods are proposed in Appendices C.3.1-C.3.2.
4.6
End-to-end hit identification with MADD
Figure 3 shows that the end-to-end generation pro-
cess, from query to molecule search, demonstrates
that MADD outperforms LLM-based approaches in
all filter groups and mean values of Docking score.
The other models can generate valid SMILES but
are not unambiguously able to construct a rela-
tionship between properties and molecules. More
detailed discussions of the novelty of the created
molecules are described in Appendix C.11 (Table 9
compares all systems for all seven diseases).
5
AutoML features of MADD
We evaluated the effectiveness of MADD’s auto-
matic training generative model by comparing it to
Table 3: Comparison of MADD with ChemAgent on L
dataset
Metric
TS
SSA
FA
MADD
83.7
95.3
79.8
ChemAgent
85.8
19.1
16.4
Comparison of DD approaches
Figure 3: Comparison of drug discovery approaches
using average metrics across considered diseases. The
filtration groups (GR) are based on molecule properties
such as Docking score and IC50 (GR1), SA (GR2),
BRENK (GR3), and the latter groups additionally in-
clude SurehEMBL, Glaxo, PAINS (GR4), and QED
(GR5).
Figure 4: Comparison of F1 score for the MADD auto-
matically created and trained ML pipeline and manually
pre-trained models for IC50 predictions.
manual training on Alzheimer’s disease. Predictive
performance was assessed across all six diseases.
Predictive models were used in the automatic learn-
ing process of generative models to stop learning
early and select the best learning epoch, consider-
ing the value of the loss function and the error in
matching the required properties in the generated
molecules.
As shown in Figure 4, by automatically running
the AutoML framework, MADD can train machine
learning pipelines for molecular property predic-
tion tasks that outperform pre-trained manual mod-
els in most cases.
The efficiency of generative models trained au-
tomatically by MADD is comparable in terms of
the number of target molecules generated and the
average docking score on the Alzheimer’s disease
example. As a result, the study significantly out-
performed the average Docking Score in terms of
the percentage of target molecules generated. The
results of training the generative models are shown
in Table 4.


## Page 8

Additionally, we evaluated whether the tool se-
lection (TS) metric is retained when adding a tool
for running generative or predictive model training.
As a result of this evaluation, tool selection accu-
racy decreased slightly from 83.7% (Table 1) to
80.5% on average. For a more detailed description
of this study, see Appendix C.8.
6
Case studies
Alzheimer’s disease case study. We conducted
a case study on Alzheimer’s disease to compare
molecules generated by MADD with experimen-
tally validated GSK-3β inhibitors from ChEMBL
(Figure 5A). The MADD-generated molecules ex-
hibited higher mean SA and QED scores com-
pared to the ChEMBL compounds. In addition,
the MADD molecules showed improved binding
affinities to the target protein. Moreover, generated
compounds demonstrated high structural diversity,
with an average Tanimoto similarity of 0.43. This
suggests that MADD can be an effective tool for
de novo drug design, potentially producing candi-
dates that surpass existing ligands in terms of key
drug-relevant properties.
Alzheimer’s disease
ChEMBL
ChEMBL
ChEMBL
SYK-FBRL
SYK-FBRL
SYK-FBRL
Thrombocytopenia
a
b
Figure 5: Results of case studies: a). Alzheimer’s
disease case study; “ChEMBL” - experimentally vali-
dated GSK-3β inhibitors from ChEMBL, “MADD” -
molecules generated by our approach. b). Thrombocy-
topenia case study; “SYK-FBRL” - molecules generated
by SYK-FBRL approach, “MADD” - molecules gener-
ated by our approach.
Thrombocytopenia case study. This study in-
volved validation using one of the recently ex-
plored drug design cases on thrombocytopenia
(Zavadskaya et al., 2025), where fragment-based
RL was employed for the generation of novel
SYK-inhibitors (hereafter, SYK-FBRL). MADD
was provided the dataset containing ∼3.2k SYK-
inhibitors to train new generative and predictive
models for this case automatically.
Using the AutoML framework, MADD could de-
rive a pipeline to predict pIC50, achieving a metric
value of R2=0.75, compared to 0.78 in SYK-FBRL.
More details about the resulting machine learning
pipelines are described in the Appendix C.7
Besides the predictive models for pIC50 and
docking score, MADD managed to fine-tune the
Transformer model. Ultimately, MADD generated
10k molecules, of which 132 matched all five of
our property filters. In contrast, the original SYK-
FBRL approach generated 76k molecules, of which
139 corresponded to the required pIC50 and DS
properties. This outcome highlights the superior
efficiency of MADD in hit identification. Table 4
summarizes the results of comparing the MADD
and SYK-FBRL approaches for thrombocytopenia.
Additionally, Figure 5b compares MADD-
generated molecules with compounds produced
by SYK-FBRL. The fully automatic pipeline of
MADD resulted in molecules with improved dock-
ing scores, QED, and SA properties. Notably, these
results were obtained without any adjustments to
MADD configuration and with no additional inputs
from human experts (e.g., medicinal chemists) (Ku-
michev et al., 2024). This outcome is the most
substantial evidence in support of the hypothesis
formulated in this work. It also highlights the excel-
lent potential for MADD to streamline and enhance
early-stage drug discovery.
7
Discussion
While MADD demonstrates promising general-
ization capabilities in the thrombocytopenia case,
which was not part of the original design, several
limitations regarding its applicability to new cases
should be further explored. In its current imple-
mentation, users are required to provide their own
dataset of molecules and corresponding target prop-
erties to train MADD’s generative and predictive
tools. In practice, many users may lack access to
well-curated datasets, impacting both model per-
formance and reproducibility. Moreover, MADD’s
pipeline assumes prior knowledge of the biological
target or relevant properties, which limits the dis-
covery of ligands for fundamentally novel targets.


## Page 9

Table 4: Comparison of an automatically trained MADD
model’s ability to create hit molecules, compared to pre-
trained models for Alzheimer’s disease and Thrombo-
cytopenia. The Novelty was compared to the dataset of
compounds reported in the SYK-FBRL study for throm-
bocytopenia.
Case
Alzheimer
Thrombocytopenia
Model
MADD
MADD
Auto
SYK-FBRL
MADD
Auto
Mean DS
-7.46
-7.57
-7.76
-8.02
Novelty
78.21
73.47
100
100
Validity
87.47
89.5
100
90.71
GR1,%
20.30
15.99
0.18
1.54
GR2,%
17.56
14.43
0.07
1.35
GR3,%
13.72
13.14
0.06
1.32
GR4, %
13.40
12.34
0.06
1.32
GR5,%
13.40
12.34
0.06
1.32
To address these issues, future work should focus
on integrating automated data curation pipelines
and incorporating modules for hypothesis genera-
tion to enable more open-ended discovery.
Additionally, the system’s current generalization
is predominantly demonstrated in silico without
broader experimental validation. We tried to en-
sure in silico validation of generated hits. How-
ever, translating these predictions into real-world
outcomes requires experimental validation through
biochemical assays, cell-based screens, and ani-
mal studies, which are inherently time-consuming,
costly, and resource-intensive processes. This im-
balance creates a bottleneck: while AI systems can
explore chemical space at unprecedented scale and
speed, only a small fraction of candidates can real-
istically be tested in the lab. Nevertheless, future ef-
forts will focus on extending this work through ex-
perimental collaborations to confirm the predicted
activities and refine the model based on empiri-
cal results. Integrating wet lab validation into the
MADD pipeline would strengthen the biological
relevance of its outputs and enable constant itera-
tive improvement, further bridging the gap between
AI-driven design and practical drug development.
8
Conclusion
This paper introduces MADD, a multi-agent
system tackling the challenge of hit identifica-
tion. MADD coordinates multiple tools to design
molecules and assess their properties, achieving
79.8% overall pipeline accuracy. MADD consis-
tently outperforms baselines in hit identification
efficiency, yielding up to 6.63% novel hits that
meet eight key drug criteria. Validation on two
case studies, including an unseen thrombocytope-
nia case, further confirms its autonomous discovery
capabilities. We contribute to the future of drug
design agents by presenting a novel benchmark
and discussing current generalization and wet lab
experimentation challenges.
Acknowledgments
This work supported by the Ministry of Economic
Development of the Russian Federation (IGK
000000C313925P4C0002), agreement No139-15-
2025-010.


## Page 10

Limitations
Beyond the limitations discussed earlier, we outline
several other considerations that may impact the
use of MADD:
• Training Time for New Disease Cases
When adapting the system to a new disease,
significant time is required for data prepara-
tion, generative model training, and predictive
model fine-tuning. While we estimate this pro-
cess should not exceed one day, a reasonable
timeframe for hit identification, it remains a
practical constraint for users seeking rapid de-
ployment.
• Interpretability of the Black-Box Pipeline
MADD operates as a closed system (black
box) in chat mode, handling queries with-
out exposing its internal workflows. While
expert chemists may prioritize results over
methodological transparency, the inclusion of
RDKit-based tools allows users to evaluate
them partially. Future iterations could incor-
porate agent decision logging to enhance in-
terpretability, explicitly detailing the tools and
actions selected during execution.
• Tool Integration Barriers
Extending MADD’s functionality with new
tools requires direct system code modification.
Despite its open-source nature, users with lim-
ited programming expertise may face chal-
lenges integrating external tools, even those
that could be seamlessly incorporated.
• Dependence on User-Defined Filtering
Effective hit identification relies on user-
specified thresholds to determine disease-
relevant molecules. This demands domain
expertise to define appropriate molecular char-
acteristics, which may limit accessibility for
non-specialists.
Additionally, we discussed the risks of our study in
the Appendix A.1.
We have further analyzed potential failure modes
of our and similar LLM-based agent systems in the
Appendix D.10.
References
Dony Ang, Cyril Rakovski, and Hagop S Atamian. 2024.
De novo drug design using transformer-based ma-
chine translation and reinforcement learning of an
adaptive monte carlo tree search. Pharmaceuticals,
17(2):161.
S Neha Ashraf, J Henry Blackwell, Geoffrey A
Holdgate, Simon CC Lucas, Alisa Solovyeva, R Ian
Storer, and Benjamin C Whitehurst. 2024. Hit me
with your best shot: Integrated hit discovery for the
next generation of drug targets. Drug Discovery To-
day, page 104143.
Dávid Bajusz, Anita Rácz, and Károly Héberger. 2015.
Why is tanimoto index an appropriate choice for
fingerprint-based similarity calculations? Journal of
cheminformatics, 7:1–13.
Christie M Ballantyne, James M McKenney, Diane E
MacDougall, Janice R Margulies, Paula L Robinson,
Jeffrey C Hanselman, and Narendra D Lalwani. 2016.
Effect of etc-1002 on serum low-density lipoprotein
cholesterol in hypercholesterolemic patients receiv-
ing statin therapy. The American journal of cardiol-
ogy, 117(12):1928–1933.
Jenny D Beebe, Jing-Yuan Liu, and Jian-Ting Zhang.
2018. Two decades of research in discovery of anti-
cancer drugs targeting stat3, how close are we? Phar-
macology & therapeutics, 191:74–91.
Theo A Berkhout, Louis M Havekes, NJ Pearce, and
PHE Groot. 1990. The effect of (-)-hydroxycitrate on
the activity of the low-density-lipoprotein receptor
and 3-hydroxy-3-methylglutaryl-coa reductase levels
in the human hepatoma cell line hep g2. Biochemical
journal, 272(1):181–186.
Alberto Bettanti, Andrea Rosario Beccari, and Marco
Biccarino. 2024. Exploring the future of biopharma-
ceutical drug discovery: can advanced ai platforms
overcome current challenges?
Discover Artificial
Intelligence, 4(1):1–16.
Arun Singh Bhadwal, Kamal Kumar, and Neeraj Ku-
mar. 2023. Gmg-ncdvae: guided de novo molecule
generation using nlp techniques and constrained di-
verse variational autoencoder. ACM Transactions
on Asian and Low-Resource Language Information
Processing.
Ozlem Bilen and Christie M Ballantyne. 2016. Bempe-
doic acid (etc-1002): an investigational inhibitor of
atp citrate lyase. Current atherosclerosis reports,
18:1–7.
Andres M Bran, Sam Cox, Oliver Schilter, Carlo Baldas-
sari, Andrew D White, and Philippe Schwaller. 2023.
Chemcrow: Augmenting large-language models with
chemistry tools. arXiv preprint arXiv:2304.05376.
Leo Breiman. 2001. Random forests. Machine learning,
45:5–32.


## Page 11

Katharina Buerger, Michael Ewers, Tuula Pirttilä, Ray-
mond Zinkowski, Irina Alafuzoff, Stefan J Teipel,
John DeBernardis, Daniel Kerkman, Cheryl McCul-
loch, Hilkka Soininen, and 1 others. 2006. Csf phos-
phorylated tau protein correlates with neocortical neu-
rofibrillary pathology in alzheimer’s disease. Brain,
129(11):3035–3041.
Maria T Cencioni,
Miriam Mattoscio,
Roberta
Magliozzi, Amit Bar-Or, and Paolo A Muraro. 2021.
B cells in multiple sclerosis—from targeted depletion
to immune reconstitution therapies. Nature Reviews
Neurology, 17(7):399–414.
Arthur Cerveira, Frederico Kremer, Darling Lourenço,
and Ulisses B Corrêa. 2024.
Evaluation frame-
work for ai-driven molecular design of multi-target
drugs: Brain diseases as a case study.
In 2024
IEEE Congress on Evolutionary Computation (CEC),
pages 1–8. IEEE.
Tianqi Chen and Carlos Guestrin. 2016. Xgboost: A
scalable tree boosting system. In Proceedings of
the 22nd acm sigkdd international conference on
knowledge discovery and data mining, pages 785–
794.
Rupesh V Chikhale, Rinku Choudhary, Jagriti Mal-
hotra, Gaber E Eldesoky, Parth Mangal, and Pri-
tee Chunarkar Patil. 2024. Identification of novel
hit molecules targeting m. tuberculosis polyketide
synthase 13 by combining generative ai and physics-
based methods. Computers in Biology and Medicine,
176:108573.
Jonathan C Cohen, Eric Boerwinkle, Thomas H
Mosley Jr, and Helen H Hobbs. 2006. Sequence
variations in pcsk9, low ldl, and protection against
coronary heart disease. New England Journal of
Medicine, 354(12):1264–1272.
R Devika, Subramaniyaswamy Vairavasundaram, C Sak-
thi Jay Mahenthar, Vijayakumar Varadarajan, and Ke-
tan Kotecha. 2021. A deep learning model based on
bert and sentence transformer for semantic keyphrase
extraction on big social data. IEEE Access, 9:165252–
165261.
Orion Dollar, Nisarg Joshi, David AC Beck, and Jim
Pfaendtner. 2021. Attention-based generative models
for de novo molecular design. Chemical Science,
12(24):8362–8372.
Juan Manuel Domínguez, Ana Fuertes, Leyre Orozco,
María del Monte-Millán, Elena Delgado, and Miguel
Medina. 2012. Evidence for irreversible inhibition of
glycogen synthase kinase-3β by tideglusib. Journal
of Biological Chemistry, 287(2):893–904.
Jerome Eberhardt, Diogo Santos-Martins, Andreas F
Tillack, and Stefano Forli. 2021. Autodock vina 1.2.
0: New docking methods, expanded force field, and
python bindings. Journal of chemical information
and modeling, 61(8):3891–3898.
Veronika Ganeeva, Kuzma Khrabrov, Artur Kadurin,
Andrey Savchenko, and Elena Tutubalina. 2024a.
Chemical language models have problems with chem-
istry: A case study on molecule captioning task. In
The Second Tiny Papers Track at ICLR 2024.
Veronika
Ganeeva,
Andrey
Sakhovskiy,
Kuzma
Khrabrov, Andrey Savchenko, Artur Kadurin, and
Elena Tutubalina. 2024b. Lost in translation: Chem-
ical language models and the misunderstanding of
molecule structures. In Findings of the Association
for Computational Linguistics: EMNLP 2024, pages
12994–13013.
Pierre Geurts, Damien Ernst, and Louis Wehenkel. 2006.
Extremely randomized trees.
Machine learning,
63:3–42.
Rafael Gómez-Bombarelli, Jennifer N Wei, David Du-
venaud, José Miguel Hernández-Lobato, Benjamín
Sánchez-Lengeling, Dennis Sheberla, Jorge Aguilera-
Iparraguirre, Timothy D Hirzel, Ryan P Adams, and
Alán Aspuru-Guzik. 2018. Automatic chemical de-
sign using a data-driven continuous representation of
molecules. ACS central science, 4(2):268–276.
Shenghui Guan and Guanyu Wang. 2024. Drug dis-
covery and development in the era of artificial intelli-
gence: From machine learning to large language mod-
els. Artificial Intelligence Chemistry, 2(1):100070.
Nina Gubina, Andrei Dmitrenko, Gleb Solovev, Lyubov
Yamshchikova, Oleg Petrov, Ivan Lebedev, Nikita
Serov, Grigorii Kirgizov, Nikolay Nikitin, and
Vladimir Vinogradov. 2024. Hybrid generative ai
for de novo design of co-crystals with enhanced
tabletability. Advances in Neural Information Pro-
cessing Systems, 37:84606–84644.
Gabriel Lima Guimaraes, Benjamin Sanchez-Lengeling,
Carlos Outeiral, Pedro Luis Cunha Farias, and Alán
Aspuru-Guzik. 2017. Objective-reinforced genera-
tive adversarial networks (organ) for sequence gener-
ation models. arXiv preprint arXiv:1705.10843.
Suhail Haroon, CA Hafsath, and AS Jereesh. 2023. Gen-
erative pre-trained transformer (gpt) based model
with relative attention for de novo drug design. Com-
putational Biology and Chemistry, 106:107911.
Yoshitaka Inoue, Tianci Song, and Tianfan Fu. 2024.
Drugagent:
Explainable drug repurposing agent
with large language model-based reasoning. arXiv
preprint arXiv:2408.13378.
Zohaib Iqbal, Shaishav Dhage, Jamal Basheer Mo-
hamad, Alaa Abdel-Razik, Rachelle Donn, Rayaz
Malik, Jan Hoong Ho, Yifen Liu, Safwaan Adam,
Basil Isa, and 1 others. 2019. Efficacy and safety
of pcsk9 monoclonal antibodies. Expert opinion on
drug safety, 18(12):1191–1201.
Tamara Isermann, Christine Sers, Channing J Der, and
Bjoern Papke. 2024.
Kras inhibitors: resistance
drivers and combinatorial strategies. Trends in can-
cer.


## Page 12

Shoichi Ishida, Tomohiro Sato, Teruki Honma, and Kei
Terayama. 2024. Large language models open new
way of ai-assisted molecule design for chemists.
Kevin Maik Jablonka, Qianxiang Ai, Alexander Al-
Feghali, Shruti Badhwar, Joshua D Bocarsly, An-
dres M Bran, Stefan Bringuier, L Catherine Brinson,
Kamal Choudhary, Defne Circi, and 1 others. 2023.
14 examples of how llms can transform materials sci-
ence and chemistry: a reflection on a large language
model hackathon.
Digital Discovery, 2(5):1233–
1250.
Jan H Jensen. 2019. A graph-based genetic algorithm
and generative model/monte carlo tree search for the
exploration of chemical space. Chemical science,
10(12):3567–3572.
Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang,
Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu.
2017. Lightgbm: A highly efficient gradient boost-
ing decision tree. Advances in neural information
processing systems, 30.
Alan Kerstjens and Hans De Winter. 2022.
Leadd:
Lamarckian evolutionary algorithm for de novo drug
design. Journal of Cheminformatics, 14(1):3.
Dongsung Kim, Lorenz Herdeis, Dorothea Rudolph,
Yulei Zhao, Jark Böttcher, Alberto Vides, Carlos I
Ayala-Santos, Yasin Pourfarjam, Antonio Cuevas-
Navarro, Jenny Y Xue, and 1 others. 2023. Pan-kras
inhibitor disables oncogenic signalling and tumour
growth. Nature, 619(7968):160–166.
Hyunseung Kim, Jonggeol Na, and Won Bo Lee. 2021.
Generative chemical transformer: neural machine
learning of molecular geometric structures from
chemical language via attention. Journal of chemical
information and modeling, 61(12):5804–5814.
Julia Krämer, Amit Bar-Or, Timothy J Turner, and Heinz
Wiendl. 2023.
Bruton tyrosine kinase inhibitors
for multiple sclerosis. Nature Reviews Neurology,
19(5):289–304.
Yanbin Kuang, Wenzheng Guo, Jing Ling, Dongliang
Xu, Yueling Liao, Hui Zhao, Xiaohui Du, Han Wang,
Mingxin Xu, Hongyong Song, and 1 others. 2019.
Iron-dependent cdk1 activity promotes lung carcino-
genesis via activation of the gp130/stat3 signaling
pathway. Cell Death & Disease, 10(4):297.
Gleb Kumichev, Pavel Blinov, Yulia Kuzkina, Vasily
Goncharov, Galina Zubkova, Nikolai Zenovkin,
Aleksei Goncharov, and Andrey Savchenko. 2024.
MedSyn: LLM-based synthetic medical text gen-
eration framework. In Joint European Conference
on Machine Learning and Knowledge Discovery in
Databases, pages 215–230. Springer.
Matt J Kusner, Brooks Paige, and José Miguel
Hernández-Lobato. 2017. Grammar variational au-
toencoder. In International conference on machine
learning, pages 1945–1954. PMLR.
Seung-Hwan Kwon, Sangjune Kim, A Yeong Park, Sae-
bom Lee, Changdev Gorakshnath Gadhe, Bo Am
Seo, Jong-Sung Park, Suyeon Jo, Yumin Oh, Sin Ho
Kweon, and 1 others. 2021. A novel, selective c-abl
inhibitor, compound 5, prevents neurodegeneration in
parkinson’s disease. Journal of medicinal chemistry,
64(20):15091–15110.
Myeonghun Lee and Kyoungmin Min. 2022. Mgcvae:
multi-objective inverse design via molecular graph
conditional variational autoencoder. Journal of chem-
ical information and modeling, 62(12):2943–2950.
Jules Leguy, Thomas Cauchy, Marta Glavatskikh, Béa-
trice Duval, and Benoit Da Mota. 2020. Evomol: a
flexible and interpretable evolutionary algorithm for
unbiased de novo molecular generation. Journal of
cheminformatics, 12:1–19.
Guo Li, Jiaxuan Li, Yujia Tian, Yunyang Zhao,
Xiaoyang Pang, and Aixia Yan. 2024a.
Ma-
chine learning-based classification models for non-
covalent bruton’s tyrosine kinase inhibitors: Predic-
tive ability and interpretability. Molecular Diversity,
28(4):2429–2447.
Rui Li, Hao Tang, Jeremy C Burns, Brian T Hopkins,
Carole Le Coz, Bo Zhang, Isabella Peixoto de Barce-
los, Neil Romberg, Amy C Goldstein, Brenda L Ban-
well, and 1 others. 2022. Btk inhibition limits b-
cell–t-cell interaction through modulation of b-cell
metabolism: implications for multiple sclerosis ther-
apy. Acta neuropathologica, 143(4):505–521.
Tianhao Li, Sandesh Shetty, Advaith Kamath, Ajay
Jaiswal, Xiaoqian Jiang, Ying Ding, and Yejin Kim.
2024b. Cancergpt for few shot drug pair synergy
prediction using large pretrained language models.
NPJ Digital Medicine, 7(1):40.
Christopher A Lipinski. 2016. Rule of five in 2015
and beyond: Target and ligand structural limitations,
ligand chemistry structure and drug discovery project
decisions. Advanced drug delivery reviews, 101:34–
41.
Sizhe Liu, Yizhou Lu, Siyu Chen, Xiyang Hu, Jieyu
Zhao, Tianfan Fu, and Yue Zhao. 2024a. Drugagent:
Automating ai-aided drug discovery programming
through llm multi-agent collaboration. arXiv preprint
arXiv:2411.15692.
Xianggen Liu,
Yan Guo,
Haoran Li,
Jin Liu,
Shudong Huang, Bowen Ke, and Jiancheng Lv.
2024b.
DrugLLM: Open large language model
for few-shot molecule generation. arXiv preprint
arXiv:2405.06690.
Andres M. Bran, Sam Cox, Oliver Schilter, Carlo Bal-
dassari, Andrew D White, and Philippe Schwaller.
2024. Augmenting large language models with chem-
istry tools. Nature Machine Intelligence, pages 1–11.
Bruno
Macedo,
Inês
Ribeiro
Vaz,
and
Tiago
Taveira Gomes. 2024. Medgan: optimized gener-
ative adversarial network with graph convolutional


## Page 13

networks for novel molecule design. Scientific Re-
ports, 14(1):1212.
Jiashun Mao, Jianmin Wang, Amir Zeb, Kwang-Hwi
Cho, Haiyan Jin, Jongwan Kim, Onju Lee, Yunyun
Wang, and Kyoung Tai No. 2023. Transformer-based
molecular generative model for antiviral drug de-
sign. Journal of chemical information and modeling,
64(7):2733–2745.
Carlos Eliel Maya Ramírez, Zeeshan Shokat, Muham-
mad Sufyan, Md Tabish Rehman, Mohamed F AlA-
jmi, and Gulam M Rather. 2024. Identification of
novel scaffolds targeting sirt3 through molecular
modeling techniques for the treatment of hepatocel-
lular carcinoma. Journal of Biomolecular Structure
and Dynamics, 42(19):10165–10174.
Marisa P McGinley, Carolyn H Goldschmidt, and
Alexander D Rae-Grant. 2021.
Diagnosis and
treatment of multiple sclerosis: a review.
Jama,
325(8):765–779.
Andrew D McNaughton, Gautham Krishna Sankar Ra-
malaxmi, Agustin Kruel, Carter R Knutson, Rohith A
Varikoti, and Neeraj Kumar. 2024. Cactus: Chem-
istry agent connecting tool usage to science. ACS
Omega.
Adrian Mirza, Nawaf Alampara, Sreekanth Kunchapu,
Martiño Ríos-García, Benedict Emoekabu, Aswanth
Krishnan, Tanya Gupta, Mara Schilling-Wilhelmi,
Macjonathan Okereke, Anagha Aneesh, and 1 oth-
ers. 2024. Are large language models superhuman
chemists? arXiv preprint arXiv:2404.01475.
Nikolay O Nikitin, Pavel Vychuzhanin, Mikhail
Sarafanov, Iana S Polonskaia, Ilia Revin, Irina V
Barabanova, Gleb Maximov, Anna V Kalyuzhnaya,
and Alexander Boukhanovsky. 2022.
Automated
evolutionary approach for the design of composite
machine learning pipelines. Future Generation Com-
puter Systems, 127:109–125.
Zhangming Niu, Xianglu Xiao, Wenfan Wu, Qiwei Cai,
Yinghui Jiang, Wangzhen Jin, Minhao Wang, Guojian
Yang, Lingkang Kong, Xurui Jin, and 1 others. 2024.
Pharmabench: Enhancing admet benchmarks with
large language models. Scientific Data, 11(1):985.
Toshiki Ochiai, Tensei Inukai, Manato Akiyama, Kairi
Furui, Masahito Ohue, Nobuaki Matsumori, Shin-
suke Inuki, Motonari Uesugi, Toshiaki Sunazuka,
Kazuya Kikuchi, and 1 others. 2023. Variational
autoencoder-based chemical latent space for large
molecular structures with 3d complexity. Communi-
cations Chemistry, 6(1):249.
Roman Odobesku, Karina Romanova, Sabina Mirzaeva,
Oleg Zagorulko, Roman Sim, Rustem Khakimullin,
Julia Razlivina, Andrei Dmitrenko, and Vladimir
Vinogradov. nanominer: Multimodal information
extraction for nanomaterials. In AI for Accelerated
Materials Design-ICLR 2025.
Openai OpenAI. 2022. Openai: Introducing chatgpt.
URL https://openai. com/blog/chatgpt.
Chao Pang, Jianbo Qiao, Xiangxiang Zeng, Quan Zou,
and Leyi Wei. 2023. Deep generative models in de
novo drug molecule generation. Journal of Chemical
Information and Modeling, 64(7):2174–2194.
Nathaniel H Park, Tiffany J Callahan, James L Hedrick,
Tim Erdmann, and Sara Capponi. 2024. Leveraging
chemistry foundation models to facilitate structure fo-
cused retrieval augmented generation in multi-agent
workflows for catalyst and materials design. arXiv
preprint arXiv:2408.11793.
Liudmila Prokhorenkova, Gleb Gusev, Aleksandr
Vorobev, Anna Veronika Dorogush, and Andrey
Gulin. 2018. Catboost: unbiased boosting with cat-
egorical features. Advances in neural information
processing systems, 31.
Oleksii Prykhodko, Simon Viet Johansson, Panagiotis-
Christos Kotsias, Josep Arús-Pous, Esben Jannik
Bjerrum, Ola Engkvist, and Hongming Chen. 2019.
A de novo molecular generation method using latent
vector based generative adversarial network. Journal
of Cheminformatics, 11:1–13.
Evgeny Putin, Arip Asadulaev, Yan Ivanenkov, Vladimir
Aladinskiy, Benjamin Sanchez-Lengeling, Alán
Aspuru-Guzik, and Alex Zhavoronkov. 2018. Re-
inforced adversarial neural computer for de novo
molecular design. Journal of chemical information
and modeling, 58(6):1194–1204.
M Reck, DP Carbone, M Garassino, and F Barlesi. 2021.
Targeting kras in non-small-cell lung cancer: recent
progress and new approaches. Annals of Oncology,
32(9):1101–1110.
Darius Saberi, Anastasia Geladaris, Sarah Dybowski,
and Martin S Weber. 2023. Bruton’s tyrosine ki-
nase as a promising therapeutic target for multiple
sclerosis. Expert Opinion on Therapeutic Targets,
27(4-5):347–359.
Yu Shi, Weina Gao, Nikki K Lytle, Peiwu Huang,
Xiao Yuan, Amanda M Dann, Maya Ridinger-Saison,
Kathleen E DelGiorno, Corina E Antal, Gaoyang
Liang, and 1 others. 2019. Targeting lif-mediated
paracrine interaction for pancreatic cancer therapy
and monitoring. Nature, 569(7754):131–135.
Michael Skarlinski, Nadolski Tyler, Braza James,
Storni Remo, Caldas Mayk, Mitchener Ludovico,
Hinks Michaela, White Andrew, and Rodriques
Sam. 2025. FutureHouse Platform: Superintelligent
AI Agents for Scientific Discovery. https://www.
futurehouse.org/research-announcements/
launching-futurehouse-platform-ai-agents/.
[Online; accessed 18-May-2025].
Michael D Skarlinski, Sam Cox, Jon M Laurent,
James D Braza, Michaela Hinks, Michael J Hammer-
ling, Manvitha Ponnapati, Samuel G Rodriques, and
Andrew D White. 2024a. Language agents achieve


## Page 14

superhuman synthesis of scientific knowledge. arXiv
preprint arXiv:2409.13740.
Michael D. Skarlinski, Sam Cox, Jon M. Laurent,
James D. Braza, Michaela Hinks, Michael J. Ham-
merling, Manvitha Ponnapati, Samuel G. Rodriques,
and Andrew D. White. 2024b.
Language agents
achieve superhuman synthesis of scientific knowl-
edge. Preprint, arXiv:2409.13740.
Lukas Stankeviˇcius and Mantas Lukoševiˇcius. 2024. Ex-
tracting sentence embeddings from pretrained trans-
former models. Applied Sciences, 14(19):8887.
Isabella Stewart and Markus J Buehler. 2024. Molec-
ular analysis and design using generative artificial
intelligence via multi-agent modeling. Molecular
Systems Design & Engineering.
Neil J Stone, Jennifer G Robinson, Alice H Lichten-
stein, C Noel Bairey Merz, Conrad B Blum, Robert H
Eckel, Anne C Goldberg, David Gordon, Daniel Levy,
Donald M Lloyd-Jones, and 1 others. 2014. 2013
acc/aha guideline on the treatment of blood choles-
terol to reduce atherosclerotic cardiovascular risk in
adults: a report of the american college of cardiolo-
gy/american heart association task force on practice
guidelines. Circulation, 129(25_suppl_2):S1–S45.
Naveen Suresh, Neelesh Chinnakonda Ashok Kumar,
Srikumar Subramanian, and Gowri Srinivasa. 2022.
Memory augmented recurrent neural networks for
de-novo drug design. Plos one, 17(6):e0269461.
Shidi Tang, Ji Ding, Xiangyu Zhu, Zheng Wang, Haitao
Zhao, and Jiansheng Wu. 2024. Vina-gpu 2.1: to-
wards further optimizing docking speed and preci-
sion of autodock vina and its derivatives. IEEE/ACM
Transactions on Computational Biology and Bioin-
formatics.
Morgan Thomas, Noel M O’Boyle, Andreas Ben-
der, and Chris De Graaf. 2022.
Augmented hill-
climb increases reinforcement learning efficiency for
language-based de novo molecule generation. Jour-
nal of cheminformatics, 14(1):68.
Eduardo Tolosa, Alicia Garrido, Sonja W Scholz, and
Werner Poewe. 2021.
Challenges in the diagno-
sis of parkinson’s disease. The Lancet Neurology,
20(5):385–397.
Austin Tripp and José Miguel Hernández-Lobato. 2023.
Genetic algorithms are strong baselines for molecule
generation. arXiv preprint arXiv:2310.09267.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017.
Attention
is all you need.(nips), 2017.
arXiv preprint
arXiv:1706.03762, 10:S0140525X16001837.
Jean-Philippe Vert. 2023. How will generative AI dis-
rupt data science in drug discovery? Nature Biotech-
nology, 41(6):750–751.
W Patrick Walters and Mark Namchuk. 2003. Design-
ing screens: how to make your hits a hit. Nature
reviews Drug discovery, 2(4):259–266.
Eric Wang, Samuel Schmidgall, Paul F Jaeger, Fan
Zhang, Rory Pilgrim, Yossi Matias, Joelle Barral,
David Fleet, and Shekoofeh Azizi. 2025. Txgemma:
Efficient and agentic llms for therapeutics. arXiv
preprint arXiv:2504.06196.
Milton H Werner and C Warren Olanow. 2022. Parkin-
son’s disease modification through abl kinase inhibi-
tion: an opportunity. Movement Disorders, 37(1):6–
15.
Soojung Yang, Doyeong Hwang, Seul Lee, Seongok
Ryu, and Sung Ju Hwang. 2021. Hit and lead discov-
ery with explorative rl and fragment-based molecule
generation. Advances in Neural Information Process-
ing Systems, 34:7924–7936.
Xiufeng Yang, Tanuj Kr Aasawat, and Kazuki Yoshizoe.
2020. Practical massively parallel monte-carlo tree
search applied to molecular design. arXiv preprint
arXiv:2006.10504.
Xiufeng Yang, Jinzhe Zhang, Kazuki Yoshizoe, Kei
Terayama, and Koji Tsuda. 2017. Chemts: an effi-
cient python library for de novo molecular genera-
tion. Science and technology of advanced materials,
18(1):972–976.
Gavin Ye. 2024. De novo drug design as gpt language
modeling: large chemistry models with supervised
and reinforcement learning. Journal of Computer-
Aided Molecular Design, 38(1):20.
Naruki Yoshikawa, Kei Terayama, Masato Sumita,
Teruki Homma, Kenta Oono, and Koji Tsuda. 2018.
Population-based de novo molecule generation, us-
ing grammatical evolution.
Chemistry Letters,
47(11):1431–1434.
Botao Yu, Frazier N Baker, Ziqi Chen, Xia Ning, and
Huan Sun. 2024a. Llasmol: Advancing large lan-
guage models for chemistry with a large-scale, com-
prehensive, high-quality instruction tuning dataset.
arXiv preprint arXiv:2402.09391.
Botao Yu, Frazier N Baker, Ziru Chen, Garrett Herb,
Boyu Gou, Daniel Adu-Ampratwum, Xia Ning, and
Huan Sun. 2024b. Tooling or not tooling? the impact
of tools on language agents for chemistry problem
solving. arXiv preprint arXiv:2411.07228.
Hua Yu, Heehyoung Lee, Andreas Herrmann, Ralf Buet-
tner, and Richard Jove. 2014. Revisiting stat3 sig-
nalling in cancer: new and unexpected biological
functions. Nature reviews cancer, 14(11):736–746.
Maria
Zavadskaya,
Anastasia
Orlova,
Andrei
Dmitrenko,
and
Vladimir
Vinogradov.
2025.
Integrating qsar modelling with reinforcement
learning for syk inhibitor discovery.
Journal of
Cheminformatics, 17(1):52.


## Page 15

Xiangxiang Zeng, Fei Wang, Yuan Luo, Seung-gu Kang,
Jian Tang, Felice C Lightstone, Evandro F Fang,
Wendy Cornell, Ruth Nussinov, and Feixiong Cheng.
2022. Deep generative molecular design reshapes
drug discovery. Cell Reports Medicine, 3(12).
Di Zhang, Wei Liu, Qian Tan, Jingdan Chen, Hang Yan,
Yuliang Yan, Jiatong Li, Weiran Huang, Xiangyu
Yue, Dongzhan Zhou, and 1 others. 2024a. Chemllm:
A chemical large language model. arXiv preprint
arXiv:2402.06852.
Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen,
Quanyu Dai, Jieming Zhu, Zhenhua Dong, and Ji-
Rong Wen. 2024b. A survey on the memory mech-
anism of large language model based agents. arXiv
preprint arXiv:2404.13501.
Chengguang Zhao, Huameng Li, Huey-Jen Lin, Shulin
Yang, Jiayuh Lin, and Guang Liang. 2016. Feed-
back activation of stat3 as a cancer drug-resistance
mechanism. Trends in pharmacological sciences,
37(1):47–61.
Zihan Zhao, Da Ma, Lu Chen, Liangtai Sun, Zihao
Li, Hongshen Xu, Zichen Zhu, Su Zhu, Shuai Fan,
Guodong Shen, and 1 others. 2024. Chemdfm: Dia-
logue foundation model for chemistry. arXiv preprint
arXiv:2401.14818.
Alex Zhavoronkov, Yan A Ivanenkov, Alex Aliper,
Mark S Veselov, Vladimir A Aladinskiy, Anas-
tasiya V Aladinskaya, Victor A Terentiev, Daniil A
Polykovskiy, Maksim D Kuznetsov, Arip Asadulaev,
and 1 others. 2019. Deep learning enables rapid iden-
tification of potent ddr1 kinase inhibitors. Nature
biotechnology, 37(9):1038–1040.
A
Appendix
A.1
Impact Statement and Potential Risks
This paper presents a comprehensive method for
drug discovery based on large language models,
various deep learning methods, and evolutionary
optimization. Based on experimental results, this
approach simplifies drug search by automatically
using the best solution from the integrations.
Thus, the proposed solution can reduce the time
to search for candidate molecules that can poten-
tially be drugs. In addition, MADD can be used
by many users with different levels of expertise
(Including in the field of AI) due to the possibility
of interacting with the multi-agent system via chat.
Additionally, our ability to utilize the system’s re-
search experience can accelerate future research by
connecting the system to a central database, which
facilitates the development of an entire research
system that accumulates its effectiveness over time.
However, the widespread adoption of multi-
agent systems in drug discovery presents poten-
tial challenges. Firstly, automation of traditionally
human-driven processes may contribute to job dis-
placement. However, in our opinion, this problem
should not be critical. MADD can help free up the
time of highly skilled researchers for tasks that only
humans can perform, while junior staff can take on
the responsibility of selecting candidate molecules.
As a result, the efficiency of drug discovery will
improve without the risk of job loss, provided that
tasks are appropriately assigned.
Secondly, one particularly concerning risk is the
potential misuse of MADD to design harmful sub-
stances. This risk can also be mitigated at the gen-
erative tool level by comparing molecules with ex-
isting databases during generation and excluding
unwanted ones. Additionally, a multi-agent system
allows for the integration of filtering methods for
content returned to the user. For instance, ChatGPT
(OpenAI, 2022) uses a moderation tool 1 to filter
out inappropriate content.
The authors recognize these challenges and un-
dertake to do everything possible to minimize po-
tential risks of misuse when the framework is re-
leased.
A.2
Declaration of AI assistance
We utilized ChatGPT only for grammatical check-
ing and LaTeX support of the content presented in
1https://platform.openai.com/docs/guides/moderation
/overview


## Page 16

this study. The authors have reviewed and edited
all content.
B
Additional Related Work
B.1
LLM-based drug design
Interest in using LLMs in chemistry is growing
as they demonstrate potential in predictive ana-
lytics, molecular modeling, and developing new
compounds. For instance, ChemDFM, a conversa-
tional platform powered by LLMs, was proposed
for working with chemical data (Zhao et al., 2024).
Research by M. Bran A. et al. (M. Bran et al.,
2024) showed that integrating LLMs with chemi-
cal tools improves molecular property predictions.
Also, Ye G. (Ye, 2024) proposed a novel approach
for de novo drug design using LLMs, enabling the
automation of new chemical compound generation.
A CancerGPT (Li et al., 2024b) model for pre-
dicting drug pair synergy using few-shot learning
was introduced to accelerate the development of
new therapies. Chemical LLM LlasMol (Yu et al.,
2024a) was trained on the SMolInstruct dataset and
can solve many chemical problems. Another recent
example is X-Lora-Gemma (Stewart and Buehler,
2024), whose main task is molecular optimization.
B.2
Chemical agent-based LLM pipelines
Agent-based pipelines have been widely used in
chemistry and pharmacology (M. Bran et al., 2024;
Zhang et al., 2024b; McNaughton et al., 2024;
Li et al., 2024b; Jablonka et al., 2023) since late
2023. These systems automate experiments, sig-
nificantly reducing time and financial costs and
enabling professionals to achieve their objectives
more efficiently.
One way to improve the results is to use a multi-
agent system. Here are several multi-agent so-
lutions that are mostly related to the proposed
MADD. DrugAgent (Liu et al., 2024a) implements
an LLM-based multi-agent pipeline for drug dis-
covery. This solution does not generate molecules
according to the given properties from scratch, but
optimizes the proposed ones. ChemCrow (M. Bran
et al., 2024) is focused on autonomous planning
and executing chemical synthesis using a robotics
platform. This solution supports 18 applied chemi-
cal tools, which are no longer supported. Another
example is chemical agent CACTUS (McNaughton
et al., 2024), which can solve tasks such as molec-
ular property prediction, similarity searching, and
drug-likeness assessment. Still, it supports only
one-step tasks, which seems insufficient for many
real-world research tasks.
One of the most recent solutions is Phoenix,
an AI-based platform developed by Future-
House (Skarlinski et al., 2024b) to accelerate chem-
istry and drug discovery research. It combines
LLM agents, chemoinformatics tools, and molecu-
lar generators in an interactive environment. Users
can define tasks in natural language, and the system
performs multi-step reasoning to generate or op-
timize compounds, plan experiments, and predict
ADMET properties. A key strength of Phoenix is
its support for both de novo design and multi-target
optimization.
The most relevant to MADD is ChemAgent (Yu
et al., 2024b), which can generate new molecules
and predict reaction results. It has 29 tools at its
disposal - calculation tools, web search, access
to chemical databases, and a molecular generator.
However, experiments show low validity generated
by ChemAgent molecules (Figure 3).
These studies highlight that LLMs can accelerate
scientific progress in chemistry and related fields,
unlocking new material and drug development op-
portunities. Nevertheless, none demonstrate suc-
cessful automation of the whole drug discovery
pipeline. Possible reasons are (1) the absence of
a readily accessible dataset with a complete drug
discovery pipeline for training and validating new
models and approaches, and (2) the weakness of
existing separated models and agent architectures
for solving the whole task with a high-quality level.
Based on these conclusions, we propose our vi-
sion of a stronger approach and a valuable newly
farmed dataset with state-of-the-art pharmaceutical
research tasks and their solutions.
B.3
Data-driven drug design methods
Drug design is a rapidly growing field combin-
ing chemistry and machine learning. Tradition-
ally, discovering new molecules or selecting chem-
ical structures to solve a particular problem relies
on existing experimental data and subjective re-
search experience, which limits the number and
variety of possible compounds that can be con-
sidered. Generative models allow efficient explo-
ration of the molecular space, which has already
fueled the explosive growth of molecular gener-
ative design. Recurrent neural networks (Suresh
et al., 2022; Dollar et al., 2021), variational au-
toencoders (Gómez-Bombarelli et al., 2018; Lee
and Min, 2022; Ochiai et al., 2023; Bhadwal et al.,


## Page 17

2023), generative-adversarial networks (Guimaraes
et al., 2017; Prykhodko et al., 2019; Pang et al.,
2023; Macedo et al., 2024), evolutionary algo-
rithms (Yoshikawa et al., 2018; Leguy et al., 2020;
Kerstjens and De Winter, 2022; Jensen, 2019; Tripp
and Hernández-Lobato, 2023), and hybrid models
using reinforcement learning (RL) methods (Putin
et al., 2018; Thomas et al., 2022; Zhavoronkov
et al., 2019) have been successfully applied to solve
various problems in chemistry.
Also, there are approaches for generating
molecules using Monte Carlo tree search (Yang
et al., 2020, 2017). These methods perform well
compared to recurrent neural networks and varia-
tional autoencoders, including Bayesian optimiza-
tion (Kusner et al., 2017).
Another advanced model for sequence genera-
tion is Transformer (Vaswani et al., 2017), which
is based on the attention mechanism. For molecule
generation tasks, this model has successfully shown
high performance in several studies (Ang et al.,
2024; Mao et al., 2023; Haroon et al., 2023). Re-
searchers attribute this architecture’s high perfor-
mance to its ability to handle long sequences,
which applies to chemical structures as they are
usually treated as a sequence of atoms and bonds.
C
Additional Case Studies
C.1
Description of related experiments
conducted outside the benchmark.
To assess the generalizability of the MADD system
to new use cases, we conducted a series of exper-
iments outside the main system framework using
the Orchestrator agent. These experiments specifi-
cally focused on data preparation and the training
of both generative and predictive models. This
separate evaluation was necessary because the ex-
periments conducted on the benchmark discussed
in the main article did not require automated model
training.
The primary objective was to determine whether
the system’s accuracy would degrade upon the in-
tegration of additional tools for automated training
into the Orchestrator agent. Our findings indicate
that the introduction of a tool capable of launch-
ing the sequential training predictive and generative
models on a user-prepared dataset does not compro-
mise the system’s accuracy. Consequently, these
experiments confirm the potential for extending the
MADD system to other disease-related use cases
beyond the original seven on which it was tested.
However, from a technical perspective of multi-
agent system architecture, several challenges must
be addressed to enable automatic expansion to
new cases. These include implementing automated
monitoring of model training for a new case, an-
alyzing the readiness of predictive models before
initiating generative model training, and keeping
the user informed about the training status for a
given case. The creation of training data neces-
sitates the search, download, and processing of
data into the format required by both the predictive
and generative models. Addressing all these tasks
requires an expanded set of tools and increases sys-
tem complexity. These functionalities were imple-
mented within the LLM-based tools AutoML-DL
and DatasetBuilder (which in subsequent research
will be utilized as standalone agents). Implemen-
tations of these agents already exist in our MADD
code as separate agents for additional studies. This
implementation, in turn, necessitated the use of
more advanced LLMs and modifications to the
multi-agent system schema. This ongoing research
is being conducted on a new system version and
falls outside the scope of the present article.
Despite the aforementioned complexities and the
need for system optimization, MADD successfully
handles all the described tasks. Optimization is
primarily required to enhance user interaction and
usability. Furthermore, the necessity for optimiza-
tion is driven by the rapid advancement of AI and
LLM technologies, which demands continuous sys-
tem development and relevance analysis. We look
forward to demonstrating our future developments
in subsequent publications.
C.2
Multi-agent system ablation study
The simplest system for comparison is single-agent
system (“MADD-v1”), which uses CoT-based
reasoning with reflection enabled. A two-agent
variant “MADD-v2A” with Decomposer and Or-
chestrator allowed intermediate user-facing outputs
but lacked final summarization and did not per-
mit answer revision. Another two-agent system
“MADD-v2B” assigned tool selection and summa-
rization to the Orchestrator. The third variant
“MADD-v2C” delegated decomposition and tool
selection to a unified Orchestrator-Decomposer,
with a separate Summarizer.
The RAG-based
three-agent system ‘MADD-v3” incorporated a
Chemical RAG agent alongside a Decomposer and
Orchestrator, following CoT logic without revision.
Unlike these, MADD integrates summarization and


## Page 18

Figure 6: Tanimoto similarity (maximum values) for all generated molecules
user interaction as distinct, modular steps, enabling
a more structured and complete workflow. The
above-mentioned variations of the MADD system
are shown in Figures 12 and 13.
A single system prompt was used throughout the
experiments, except for agent-specific instructions.
Prompts were updated depending on the responsi-
bilities of the specific agent. All agents used LLM
Llama-3.1-70b, as it was the one that showed the
best result in past experiments.
Explicit CoT Implementation: The prompt en-
forces sequential reasoning mirroring the multi-
agent process:
Question →Thought (decomposition) →Action
(tool selection and execution) →Observation (tool
output analysis) →Final Answer (synthesis and
tabular presentation)
This CoT sequence ensures the agent:
Decomposition:
The "Thought" step breaks
down the query, akin to the Decomposers role.
Orchestration: JSON-blob tool calls replicate
the Orchestrator.
Summarization: The final out.
The full prompt and agent logic are avail-
able in the repository:
https://github.com/
ITMO-NSS-team/MADD/blob/main/multi_
agent_system/run_singleAgentSystem.py
C.3
Comparisons with existing LLM solutions
The aim was to evaluate the performance of these
systems in handling our specific task across all
our datasets. The answer was assumed correct if
the molecule proposed by LLM passed through
the GR1 filter. In contrast, for ChemAgent, the
presence of a generator tool call, the absence of
unnecessary calls, and the summarization of the
final answer were checked.
ChemAgent settings:
• We evaluated the system ChemAgent on all
datasets.
• The high computational cost of generating re-
sponses was a major limitation. Specifically,
using the GPT-4o model in ChemAgent re-
quires multiple queries per example, greatly
escalating the system’s financial cost.
• To determine the correct selection of tools
by the system, we categorized them into sev-
eral groups. MoleculeGenerator was consid-
ered the appropriate tool since all the tasks
in our benchmark included molecule gener-
ation. The inappropriate tools were HIVIn-
hibitorPredictor and LogDPredictor, as their
usage meant that the model misunderstood the
prompt. All other tools were considered neu-
tral, as they could be used to solve problems
in the benchmark, but were not mandatory.
Thus, we considered that the model correctly
selected tools if it did not use the inappropri-
ate ones and used the appropriate ones at least
once.
• To determine the summarization quality, we
checked whether the molecules generated
with the generator agent were present in the
final response to the user. The molecules in
the answer had to be in SMILES format, and
their number had to be greater than or equal
to the number of tasks in the prompt.
C.3.1
Phoenix
We also evaluated the system Phoenix across a
representative set of our benchmark tasks.
Phoenix offers a powerful multi-agent architec-
ture capable of solving complex chains of reason-
ing, from molecular generation to retrosynthetic
analysis and cost estimation. The system dynami-
cally selects agents for specific subtasks, enabling
deep integration of tools such as structure genera-
tors, property predictors, and synthesis planners.
However, this complexity may also be its weak-
ness. The system failed to deliver a final answer


## Page 19

Table 5: Comparing MADD with ChemAgent, LlasMol, X-Lora-Gemma, and ChemDFM on datasets of different
complexity.
Metric
MADD
ChemAgent
LlasMol
X-Lora-Gemma
ChemDFM
Dataset S
Tool Selection
86.9
57.8
-
-
-
Summarization
100
21.5
-
-
-
Final Acc. (%)
86.9
12.4
0.46
0.44
5.31
Dataset M
Tool Selection
86
68.3
-
-
-
Summarization
98
22.4
-
-
-
Final Acc. (%)
84.3
15.3
0.24
0.12
0.33
Dataset L
Tool Selection
83.7
85.8
-
-
-
Summarization
95.3
19.1
-
-
-
Final Acc. (%)
79.8
16.4
0
0
0
in multiple test cases, likely due to interruptions or
breakdowns during the reasoning process. While
intermediate outputs (e.g., molecules or partial
plans) were often correct, Phoenix sometimes ter-
minated without producing a complete or action-
able response.
Despite these limitations, Phoenix has strong
potential for real-world applications in automated
drug discovery and chemistry workflows, provided
further robustness improvements are implemented.
Results. Final results were obtained for slightly
more than half of the questions, with the rest fail-
ing due to system interruptions during multi-agent
reasoning. The generated molecules were evalu-
ated using our in-house filtering criteria. Out of 40
generated structures, only 6 passed the GR1 filter
group, and just one Alzheimer-focused molecule
satisfied the more stringent GR2 filters. Notably,
many proposed molecules were identified as known
compounds from the ChEMBL database, indicat-
ing the system’s tendency to rely on existing chem-
ical space. However, a subset of generated struc-
tures appeared to be unique, showing no matches
in either PubChem or ChEMBL, which suggests
the potential of Phoenix to explore novel chemical
scaffolds.
C.3.2
TxGemma
We also evaluated the TxGemma models (9B-chat
and 27B-chat) on a representative subset of our
benchmark.
TxGemma is a family of open-source models de-
veloped by Google DeepMind, fine-tuned from the
Gemma 2 foundation to support a broad spectrum
of therapeutic development tasks. These include
classification (e.g., BBB permeability), regression
(e.g., binding affinity prediction), and generation
(e.g., retrosynthesis). The ’chat’ variants, which we
evaluated, are instruction-tuned for conversational
use and scientific reasoning, making them well-
suited for exploratory dialogues and hypothesis-
driven research.
Despite their strengths in structured therapeutic
tasks and interactive reasoning, the TxGemma-chat
models underperformed in our benchmarks. Specif-
ically, the 9B and 27B-chat models failed to gener-
ate valid molecular structures that passed our GR1
or GR2 filters.
This underperformance is likely rooted in the
models’ training focus. While TxGemma excels in
task-specific predictive settings—particularly in ret-
rosynthesis and toxicity classification—the broader,
unguided generation of novel molecular structures
was not a primary training objective. Consequently,
the models struggled with generic SMILES genera-
tion tasks beyond narrow, supervised domains.
Results.
Across all evaluated prompts, nei-
ther TxGemma-9B-chat nor TxGemma-27B-chat
produced structures that met filtering thresholds.
These results suggest that, while TxGemma-chat
holds promise as a reasoning and explanation en-
gine for therapeutic R&D, it currently lacks the
generative robustness required for open-ended de
novo molecule design.
C.4
Dataset preparing studies
C.4.1
Benchmark Preparation Pipeline
The initial validation dataset, subsequently used to
generate modified versions for experimental pur-
poses, consists of 245 potential user queries con-
taining mentions of target proteins, properties, and
disease symptoms.
The dataset was designed in the following steps:
Initial query design. Thirty queries were cre-
ated manually, representing cases that users with


## Page 20

different levels of chemistry expertise could pose.
Each query was labeled with a corresponding dis-
ease/property name. Most of these queries did
not explicitly specify the type of task, e.g., genera-
tion/properties calculation or request the invocation
of a specific function.
Dataset expansion via few-shot learning. The
dataset was expanded using few-shot learning tech-
niques applied to several LLMs, including GPT-4o,
o1-mini, Claude Sonnet 3.5, and Gemini 1.5 Pro.
The LLMs were provided with a few examples and
instructions to generate similar but non-redundant
examples. Instructions included explicit requests
to generate some examples from the perspective of
an experienced professional and a beginner. Also,
we excluded uniform queries from the dataset by
analyzing the similarity of query embeddings using
sentence transformers (Stankeviˇcius and Lukoše-
viˇcius, 2024; Devika et al., 2021). Upon comple-
tion, the dataset was expanded to a total of 400
queries.
Validation by chemistry experts. Chemistry ex-
perts reviewed the synthetically generated queries
and selected the most plausible ones. This step
yielded the final dataset of 245 queries. The re-
sulting dataset is called Dataset S. It is the easiest
because each query consists of one task. From the
examples of this dataset, two subsequent datasets
were collected using combinations of several tasks
in a single query. Medium difficulty Dataset M is
a combined dataset containing both easy samples
and samples of higher complexity. Each query can
have from 1 to 3 tasks that require calling differ-
ent functions. Finally, the most complex dataset is
called Dataset L. All queries consist of a minimum
of 4 tasks and a maximum of 5.
C.4.2
Dataset diversity
The presented datasets are pretty diverse.
We
checked their diversity by calculating the Cosine
similarity for each pair of embeddings. Figure
8 shows the correlation matrix between dataset
queries.
C.4.3
Examples of queries from each dataset
• Sample from Dataset S:
– Generate molecules for highly efficient
inhibition of Bruton’s tyrosine kinase by
non-covalent binding that is highly selec-
tive for BMX simultaneously.
• Sample from Dataset M:
– Generate GSK-3β inhibitors with high
docking score and low brain-blood bar-
rier permeability. Generate inhibitors
of KRAS protein with G12C mutation.
The inhibitors should be selective and
not bind with HRAS and NRAS proteins.
• Sample from Dataset L:
– Suggest several molecules with high
docking affinity with KRAS G12C pro-
tein. Molecules should possess common
drug-like properties, including low tox-
icity, high QED score, and high level
of synthesizability. Generate highly po-
tent non-covalent BTK tyrosine kinase
inhibitors from the TEC family of tyro-
sine kinases that can potentially affect
B cells as a therapeutic target for treat-
ing multiple sclerosis. Can you suggest
molecules that inhibit Proprotein Con-
vertase Subtilisin/Kexin Type 9 with en-
hanced bioavailability and the ability to
cross the BBB? Generate a new drug that
enhances neurotransmitter balance, pro-
motes neuroprotection, and reduces ox-
idative stress. These compounds should
possess high bioavailability, cross the
blood-brain barrier efficiently, and show
minimal metabolic degradation.
C.4.4
Dataset Preparation for Generative
Model Training
A dataset of 500,000 unique small molecules
for each disease was assembled from the public
ChEMBL database. To select these molecules, we
chose those with low molecular weight. According
to Lipinski’s rule (Lipinski, 2016), to which a drug
molecule should ideally conform, the molecular
mass of a compound should not exceed 500 g/-
mol. This was necessary because such compounds
tend to have better penetration ability, increased
bioavailability, and greater synthetic availability,
making them more promising drug candidates.
Structural and physicochemical descriptors were
calculated for each molecule, representing a com-
pact quantitative description of the molecular prop-
erties.
First, molecular docking simulations were per-
formed for each molecule using AutoDock Vina
GPU 2.1, from which the minimum binding energy
value was calculated (Appendix D.4.2. This index
reflects the degree of affinity of the molecule to a


## Page 21

Figure 7: The process of obtaining a validation dataset for an experimental study.
given protein target, which is essential for evaluat-
ing its potential as an inhibitor.
A binary label IC50 (inhibition efficiency) was
generated by training machine learning models on
open biological data from specialized databases
such as ChEMBL and BindingDB to predict the
molecules’ biological activity. Relevant protein
targets were selected for each of the six diseases
under consideration, and binary classification tasks
were generated based on them to identify poten-
tially active compounds.
The calculation of the remaining descriptors was
performed using the RDKit library and included
the following metrics:
• QED (Quantitative Estimate of Drug-likeness)
- a comprehensive metric reflecting the “drug-
likeness” of a molecule and taking into ac-
count multiple parameters such as molecular
mass, number of hydrogen bond donors and
acceptors, number of fragments, lipophilicity
(logP), etc.;
• Synthetic Accessibility Score (SAS) - a nu-
merical assessment of the difficulty of synthe-
sizing a compound (the higher the value, the
more laborious the synthesis);
• Toxicity and unwanted fragment filters includ-
ing:
– PAINS (Pan-Assay INterference com-
poundS) - indicator of potentially false
positive results in biological tests;
– Brenk, Glaxo, and SureChEMBL filters
identify known structural fragments asso-
ciated with toxicity, reactivity, and other
undesirable effects.
Thus, six datasets of approximately 500k
molecules (totaling over 3M) were obtained for
generative model training.
C.5
Comparing LLM accuracy and costs
The cheapest model with a price of 0.01$ per 1000
tokens was llama3.1-70b-int4, showing the worst
accuracy of 26.5%. The most expensive model
priced at 6$ per 1000 tokens was o1-mini, while its
accuracy was low (only 67.5%). The optimal solu-
tion was Llama3.1-70b: with the highest accuracy
of 92.3%, the cost was 1.2$. That is why we chose
this LLM for MADD pipelines.
The price was taken from the reseller’s website,
which provided the models with API.
C.6
Properties prediction by manual
pre-trained ML models experimental
studies
Settings:
To be confident in the results of generating
molecules of the whole system, it is necessary to
verify the accuracy of the predictive models and
select the best one. To select the best models for
lgIC50 prediction, cross-validation was performed
for CatBoost (Prokhorenkova et al., 2018), XG-
Boost (Chen and Guestrin, 2016), Random Forest
(Breiman, 2001), Extra Trees (Geurts et al., 2006),
and LightGBM (Ke et al., 2017) models. The best
models for each task were selected from 5 candi-
dates.
Results:
The best predictive model selection resulted in
selecting the top 3 best models. For Dyslipidemia,
Drug resistance, and Parkinson’s disease, CatBoost
performed best, achieving values of F1 scores of
0.82, 0.85, and 0.92, respectively. For Alzheimer’s
disease and lung cancer, Extra Trees was the most
effective model, achieving F1 scores of 0.83 and
0.84, respectively.
Finally, the Random Forest
model showed the best result for multiple sclerosis,
with an F1 score equal to 0.92.
These results are detailed in Table 6 from Ap-
pendix C.7.


## Page 22

C.7
Additional AutoML tool results
Using a multi-agent system’s capability to run au-
tomatic ML model training, we conducted a model
training experiment using MADD to predict IC50
values on the considered diseases and the SYK pro-
tein. MADD AutoML is based on the open-source
framework F. The Morgan fingerprints (2048, ra-
dius=2) were used for data preprocessing, namely,
translation of SMILES molecules into vector em-
bedding.
Our system autonomously selects optimized
pipelines for each disease, leveraging ensemble
strategies and hyperparameter tuning to outperform
or closely match baseline models. Specifically, it
employs stacking (stacked generalization) and bag-
ging (bootstrap aggregation). Stacking improves
predictions by combining multiple models through
a meta-model trained on their outputs, while bag-
ging reduces variance by training models on differ-
ent data subsets and aggregating their results. The
results demonstrate MADD’s flexibility in adapting
pipelines to diverse datasets:
• Alzheimer’s disease: A stacking ensemble
of gradient-boosted models (CatBoost, Light-
GBM, XGBoost) with an L2-regularized lin-
ear meta-model achieved an accuracy of 0.963
and F1-score of 0.978, surpassing all individ-
ual baselines (e.g., Extra Trees: 0.823 F1).
• Multiple sclerosis: Weighted averaging of
gradient boosts yielded competitive perfor-
mance (0.889 accuracy, 0.921 F1), closely
matching the best baseline (Random Forest:
0.887 accuracy).
• Parkinson’s disease: Despite using a similar
as Alzheimer’s disease case stacking pipeline
(accuracy: 0.872), CatBoost alone performed
better (0.910 accuracy), suggesting simpler
models may suffice for certain datasets.
• Dyslipidemia: Bootstrap-aggregated stacking
with L2 regularization achieved 0.767 accu-
racy, while CatBoost (0.778 accuracy, 0.818
F1) remained the strongest standalone model.
• Drug resistance: A single, hyperparameter-
optimized Random Forest under MADD out-
performed its vanilla counterpart (0.845 vs.
0.828 accuracy), highlighting the value of au-
tomated tuning.
• Lung cancer: The stacking pipeline (0.780
accuracy) underperformed versus Extra Trees
(0.843 accuracy), indicating potential over-
complexity for this case.
• Thrombocytopenia (SYK): The system se-
lected a stacked generalization with L2-
regularized linear regression as the meta-
learner and base learners CatBoost Regressor,
XGBoost Regressor, and LightGBM Regres-
sor, achieved an MSE of 0.32 and R2 of 0.75
on the test set.. The specialized SYK-FBRL
method slightly outperformed our approach
(MSE: 0.27, R2: 0.78).
The more detailed results for ML experiments
are provided in Table 6.
C.8
Training tool selection
Additionally, we tested the tool’s ability to au-
tonomously select appropriate training strategies
when no pre-trained models were available upon
user request.
In this experiment, the orchestrator, in addition
to his prompt, also received a dictionary containing
the names and descriptions of already trained gen-
erative models that could be used for inference. We
conducted experiments on two cases, Alzheimer’s
disease and thrombocytopenia. We prepared 28
queries for each disease to generate molecules to
treat that disease. We conducted two experiments
for each case. In the first one, we did not add the
case from the dataset to the dictionary with trained
models; in the second, we did. Thus, in the first
case, the orchestrator had to call an agent to train
a new generative model and select optimal train-
ing parameters. In the second case, it had to call
a tool to generate molecules using parameters cor-
responding to the disease from the query. As a
result, having obtained metrics for each disease,
we took an average of the two experiments. We
obtained a tool selection metric equal to 0.79 in the
case of Alzheimer’s disease and 0.82 in the case
of Thrombocytopenia. This shows that the system
can determine the necessary tool depending on the
query and choose between training a new genera-
tive model and using an already trained one if it
is available and presented in the trained models
dictionary provided to the orchestrator.
Analyzing the automatic selection of genera-
tive tools based on previous experience.
A comparison of the MADD framework and
other LLM-based systems is performed on all


## Page 23

Table 6: Comparison of Accuracy and F1 score for the considered machine learning models. Molecular representa-
tions used as features: Alzheimer’s disease case - MACCS fingerprints; multiple sclerosis - Morgan fingerprints
(1024, radius=2); Parkinson’s disease - RDKit descriptors and Avalon fingerprints; lung cancer - Morgan fingerprints
(512, radius=2); dislipidemia - RDKit descriptors and Avalon fingerprints; drug resistance - RDKit descriptors and
Avalon fingerprints.
Case
Model
Accuracy
F1 score
MADD Auto
0.963
0.978
Alzheimer’s disease
CatBoost
0.810
0.810
Random Forest
0.822
0.829
XGBoost
0.803
0.803
Extra Trees
0.823
0.829
LightGBM
0.810
0.820
Mupltiple sclerosis
MADD Auto
0.889
0.921
CatBoost
0.865
0.905
Random Forest
0.887
0.920
XGBoost
0.876
0.912
Extra Trees
0.886
0.919
LightGBM
0.885
0.918
Parkinson’s disease
MADD Auto
0.872
0.856
CatBoost
0.910
0.920
Random Forest
0.890
0.900
XGBoost
0.910
0.910
Extra Trees
0.890
0.900
LightGBM
0.900
0.910
Dyslipidemia
MADD Auto
0.767
0.694
CatBoost
0.778
0.818
Random Forest
0.775
0.813
XGBoost
0.725
0.772
Extra Trees
0.750
0.782
LightGBM
0.725
0.775
Drug resistance
MADD Auto
0.845
0.896
CatBoost
0.838
0.848
Random Forest
0.828
0.836
XGBoost
0.823
0.826
Extra Trees
0.837
0.844
LightGBM
0.832
0.835
Lung cancer
MADD Auto
0.780
0.770
CatBoost
0.822
0.822
Random Forest
0.838
0.835
XGBoost
0.827
0.826
Extra Trees
0.843
0.842
LightGBM
0.811
0.809
queries of our dataset. In this case, MADD could
use all available generative tools and select them
based on the results of the generative model’s per-
formance evaluation experiment. The Appendix
E.5 presents an example of the instructions and the
results that were passed. MADD selected a suitable
generative method to reduce the experimentation
time using tools with already trained models.


## Page 24

C.9
Generative model comparisons study
Settings. We used the percentage of remaining
target molecules after filtering by criterion groups
as a metric. It was decided to make five such fil-
ters; the higher the filter level, the stricter the fil-
tering by properties. Each subsequent filter group
includes all previous filter groups. This was done
to understand better how accurately the generated
molecules could match the requested properties to
be a drug candidate for the selected disease. A de-
tailed explanation of the choice of filter groups is
presented in Appendix D.5.
Results. The results of experiments with gen-
erative tools show that different models perform
differently on different tasks. Non-LLM-based
models show stable results, sometimes outperform-
ing all other solutions. LLM-based solutions are
generally able to generate molecules that are be-
tween 27.7% and 99.84% chemically valid, which
is shown in Table 9. However, if it is necessary
to create a molecule that meets two or more prop-
erties, the results do not exceed 2.56 percent of
the target molecules. Only ChemDFM showed
relatively good results, generating up to 10.71%
of target molecules passing the fifth filter group
for Alzheimer’s disease. It is worth noting that this
model is pre-trained, and it is difficult to verify how
many new molecules were generated that were not
in the training dataset.
Among the generative methods that do not use
LLM, the transformer implemented in MADD is
the most stable solution, outperforming the other
solutions for three diseases and giving satisfactory
results in the other cases, as shown in Table 8.
Other considered generative approaches may show
much better results in different cases. For example,
MTDD-EF generates up to 28% of target drugs
corresponding to the fifth filter group for Multiple
sclerosis disease. However, the resulting molecules
are similar in Tanimoto similarity (Diversity) (Ba-
jusz et al., 2015), and this high result was not re-
produced in other diseases.
C.9.1
Our developed generative models
Our GAN implementation consists of 2 LSTM
blocks with one bidirectional layer, input layer, and
hidden layer of size 128.
Inspired by the transformer-based conditional
VAE (Kim et al., 2021), we implemented our trans-
former for a targeted generation with property con-
trol. We trained this model with seven properties
in the conditional block and a vocabulary size of
126 to encode SMILES molecules. The number
of transformer layers and heads in the encoder and
decoder was also increased to 12. The architecture
of our transformer is shown in Figure 10.
In the context of our study, generating 10,000
molecules using the Transformer model took ap-
proximately 45 minutes on an NVIDIA RTX
A6000 GPU, while the GAN completed the same
task in just 1.88 seconds (Table 7). On a more
commonly available NVIDIA GeForce RTX 2070,
the generation times were 3.73 hours for the Trans-
former and 3.37 seconds for the GAN. However,
a combination of generative models is required to
enable a more comprehensive exploration of the
target chemical space.
C.10
Analysis of an evolutionary approach for
hit molecule generation
Testing MTDD-EF as a tool for molecule genera-
tion allowed us to evaluate the potential of EO for
drug-candidate design. Table 8 presents the per-
formance indicators in solving the proposed cases.
The low performance in non-brain disease cases
(Drug resistance, Dyslipidemia, Lung cancer) was
expected, as MTDD-EF is not adapted to solve
cases based on diseases of this type.
Otherwise, the evolutionary algorithm looks
promising in the molecule generation task, as
shown in the Alzheimer’s and Parkinson’s cases.
However, an interesting one is the case of Multi-
ple Sclerosis, for which no molecules with satis-
factory performance criteria were obtained in the
generation result. This shows that EO is very sensi-
tive to the initial approximation. This fact can be
further confirmed by the substantial difference in
molecular mass (as an indirect criterion of struc-
tural complexity) between experimentally validated
molecules and those generated using MTDD-EF
(Figure 11a for Alzheimer’s disease 16%, Figure
11b for Multiple sclerosis 35%). Thus, to use EO
effectively, it is necessary to use methods other
than those used in MTDD-EF to create an initial
approximation for the initial population.
C.11
Overall efficiency analysis
It is important to note that we also exclude
molecules that existed in the generative tools train-
ing dataset for MADD. For other solutions, it is
impossible to calculate the novelty of the gener-
ated molecules because we do not have the data
on which the solutions were trained. For exam-
ple, ChemDFM has chemical data because it was


## Page 25

Table 7: Comparison of GPU memory usage, training, and generation times.
Model
GAN
Transformer
GPU memory (GB)
6.40
8.43
Training time (hours)
2.82
23.73
Generation time (ms/molecule)
0.19
295.00
Table 8: Percentage of target molecules across filter groups obtained during the generation series by each model.
Case
Model
GR1, %
GR2, %
GR3, %
GR4, %
GR5, %
Diversity
Alzheimer’s disease
GAN
19.03
14.75
11.70
11.32
11.32
0.37
Transformer
26.06
23.58
18.47
18.15
18.15
0.24
RL
15.8
14.34
10.99
10.74
10.74
0.21
MTDD-EF
69.00
69.00
37.00
28.00
28.00
0.18
ChemTSv2
9.03
9.03
0.53
0.50
0.50
0.12
Multiple sclerosis
GAN
5.90
4.35
3.49
3.36
3.36
0.39
Transformer
15.43
13.75
13.32
13.29
13.29
0.25
RL
22.81
20.34
18.39
18.22
18.22
0.11
MTDD-EF
0.00
0.00
0.00
0.00
0.00
0.14
ChemTSv2
2.44
2.44
1.66
1.55
1.55
0.44
Parkinson’s disease
GAN
14.45
11.48
8.92
8.57
8.57
0.36
Transformer
3.32
3.06
2.69
2.65
2.65
0.24
RL
0.03
0.03
0.00
0.00
0.00
0.17
MTDD-EF
24.00
24.00
12.00
12.00
12.00
0.16
ChemTSv2
24.21
24.21
14.39
13.47
13.47
0.44
Drug resistance
GAN
0.23
0.15
0.10
0.10
0.10
0.39
Transformer
8.32
6.92
6.14
6.05
6.05
0.77
RL
0.63
0.52
0.40
0.38
0.38
0.13
MTDD-EF
0.00
0.00
0.00
0.00
0.00
0.11
ChemTSv2
0.14
0.14
0.09
0.09
0.09
0.43
Dyslipidemia
GAN
7.27
6.15
4.92
4.72
4.72
0.34
Transformer
28.87
28.27
25.07
24.50
13.16
0.21
RL
0.02
0.02
0.02
0.02
0.02
0.05
MTDD-EF
0.00
0.00
0.00
0.00
0.00
0.06
ChemTSv2
0.12
0.12
0.06
0.06
0.06
0.44
Lung cancer
GAN
5.53
4.41
3.43
3.31
3.31
0.39
Transformer
6.12
5.72
4.97
4.76
4.76
0.8
RL
0.57
0.53
0.51
0.51
0.51
0.09
MTDD-EF
1.00
1.00
1.00
1.00
1.00
0.11
ChemTSv2
6.65
6.65
4.06
3.53
3.53
0.43
explicitly trained for chemical problems, but it is
difficult to say how novel the molecules it creates
are.
To assess the novelty of the molecules created,
MADD was compared with the training dataset
on which GAN and Transformer were trained.
Thus, we counted the number of chemically valid
molecules for MADD and how many potentially
new ones our system could create. Of course, if we
train models on a larger dataset, it becomes more
challenging to develop new ones. On the other
hand, Transformers can become more efficient by
training on a larger dataset. Thus, the fact that it
is possible to create new molecules with the de-
sired properties emphasizes the efficiency of our
proposed system. The novelty and validity of the
molecules for each disease are described in Table
9.
Alternatively, for other LLM-based approaches,
we cannot test the novelty of the molecules,
and these solutions are doubtful in creating new
molecules.


## Page 26

Table 9: Full table of Mean Docking score, novelty, validity, and percentage of target molecules across filter groups
obtained during the generation series by each approach. * filtered
Case
Model
Novelty
%
Validity
%
Mean
DS
GR1,
%
GR2,
%
GR3,
%
GR4,
%
GR5,
%
Alzheimer
MADD
78.21
87.47
-7.46
20.30
17.56
13.72
13.40
13.40
MADD
Auto
73.47
89.5
-7.57
15.99
14.43
13.14
12.34
12.34
Llasmol
-
64.00
-5.36
4.54
4.54
4.54
0.00
0.00
X-LoRA
Gemma
-
43.60
-4.14
0.00
0.00
0.00
0.00
0.00
ChemAgent
-
23.50
-6.42
2.50
0.00
0.00
0.00
0.00
ChemDFM
-
99.84
-6.80
28.57
17.86
10.71
10.71
10.71
Sclerosis
MADD
73.45
84.32
-9.24
14.71
12.81
11.73
11.62
11.62
Llasmol
-
58.00
-6.08
0.00
0.00
0.00
0.00
0.00
X-LoRA
Gemma
-
43.60
-5.81
5.12
2.56
2.56
2.56
2.56
ChemAgent
-
29.20
-6.50
1.04
0.52
0.52
0.52
0.52
ChemDFM
-
85.70
-8.13
11.11
5.56
5.56
5.56
0.00
Parkinson
MADD
61.21
78.21
-6.04
5.93
4.86
3.87
3.74
3.74
Llasmol
-
68.00
-6.18
0.00
0.00
0.00
0.00
0.00
X-LoRA
Gemma
-
45.50
-5.01
0.00
0.00
0.00
0.00
0.00
ChemAgent
-
27.70
-3.41
0.00
0.00
0.00
0.00
0.00
ChemDFM
-
94.60
-6.03
2.50
2.50
0.00
0.00
0.00
Drug
Resistance
MADD
67.17
63.21
-6.97
3.06
2.53
2.21
2.18
2.18
Llasmol
-
69.00
-3.89
0.00
0.00
0.00
0.00
0.00
X-LoRA
Gemma
-
42.80
-3.73
0.00
0.00
0.00
0.00
0.00
ChemAgent
-
31.00
-4.96
0.00
0.00
0.00
0.00
0.00
ChemDFM
-
94.70
-6.08
2.78
2.78
0.00
0.00
0.00
Dyslipidemia
MADD
71.21
73.21
-7.68
12.05
11.48
10.00
9.75
5.97
Llasmol
-
60.12
-3.97
0.00
0.00
0.00
0.00
0.00
X-LoRA
Gemma
-
44.80
-4.65
0.00
0.00
0.00
0.00
0.00
ChemAgent
-
29.04
-5.75
0.00
0.00
0.00
0.00
0.00
ChemDFM
-
97.50
-5.68
0.00
0.00
0.00
0.00
0.00
Lung
cancer
MADD
60.12
67.21
-6.32
4.07
3.55
2.97
2.86
2.86
Llasmol
-
59.12
-3.9
0.00
0.00
0.00
0.00
0.00
X-LoRA
Gemma
-
43.10
-4.73
0.00
0.00
0.00
0.00
0.00
ChemAgent
-
33.50
-6.54
0.00
0.00
0.00
0.00
0.00
ChemDFM
-
97.60
-5.18
0.00
0.00
0.00
0.00
0.00
Thrombo-
cytopenia
MADD
Auto
90.71
-8.02
1.54
1.35
1.32
1.32
1.32
SYK-FBRL
100*
-7.76
0.70
0.28
0.26
0.25
0.25
D
Supporting information
D.1
Examples of Queries:
D.1.1
Manually Composed Queries by
Experts
• Alzheimer’s disease:


## Page 27

– Generate GSK-3β inhibitors with high
docking score and low brain-blood bar-
rier permeability.
– Generate GSK-3β inhibitors with high
activity.
• Lung cancer:
– Generate inhibitors of KRAS protein with
G12C mutation. The inhibitors should
be selective and not bind with HRAS and
NRAS proteins.
– Suggest some small molecules that in-
hibit KRAS G12C - a target responsible
for non-small cell lung cancer.
• Sclerosis:
– Generate molecules for highly efficient
inhibition of Bruton’s tyrosine kinase by
non-covalent binding that is highly selec-
tive for BMX simultaneously.
– Generate high activity tyrosine-protein
kinase BTK inhibitors.
• Parkinson:
– Can you suggest molecules that inhibit
ABL tyrosine-protein kinase with an
LD50 toxicity of 501 mg/kg or more and
a half-life of 4 hours?
– I am writing to you with an important
question. For my work, I need to come
up with 6 new molecules that would have
an affinity to the tyrosine-protein kinase
ABL.
• Dyslipidemia:
– Generate me 2 molecules that would help
me with my blood lipid spectrum disor-
der, which is manifested by an increase in
cholesterol, triglycerides, low and very
low-density lipoproteins, and a decrease
in high-density lipoproteins, or alpha
lipoproteins. It is important that med-
ications do not produce side effects such
as muscle pain and liver problems.
– Can you suggest molecules that inhibit
Proprotein Convertase Subtilisin/Kexin
Type 9 with enhanced bioavailability and
the ability to cross the BBB?
• Drug Resistance:
– Can you suggest molecules that inhibit
signal transducer and activator of tran-
scription 3 (STAT3) with water solubility
greater than 60 mg/mL and inhibitory
ability to P450 CYP1A2?
– Generate me 2 molecules that could
overcome chemotherapeutic resistance
in cancer treatment, specifically target-
ing mechanisms such as increased drug
efflux, enhanced DNA repair, or apop-
tosis evasion. It is important that these
compounds avoid toxicity to healthy cells
and minimize side effects like immuno-
suppression or gastrointestinal distress.
D.1.2
Synthetic Queries Generated by LLMs
• Alzheimer’s disease:
– Generate structures with many aromatic
rings to facilitate π-π stacking interac-
tions with beta-amyloid aggregates.
– Generate compounds with flexible linkers
to allow conformational adaptability in
binding to amyloid-beta.
• Lung cancer:
– Develop potential drug candidates to in-
hibit KRAS G12C in non-small cell lung
cancer, taking into account selectivity
and avoiding cross-reactivity with other
RAS proteins.
– Generate 4 tight-binding inhibitors for
KRAS G12C, making sure they avoid
HRAS and NRAS.
• Sclerosis:
– Formulate novel small-molecule drugs
that disrupt abnormal immune signaling
to slow the progression of multiple scle-
rosis.
– Develop immune-modulating agents to
dampen the overactive immune response
in multiple sclerosis patients selectively.
• Parkinson:
– Synthesize a novel tyrosine hydroxylase
activator with cellular specificity.
– Design a BBB-permeable antioxidant
with mitochondrial tropism.
• Dyslipidemia:


## Page 28

– Create compounds with dual inhibition
of CETP and HMG-CoA reductase to
enhance HDL cholesterol levels.
– Generate ligands that selectively activate
the AMPK pathway to promote fatty acid
oxidation.
• Drug Resistance:
– Generate potential anti-resistance agents
targeting the Ras-Raf-MEK-ERK signal-
ing pathway.
– Design one unique molecular entity that
specifically targets the primary mecha-
nisms of drug resistance in malignant
cells. This compound should exhibit high
binding affinity and selectivity towards
the resistance pathways prevalent in var-
ious cancer types.
D.2
More detailed discussion on LLM-based
solutions
Before comparing MADD with other solutions, we
searched for similar pipelines. However, not all of
them could be directly compared due to significant
differences in pipeline logic, discontinuation of
tools used in the solutions, or lack of available
code accompanying the paper.
• ChemCrow (Bran et al., 2023).
We could not compare ChemCrow with our
approach because the tools integrated into
ChemCrow are no longer supported. There-
fore, it is impossible to show its effectiveness
with our dataset. This information can be
confirmed in the official ChemCrow repos-
itory on GitHub (https://github.com/
ur-whitelab/chemcrow-public). The visu-
alization of the agent architecture is shown in
Figure 15.
• DrugLLM (Liu et al., 2024b).
LLM solves the problems of generation and
optimization of pharmacological molecules
(based on two submitted molecules). It would
be interesting to compare it with it, but the
paper’s authors did not publish the model
weights.
• DrugAgent: Automating AI-aided Drug Dis-
covery Programming through LLM (Liu et al.,
2024a).
DrugAgent addresses three key challenges
in drug development: predicting ADMET
properties, predicting drug-target interactions
(DTIs), and molecular optimization. It fo-
cuses on small-molecule drugs, which account
for more than 90% of approved drugs. This
solution was hard to match, as it does not gen-
erate molecules from scratch according to the
given properties. It optimizes the proposed
one because the authors have not made the
code freely available.
• DrugAgent: Explainable Drug Repurposing
Agent with Large Language Model-Based
Reasoning (Inoue et al., 2024).
There is no molecule generation here; instead,
only chemical databases are searched. The
proposed predictive tools cannot identify the
targeting molecule requested in our dataset
examples. Therefore, no comparison was per-
formed.
• Large Language Models Open New Way of
AI-Assisted Molecule Design for Chemists
(Ishida et al., 2024).
This agent system has only one scenario: user
data preparation and model refinement us-
ing the AutoML block, running a generative
model. The system’s logic is very different
from MADD’s to make a comparison.
• Leveraging Chemistry Foundation Models to
Facilitate Structure-Focused Retrieval Aug-
mented Generation in Multi-Agent Workflows
for Catalyst and Materials Design (Park et al.,
2024).
The system can optimize a user’s molecule by
blog radar searching for a similar embedding
molecule in a database and updating the origi-
nal molecule to reflect the found one. Unfor-
tunately, the authors did not publish the code.
The visualization of the agent architecture is
shown in Figure 14.
D.3
More detailed description of the tools
D.4
Properties prediction algorithms
D.4.1
IC50
IC50 is the concentration of a substance required to
inhibit a biological process by 50%. Data from
ChEMBL and BindingDB were used to create
machine-learning models for predicting the effi-
cacy of inhibitors of GSK-3β, BTK, and ABL2. In
the case of BTK inhibitors, the data were supple-
mented from a recent paper (Li et al., 2024a) that


## Page 29

also utilizes ML for this task. The original data
set was presented as molecules in SMILES for-
mat and IC50 values (nmol/L). The necessary data
pre-processing was performed in each case, includ-
ing data normalization and duplicate removal. The
IC50 prediction task was formulated as a binary
classification. The molecules in the data set were di-
vided into two classes by the median of the lgIC50
distribution. Thus, molecules with lgIC50 less than
the median were defined as “active” and all others
as “inactive”. The structures of the molecules were
represented in various ways, particularly Morgan
fingerprints, Avalon, and RDKit descriptors.
D.4.2
Docking score
We calculated docking scores for the disease-
specific target proteins using AutoDock Vina (Eber-
hardt et al., 2021) and QuickVina GPU 2.1 (Tang
et al., 2024) frameworks. The latter allowed us
to significantly reduce the time required for dock-
ing score calculations, averaging just 0.14 seconds
compared to 5 seconds with AutoDock Vina. As a
result, the total time needed to calculate molecular
docking scores for our dataset dropped from 1667
hours to 19 hours.
D.5
Justification of the choice of filter groups
The filtering groups have the following structure:
• Group 1 (GR1): Docking score ≤-7 and
IC50 = 1
• Group 2 (GR2): SA score ≤3
• Group 3 (GR3): Brenk = 0
• Group 4 (GR4): SureChEMBL = 0, Glaxo =
0, and PAINS = 0
• Group 5 (GR5): QED > 0.6
Group 1 (GR1): This is the main group of fil-
ters that considers the biological activity of the
generated molecules, the properties of which are
proposed to be used as a primary focus.
Group 2 (GR2): Here, filtering by the possibil-
ity of synthesizing substances (SA) to the filters in
the first group. This level of filtering additionally
shows how many of the generated molecules can
potentially be synthesized.
Group 3 (GR3): The Brenk filter removes
molecules that contain substructures with undesir-
able pharmacokinetics or toxicity.
Group 4 (GR4): SureChEMBL is a publicly
available resource containing compounds extracted
from patent documents. Glaxo filters are designed
to exclude unstable and other problematic com-
pound classes. Pan-assay interference compounds
(PAINS) are chemical compounds that often give
false positive results in high-throughput screens.
PAINS react non-specifically with numerous bio-
logical targets, often leading to side effects.
Group 5 (GR5): The most stringent group in
terms of filtering includes restrictions on the QED
property. Thus, when requiring the inclusion of an
assessment of molecules by drug similarity, it is
necessary to focus on the fifth group.
D.6
Analyzed disease cases
D.6.1
Alzheimer’s disease
Currently, there are no medications that entirely
prevent or halt Alzheimer’s disease (AD). Existing
drugs only reduce symptoms. Tau proteins play
a role in stabilizing microtubules, which maintain
the healthy state of neurons (Buerger et al., 2006).
In a healthy brain, tau proteins undergo phosphory-
lation and dephosphorylation, processes regulated
by various kinases. Glycogen synthase kinase-3
(GSK-3β) is a serine/threonine kinase that plays a
key role in cellular metabolism and signal transduc-
tion. It is associated with various diseases, includ-
ing AD, by promoting tau protein hyperphospho-
rylation, a significant component of neurofibrillary
tangles, one of the hallmarks of AD. One of the
inhibitors of this kinase, tideglusib, has completed
phase I and II clinical trials, during which it was
found that cognitive function in patients improved
slightly compared to placebo (insufficient efficacy),
and gastrointestinal side effects (toxicity) were ob-
served (Domínguez et al., 2012). Thus, developing
novel GSK-3β inhibitors with enhanced properties
is of great importance.
D.6.2
Multiple sclerosis
Multiple sclerosis (MS) is a chronic autoimmune
disorder affecting the central nervous system, char-
acterized by inflammation, demyelination, gliosis,
and neuroaxonal degeneration (McGinley et al.,
2021). While it is traditionally thought that MS is
primarily mediated by T-cells, B-cells and almost
all types of innate immune cells appear to play a
significant role in both the initiation and propaga-
tion of the disease. Peripheral immune cells that
cross the blood-brain barrier (BBB) induce relapses
and the formation of focal demyelinating plaques
(Cencioni et al., 2021). Bruton’s tyrosine kinase
(BTK) is a protein that plays a critical role in the de-


## Page 30

velopment and function of immune cells. The use
of BTK inhibitors for treating MS is a promising
area of research, as these drugs have been shown to
reduce B-cell activity and decrease inflammation
in the brain and spinal cord (Krämer et al., 2023).
By targeting BTK, these drugs may slow or halt
the progression of MS, improve symptoms, and en-
hance the quality of life of patients (Li et al., 2022).
Currently, at least six BTK inhibitors (BIIB091,
Evobrutinib, Fenebrutinib, Orelabrutinib, Remibru-
tinib, Tolebrutinib) are in phase II-III clinical trials.
Despite promising results, there are still areas for
improvement in BTK inhibitors, such as binding
mechanism (non-covalent inhibitors are less potent
and require higher doses, but they offer increased
selectivity and a lower propensity for resistance)
and blood-brain barrier permeability (Saberi et al.,
2023). This case aims to generate noncovalent BTK
inhibitors with improved IC50 values and enhanced
BBB permeability.
D.6.3
Parkinson’s disease
Parkinson’s disease is a progressive neurodegener-
ative disorder, which is characterized by the loss
of dopaminergic neurons (Tolosa et al., 2021). The
primary causes and mechanisms of development
include mitochondrial dysfunction, oxidative stress,
genetic mutations, protein manifolding and aggre-
gation, and disruptions in cellular clearance pro-
cesses. These factors contribute to neuronal degen-
eration and make them key targets for therapeutic
approaches. Two primary targets are being inves-
tigated for treating Parkinson’s disease: tyrosine-
protein kinase ABL and catecholamines. Inhibi-
tion of ABL is considered a promising approach
to slowing neurodegenerative processes (Werner
and Olanow, 2022). This protein kinase regulates
cellular metabolism, and its hyperactivation is as-
sociated with increased oxidative stress and the ac-
cumulation of damaged proteins, which contribute
to neuronal death (Kwon et al., 2021). This case
study focuses on generating new ABL inhibitors
with improved properties.
D.6.4
Drug resistance
STAT3 (Signal Transducer and Activator of Tran-
scription 3) is a pivotal regulator in cancer biol-
ogy, associated with cell proliferation, survival, and
immune evasion, making it an attractive pharma-
ceutical target (Yu et al., 2014). Aberrant STAT3
signaling, driven by overexpression or activation
through factors like LIF, CDK1/9, and miRNAs,
is implicated in various cancers, with emerging
studies uncovering additional mechanisms and tu-
mor types linked to its dysregulation (Shi et al.,
2019; Kuang et al., 2019). While several STAT3
inhibitors are in clinical or preclinical development,
challenges such as toxicity, adverse reactions, and
limited selectivity persist, constraining their ther-
apeutic potential (Beebe et al., 2018). Addressing
these issues requires discovering highly selective
agents that spare healthy cells and identifying pre-
dictive biomarkers to stratify responsive patients
(Zhao et al., 2016). The promising strategy of com-
bining STAT3 inhibitors with RTK-targeting ther-
apies could further mitigate drug resistance. Con-
tinued exploration of STAT3’s biological role and
crosstalk with other signaling pathways is expected
to drive innovations in targeted cancer treatments.
D.6.5
Dyslipidemia
Dyslipidemia, a major contributor to atheroscle-
rotic cardiovascular disease (ASCVD), has driven
the search for innovative therapeutic targets to
achieve effective lipid regulation (Stone et al.,
2014). Among the most promising approaches
are ATP citrate lyase (ACL) inhibitors and pro-
protein convertase subtilisin/kexin type 9 (PCSK9)
inhibitors (Berkhout et al., 1990; Ballantyne et al.,
2016; Cohen et al., 2006). Existing ACL inhibitors,
such as bempedoic acid, have demonstrated effi-
cacy in LDL cholesterol (LDL-C) reduction, yet
their reliance on overlapping mechanisms with
statins can limit their additive effect (Bilen and
Ballantyne, 2016). Developing new ligands with
unique binding properties may increase efficacy,
reduce side effects, and improve compatibility with
other lipid-lowering drugs. Similarly, monoclonal
antibodies against PCSK9, including everocumab
and alirocumab, have demonstrated significant clin-
ical success but have problems with these drugs’
cost-effectiveness and long-term safety.
(Iqbal
et al., 2019). These novel ligands could provide
more accessible, durable, and convenient treatment
options, enabling broader application across di-
verse patient populations and addressing unmet
needs in dyslipidemia management.
D.6.6
Lung cancer
KRAS is a protein that helps control cell growth by
acting like an on/off switch. When KRAS has spe-
cific mutations, it gets stuck in the “on” position,
causing uncontrolled cell growth without external
signals (Isermann et al., 2024). These mutations are


## Page 31

widespread in non-small cell lung cancer, which
makes up over 80% of all lung cancer cases (Reck
et al., 2021). New drugs, like KRAS G12C in-
hibitors (such as sotorasib and adagrasib), have
shown promise. Still, they have some drawbacks,
including working only for some patients, causing
unintended side effects, and leading to resistance
over time (Kim et al., 2023). This case study fo-
cuses on developing better KRAS inhibitors with
improved effectiveness.
D.7
Additional results analysis of generated
molecules
D.7.1
Detailed evaluation: Alzheimer’s
disease
In addition to evaluating individual molecular prop-
erties, we comprehensively analyzed the drug can-
didates generated through our benchmark. Due to
the limited volume, we present detailed validation
results for Alzheimer’s disease cases only.
16,082 novel GSK-3β inhibitors were gener-
ated using the transformer model.
To validate
generated molecules with already known com-
pounds, we compared novel inhibitors with active
inhibitors from the ChEMBL dataset, which was
used to create the IC50 prediction model (see Fig-
ure 5). The average SA Score of the generated
molecules is lower than that of experimentally val-
idated compounds, suggesting easier laboratory
synthesis methods. Moreover, the average QED
score of generated molecules increased 11.8%,
which indicated enhanced pharmacological prop-
erties. Lower toxicity can also be reported since
all the generated molecules have passed the Brenk
filter. At the same time, the Tanimoto similarity
of 0.43 between novel and ChEMBL molecules
leads to the conclusion that, along with improved
properties, the obtained compounds make up a dif-
ferent chemical space, which can potentially result
in unconventional and effective solutions for this
case (Ganeeva et al., 2024a,b).
D.8
Formulas for calculating agent pipeline
metrics:
D.8.1
Formula for calculating the
Orchestrator accuracy
OA = Number of Correct Tools
Number of Tools
(%)
D.8.2
Formulas for calculating the accuracy
per query as a whole
Final Accuracy (FA, %) was measured at several
steps. It was calculated separately after tool selec-
tion and separately for summarization. Then, the
probabilities were multiplied, and the final accu-
racy was obtained.
TS = Number of Correct Queries
Total Number of Queries
SSA = Correct Responses
Total Responses
Final Accuracy = TS × SSA(%)
Here, “Correct Response” refers to a response
in which no molecules or property values are lost,
while “Correct Query” refers to a case in which the
tool was defined correctly.
D.9
Schemes of the considered agent
architectures
A visualization of the 2-agent and 3-agent systems
is shown in Figure 12 and Figure 13.


## Page 32

D.10
MADD failure cases examples
As discussed in the main body of the paper, our
experiments demonstrate that the system achieves
an accuracy of 79.8% on complex queries. This
result significantly outperforms competing systems
(e.g., ChemAgent, which attains a maximum ac-
curacy of 16.4%). In Sections 4.2–4.6, we outline
the key criteria explaining why other LLM-based
approaches and alternative multi-agent systems are
prone to higher error rates.
However, researchers interested in applying our
methodology may require a more detailed analysis
of the limitations inherent to MADD-like systems,
which can lead to erroneous outcomes.
The generative model may occasionally produce
molecules unsuitable for target properties, as the
probability of generating hit molecules is not abso-
lute. This probability was quantified as a percent-
age across filter groups GR1–GR5, reaching 18.3%
for our custom-developed models integrated into
MADD. However, this issue is mitigated by incor-
porating validation methods within the multi-agent
system, including iterative resampling of genera-
tive tools until user-specified criteria are met. To
ensure transparent evaluation of the system’s base-
line performance, all experiments were conducted
without this validation function.
Generated molecules may exhibit low synthetic
feasibility despite rigorous validation for chemi-
cal validity, drug-likeness, and target protein activ-
ity. Nevertheless, such occurrences are statistically
rare, and recent studies (e.g., [Atz, Kenneth, et
al. Nature Communications 15.1 (2024): 3408])
demonstrate that computational filtering methods
can reliably prioritize synthetically tractable candi-
dates for laboratory synthesis.
Most errors in the agent system occur at the step
of decomposing the input query into tasks, that
is, during the invocation of the Decomposer agent.
But there are also errors when selecting the right
tool in the Orchestrator agent.
Example №1:
Initial query from M dataset:
“Generate small molecules that specifically in-
hibit the KRAS G12C mutation. Ensure they do
not interact with HRAS and NRAS proteins. For-
mulate novel small-molecule drugs that disrupt ab-
normal immune signaling to slow the progression
of multiple sclerosis.”
Decomposer identifies the following tasks:
• “Generate small molecules that specifically
inhibit the KRAS G12C mutation.”
• “Ensure they do not interact with HRAS and
NRAS proteins.”
• “Formulate novel small-molecule drugs that
disrupt abnormal immune signaling to slow
the progression of multiple sclerosis.”
True tasks:
• “Generate small molecules that specifically
inhibit the KRAS G12C mutation. Ensure
they do not interact with HRAS and NRAS
proteins.”
• “Formulate novel small-molecule drugs that
disrupt abnormal immune signaling to slow
the progression of multiple sclerosis.”
Thus, the Decomposer creates 1 invalid task, which
will be further processed by the Orchestrator agent,
and will make an extra call of tool.
Example №2:
Initial query from M dataset:
• “Generate ligands that selectively inhibit the
FTO enzyme to influence energy balance and
lipid levels.”
Decomposer correctly selects the following task:
• “Generate ligands that selectively inhibit the
FTO enzyme to influence energy balance and
lipid levels”
Then the task goes to the Orchestrator agent, and
it makes a mistake by choosing a model for gen-
erating a drug molecule for the treatment of Drug
Resistance, instead of treating Dyslipidemia.
Decomposer errors are less critical, as users will
ultimately receive correct answers—albeit with one
additional response to a nonexistent query. In con-
trast, Orchestrator errors are significantly more crit-
ical. However, we can assure you that the correct
selection rate is high, making such errors rare. To
address your request about reducing agent errors’
impact on pipeline quality, we implemented a fea-
ture using LLM-driven answer reflection. This
can mitigate errors, even for complex questions.
Notably, queries from Datasets M and L are inher-
ently more complex, whereas our system handles
simpler queries from Dataset S with far greater
ease. As shown in Table 5, pipeline performance
declines with increasing query complexity: Final


## Page 33

Accuracy is 86.9% for Dataset S but drops to 79.8%
for Dataset L.
Notably,
no
existing
LLM-based
system
achieves absolute reliability. For instance, our anal-
ysis of the Phoenix model (Future House) revealed
critical vulnerabilities:
• SMILES Processing Failure: When designing
an ACC2 inhibitor, the system initiates correct
reasoning but generates an invalid SMILES
string containing two disconnected molecules.
Despite the "No modifications found" error, it
repeats the request without correction.
• Invalid
Structure
Generation:
A
modified
SMILES
string
(C1CCC2(CC1)CCCC2=OCCN3C=NC=CC3=O)
passes basic checks but contains trivalent
oxygen (RDKit valence error).
This high-
lights how insufficient validation can yield
formally "correct" but chemically impossible
outputs—a key limitation of multi-agent
architectures (see Appendix C2.1-C2.2 for
edge-case analyses of competing systems).
The described edge cases demonstrate that even
advanced Multi-Agent Cognitive (MAC) systems,
such as MADD, may occasionally misinterpret user
queries. However, MADD exhibits significantly
higher reliability than alternatives, with pipeline
errors being sporadic. Extended error case studies
will be provided in the Appendix of the final article
to facilitate a deeper technical understanding of
MADD-like systems.
We would like to draw the reviewer’s attention
to the fact that agentic solutions is a rapidly evolv-
ing field. To our knowledge, MADD is the first
of its kind multi-agent solution that effectively ad-
dresses the problem of hit molecule identification
in a fully automated way, as multiple case studies
in our paper show. It is not free of practical restric-
tions at this moment, but we are actively working
to address them. We intend to have released several
major updates by the paper decision date, such as
a dataset collection feature, logging exports, im-
proved UI, and others. Therefore, we would like to
ask the reviewer to consider increasing the score to
support our ambitious effort and help promote the
work of our team.
E
Agents prompts and examples of
multi-agent system workflow
E.1
Results of system workflow.
An example of a user dialogue with the MADD
is shown in Figure 17. Note that the response is
structured and divided into paragraphs depending
on the number of requests in one message.
E.2
Examples of prompts extracts for the
main agents in the system.
The Orchestrator agent is the main agent that
calls functions corresponding to other agents and
tools. Our system receives two dictionaries stored
in JSON format. The first dictionary contains infor-
mation about available functions, e.g., for calling
existing generative models or training new genera-
tive models. The second dictionary contains infor-
mation about generative models that have check-
points and are currently available for inference.
The prompt for the orchestrator, therefore, con-
sists of several parts. First, the agent receives basic
instructions on function calling:
”You are a orchestrator with tool calling capabilities.
When you are using tools, respond in the format
{"name": function name, "parameters": dictionary
of function arguments}...”
The agent then receives a dictionary containing
the available tools with instructions for each and
descriptions of their arguments. Dictionary with
the description of one of the tools:
”name: gen_mols,
description:
Generate molecules by gener-
ative models.
Only use this function if the
user asks to generate molecules for cases
with already available generative models that
can be found in a special dictionary AVAIL-
ABLE_TRAINED_GEN_MODELS. If the user
wants to generate molecules for another case you
should train new model.,
arguments:
name: case,
type: str,
description: Name of the case same as in AVAIL-
ABLE_TRAINED_GEN_MODELS dictionary, for
example ’Cnsr’ - generation of molecules for the
treatment of lung cancer.


## Page 34

name: num,
type: int,
description: Number of molecules for a generation.
name: model,
type: str,
description:
Model for generation, can be:
’CVAE’, ’LSTM’, ’RL’, ’GraphGA’, each model
can be used depending on the case, default model
’CVAE’. ”
Another example of a dictionary with a tool de-
scription:
”name: train_gen_models,
description:
Train a generative model with a
custom dataset (use if the user requests gen-
eration for a case that is not presented in the
current generative models dictionary AVAIL-
ABLE_TRAINED_GEN_MODELS),
arguments:
name: model,
type: str,
description: Model for finetuning for specific case.
Available: ’RL’, ’CVAE’, ’LSTM’, ’GraphGA’.
Default model ’CVAE’,
name: epoch,
type: int,
description: Number of train epochs.
Default
value 100,
name: case_name,
type: str,
description: The name of the disease for which
the model will be trained (in the future, the user
will ask for inference using this name). Default to
’Short word for user’s prompt’, ”
After all tools have been defined, the orchestra-
tor receives a dictionary that contains information
about all trained and available generative models
for inference. This dictionary is automatically up-
dated when generative model training is used. Ex-
ample of a dictionary for the case of a trained model
for Alzheimer’s disease:
”name: Alzhmr,
description: Generation of drug molecules for
the treatment of Alzheimer’s disease. GSK-3beta
inhibitors with high activity. These molecules can
bind to GSK-3beta protein, molecules has low
brain-blood barrier permeability."
The agent is then given more general instructions
on interacting with the user and prioritizing calling
different agents and tools. The Orchestrator then
receives the following examples of requests and
expected responses:
” Query from user: What can you do?
You:
{’name’:
’make_answer_chat_model’,
’parameters’: {’msg’: ’What can you do?’}}
Query from user:
Generate highly potent
non-covalent BTK tyrosine kinase inhibitors from
the TEC family of tyrosine kinases that have the
potential to affect B cells as a therapeutic target
for the treatment of multiple sclerosis.
# If you have trained generative model for
this case and you recognize from description
that you have this generative model in AVAIL-
ABLE_TRAINED_GEN_MODELS
You: {’name’: ’gen_mols’, ’parameters’: {’case’:
’Sclrerosis’, ’num’: 1}}
Query from user:
Suggest several molecules
that have high docking affinity with KRAS G12C
protein.
# If you don’t have trained generative model
for this case and you do not see it in AVAIL-
ABLE_TRAINED_GEN_MODELS
You: {’name’: ’train_gen_models’, ’parameters’:
{’model’: ’CVAE’, ’epoch’: 100, ’case_name’:
’Cancer’}}
”
Full prompts can be found in the reposi-
tory
(https://github.com/ITMO-NSS-team/
MADD-CoScientist/blob/main/MADD/mas/
prompts/prompts.py).
E.3
Examples of Agents decision. System
Logs.
Full
logs
can
be
found
at
link
(https:
//github.com/ITMO-NSS-team/MADD/blob/
main/examples/logs_example.txt)
For clarity, all steps are separated by the separa-
tor:
—new step—
Let’s examine step by step. Initially, the state


## Page 35

contains only input data:
’input’:
’Generate molecule of GSK-3beta
inhibitors with high docking score’
After calling the Decomposer agent, the Orches-
trator is called with a task:
’plan’:
[’Generate molecule of GSK-3beta
inhibitors with high docking score’]
The Orchestrator select tool for generation:
TOOL: gen mols alzheimer, ’num’: 1
Molecule generation results:
1
{’Smiles ’: {’0’: ’OC12C3C=C(Br)C4=
NCC5(CN6CC65)C4N1CC2CC3 ’,
2
’1’: ’COC1(OC)C2C3C4CCN(
C#N)C3C41c1nncn12 ’},
3
’Brenk ’: {...}, ’QED ’: {...}, ’
Synthetic Accessibility ’:
{...},
4
’LogP ’: {...}, ’Polar Surface Area
’: {...}, ’H-bond Donors ’:
{...},
5
’H-bond Acceptors ’: {...}, ’
Rotatable Bonds ’: {...},
6
’Aromatic Rings ’: {...}, ’Glaxo ’:
{...}, ’SureChEMBL ’: {...},
7
’PAINS ’: {...}, ’Validity ’: {...} ,
’Duplicates ’: {...},
8
’docking_score ’: {...}, ’IC50 ’:
{...}}
FINAL ANSWER: Generated Molecules for GSK-
3beta Inhibitors with High Docking Score
• Molecule 1:
– SMILES:
OC12C3C=C(Br)C4=NCC5(CN6CC65)C4N1CC2CC3
– Docking Score: -6.36
• Molecule 2:
– SMILES:
COC1(OC)C2C3C4CCN(C#N)C3C41c1nncn12
– Docking Score: -6.79
E.4
Examples of Agent systems/LLMs
answers
E.4.1
MADD answers
Examples of the dialogue with MADD are shown
in Figures 17-18.
• Decomposer
– Input: Generate GSK-3β inhibitors with
high docking score and low blood-brain
barrier permeability. Generate inhibitors
of KRAS protein with G12C mutation.
The inhibitors should be selective, mean-
ing they should not bind with HRAS and
NRAS proteins.
– Response: {"steps":
[["Generate
GSK-3β
inhibitors
with
high
docking score and low blood-brain
barrier
permeability.",
"Generate
inhibitors
of
KRAS
protein with G12C mutation. The
inhibitors should be selective,
meaning
they
should
not
bind
with HRAS and NRAS proteins"]]}
• DataGathering
– Input:
Execute the following task:
Download data from ChEMBL for GSK
with IC50 values using the available
tools.
– Response:
"Found
653
entries
for
GSK.
Saved
to
MADD/ds/molecules_GSK.csv"
E.4.2
ChemAgent answers
• Question 1: “Can you suggest molecules that
inhibit ABL tyrosine-protein kinase with an
LD50 toxicity of 501 mg/kg or more and a
half-life of 4 hours?”
• Response 1: “Dasatinib is suggested as an
ABL tyrosine-protein kinase inhibitor that
closely fits the required half-life of around
4 hours and is generally well-tolerated, al-
though specific LD50 values at 501 mg/kg may
not be precisely documented.” (no molecules
provided).
• Question 2: “I am writing to you with a
very important question. For my work, I need
to come up with new molecules that would
have an affinity to the protein ATP citrate syn-
thase.”
• Response 2: “I have generated several candi-
date molecules with potential affinity for ATP
citrate synthase. Based on iterative genera-
tion processes, here are two original promis-
ing leads in SMILES format:
1) Fourth Molecule: UNVALID MOLECULE


## Page 36

2) Sixth Molecule: UNVALID MOLECULE
For further exploration, you might consider
using combinatorial chemistry techniques or
computational methods to derive more poten-
tial candidates, ensuring a more extensive
structure-based approach for targeting ATP
citrate synthase effectively.”
E.4.3
LlasMol answers
• Question 1:
“Generate therapeutic com-
pounds that can potently inhibit Bruton’s ty-
rosine kinase through non-covalent interac-
tions”
• Response
1:
“
CN1C(=O)C2=C(N=CN2CC(O)CO)N(C)C1=O
.”
• Question 2: “Develop innovative bile acid
sequestrants to decrease intestinal cholesterol
absorption.”
• Response 2: “Given the following reactants
and reagents, write a possible product.
C1CCOC1.CC(C)(C)OC(=O)N1CCC(C(=O)
O)CC1.CCN(CC)CC.CCOC(=O)Cl.CN(C)C
=O.O.[N-]=[N+]=[N-].[Na+]
CC(C)(C)OC(=O)N1CCC(N)CC1”
E.4.4
X-Lora-Gemma answers
• Question 1: “Develop kinase-binding agents
that specifically inhibit Bruton’s tyrosine ki-
nase for therapeutic use in multiple sclerosis.”
• Response 1: “
Developing Kinase-Binding Agents for MS: A
Molecular Engineer’s Journey
Initial Thoughts:
..
Final Answer:
The final molecular formula for the optimized
kinase-binding agent is:
C1C2OC1C21OC1C=O
This compound exhibits high binding affinity
and specificity for Btk. It also displays good
solubility and stability in aqueous solutions,
making it a promising candidate for therapeu-
tic use in MS.
Conclusion:
In summary, I have developed a kinase-
binding agent that specifically inhibits Btk for
therapeutic use in MS. By combining struc-
tural analysis, molecular modeling, and sim-
ulation techniques, I was able to overcome
the challenges associated with designing se-
lective inhibitors for this complex kinase. This
drug candidate has the potential to provide
a novel and effective treatment for MS, with
minimal off-target effects and improved pa-
tient outcomes.”
• Question 2: “Generate 5 small molecules
that selectively inhibit KRAS G12C mutation,
without interacting with HRAS or NRAS.”
• Response 2: “
Intermediate thoughts:
...
Final answer:
C1NC1(C=O)C1C2CN1C2O
This molecule is a small molecule that selec-
tively inhibits the KRAS G12C mutation with-
out interacting with HRAS or NRAS. It targets
the G-box of KRAS G12C, preventing its in-
teraction with GTP and subsequent activation.
This approach aims to block the intrinsic activ-
ity of KRAS G12C and prevent its downstream
signaling events.”
E.4.5
Phoenix answers
• Question 1: “Generate derivatives that incor-
porate multi-targeted inhibition to address the
complex mechanisms underlying Alzheimer’s
disease.”
• Response 1: “
Multi-Target Therapeutics for Alzheimer’s: A
Polypharmacological Strategy
Initial Thoughts:
..
Final Answer:
Here are two novel derivatives designed
for multi-targeted inhibition in Alzheimer’s
disease, based on well-established pharma-
cophores: Donepezil derivative (AChE inhi-
bition + potential amyloid/oxidative modula-
tion):
COc1cc2c(cc1OCCCN1CCCC1)CC(CC1
CCN(Cc3ccccc3)CC1)C2=O
Functional groups:
tertiary amine (for
AChE inhibition), ketone, ether, aromatic
ring, alkylarylether. Rationale: Maintains
cholinesterase inhibitor features and side-
chain flexibility for additional targeting. Cur-
cumin derivative (anti-amyloid + antioxidant
+ potential AChE interaction):
COc1cc(/C=C/C(=O)CC(=O)/C=C/c2ccc


## Page 37

(O)c(O)c2)ccc1O
Functional groups:
1,2-diphenol (antioxi-
dant), conjugated trans and cis double bonds
(amyloid aggregation inhibition), aromatic
rings, phenol. Rationale: Designed for antiox-
idation and amyloid-beta inhibition, with aro-
matic/phenolic groups offering multi-modal
targeting. These structures can inspire further
optimization for polypharmacology, aligning
with complex pathological mechanisms in
Alzheimer’s disease.
Conclusion:
In summary, the proposed donepezil and
curcumin derivatives exhibit multifunctional
pharmacophoric features that support inhibi-
tion of AChE, reduction of oxidative stress,
and interference with amyloid-beta aggrega-
tion. These structures are aligned with the
principles of polypharmacology and offer a
strong starting point for further development
into next-generation Alzheimer’s therapeutics
with improved efficacy and disease-modifying
potential.”
• Question 2: “Design novel small molecule
inhibitors
targeting
P-glycoprotein
(P-
gp/ABCB1) efflux pumps to reverse multidrug
resistance.”
• Response 2:
Initial Thoughts:
To design novel small
molecule inhibitors of P-glycoprotein (P-gp
or ABCB1) to reverse multidrug resistance
(MDR), I should first propose potential
small molecule binders targeting P-gp, using
knowledge from literature and computational
predictions. Then, I’ll focus on novel struc-
tures rather than known ones, and ensure
drug-like properties. The target gene symbol
for P-glycoprotein is ABCB1.
...
Final answer:
Here are five optimized
molecular structures predicted to inhibit
P-gp:
Derivative
1:
COc1ccc(C(C#N)(CCCN(C)CCc2ccc
(OC3CC3)c(OC)c2)C(C)C)cc1OC
Derivative
2:
O=c1cc(-
c2ccc(OC(COCNCc3ccc(CO)cc3O
)cc3)o2)oc2ccccc12
..
These compounds are predicted to bind and
block P-gp efflux channels, thereby enhancing
intracellular drug retention and reversing
resistance in cancer cells.
E.4.6
ChemDFM answers
• System prompt ”You are an expert chemist.
Given the molecular requirements description,
your task is to design a new molecule using
your experienced chemical Molecular Design
knowledge. Please strictly follow the format,
no other information can be provided. You
should only reply with SMILES string nota-
tions to represent the generated molecule. The
SMILES must be valid and chemically reason-
able.”
• Question 1: “Generate 6 compounds that
selectively bind and inhibit KRAS G12C, en-
suring no effect on HRAS or NRAS proteins.”
• Response
1:
“C=CC(=O)N1CCN(c2nc(=O)n(-
c3c(C)ccnc3C)c(=O)n2C)C(C(=O)OC)C1”
• Question 2: “Develop selective tyrosine ki-
nase inhibitors with strong binding affinity for
BTK.”
• Response 2: “CC(C)N1CC/C(=C2¸ccc3oc(-
c4ccccc4)nc3c2)C1“
E.5
Analyzing the effectiveness of the agent’s
generative model selection for training.
In addition to the user query, the generative model
training agent also received information about the
performance of different generative model architec-
tures depending on the case. This data was based
on Table 8 and was intended to help choose the op-
timal architecture for generative model training for
a new case. Example of additional prompt with in-
formation on different architectures’ performance
in case of CVAE model:
’CVAE’ - method shows the following metrics
depending on the choice of case (disease):
Filter group
1 2 3 4 5 Diversity
Alzheimer 26,1% 23,6% 18,5% 18,2% 18,2% 0,2
Sclerosis 15,4% 13,8% 13,3% 13,3% 13,3% 0,3
Lung cancer
6,1% 5,7% 5,0% 4,8% 4,8% 0,8
Drug resistance 8,3% 6,9% 6,1% 6,1% 6,1% 0,8


## Page 38

Dyslip. 28,9% 28,3% 25,1% 24,5% 13,2% 0,2
Parkinson
3,3% 3,1% 2,7% 2,7% 2,7% 0,2;
As a result, our dataset S agent system had a
97.4% probability of selecting either the best model
in the considered disease or the Transformer model,
which has shown to be optimal in many cases.
When the agent was given the results without
naming the diseases and asked to create a drug for a
new disease, the agent chose MTDD-EF and Trans-
former with approximately equal probabilities of
39.4% and 38.2%, respectively. The agent chose
other proposed generative solutions in the remain-
ing 22.4% of cases. Thus, applying this approach
allows the automatic selection of a generative tool,
in the best case, with almost a 40% probability of
selecting the best one. In the worst case, the agent
searches through all available tools until it finds
the best one. As a result, this method is better than
random search and removes the requirement for the
user to understand the available generative tools,
thus simplifying and automating the human task.
F
Licenses of used artifacts
We used data from an article (SYK (Zavadskaya
et al., 2025)) written under the CC BY-NC-ND
4.0 license. ChemAgent is released under the MIT
License, which allows for free use, modification,
and distribution, as long as proper attribution is
given.
ChemCrow is also licensed under the MIT Li-
cense, providing broad permissions for reuse with
minimal restrictions.
LlaSMol is distributed under the Creative Com-
mons Attribution 4.0 International (CC BY 4.0)
license. This requires users to give appropriate
credit and indicate if changes were made, even in
derivative works.
X-LoRA-Gemma is available under the Apache
License 2.0, which permits commercial use, modi-
fication, and redistribution, provided copyright no-
tices and the license text are retained.
DeepSeek (including DeepSeek-Coder and
DeepSeek-VL) is licensed under the MIT License,
offering wide flexibility for research and commer-
cial applications.
RDKit, a cheminformatics toolkit, is released
under the BSD License, a permissive open-source
license allowing unrestricted use, modification, and
distribution, including in proprietary software.
ChEMBL, a large-scale bioactivity database, is
provided under the Creative Commons Attribution-
ShareAlike 3.0 (CC BY-SA 3.0) license. Users
must credit the source and distribute derivative
work under the same license.
ChemDFM-13B: GNU Affero General Public
License v3.0, ChemLLM-7B-Chat: Apache license
2.0, Mistral-7B: Apache license 2.0, LLaMA-3.1-
8B: Llama 3.1 Community License.
AutoDock Vina GPU 2.1 : Apache license 2.0


## Page 39

Figure 8: The correlation matrix between Dataset S queries.
×10-3
Figure 9: Comparison of the accuracy (in %) and cost (in USD) of agent pipelines for different models and system
prompts.


## Page 40

Figure 10: Our CVAE transformer architecture
Figure 11: Comparison of the molecular mass of experimentally validated molecules (“Real”) with generated
molecules using MTDD-EF and integrated models for: a) Alzheimer’s disease, b) Multiple sclerosis.


## Page 41

Figure 12: Visualisation of MADD-v2A and MADD-v3 systems.
Summarizer
Decomposer
Decomposer-
Orchestrator
Summarizer-
Orchestrator
Tools
Tools
Query
Query
Answer
Answer
a
b
Figure 13: Visualisation of MADD-v2C and MADD-v2B systems.
Figure 14: MolFormer. Schematic representation of the system.


## Page 42

Safety Tools
Reactions Tools
Molecular Tools
General Tools
Query
Central
ReAct Agent
WebSearch
Name2Smiles
RXN2Name
Smiles2Cas
Synthesis Plan
Safety Assessment
PatentCheck
Python REPL
Human Expert
LitSearch
Smiles2Price
RXN Predict
Similarity
Synthesis Execute
Explosive Check
Functional Groups
Modify Molecule
Smiles2Weight
Figure 15: ChemCrow. Schematic representation of the system.
Figure 16: ChemAgent. Schematic representation of the system.


## Page 43

Figure 17: Question-answer for MADD. The question from Dataset L (hardest)
Figure 18: Question - answer for MADD. The question from Dataset M (medium difficulty).
