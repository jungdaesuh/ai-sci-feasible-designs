=== Source Code Consolidation ===


================================================================================
File: planner.py
================================================================================

"""Agentized planning helper for Phase 3 (docs/roadmap & improvement plan guidance)."""

from __future__ import annotations

import hashlib
import json
from dataclasses import asdict
from pathlib import Path
from typing import Any, Mapping, Sequence

from ai_scientist import agent as agent_module
from ai_scientist import config as ai_config
from ai_scientist import tools
from ai_scientist import tools_api
from ai_scientist import rag


class PlanningOutcome:
    """Structured output that mirrors the JSON sections sent to the planning agent prompt."""

    def __init__(
        self,
        context: Mapping[str, Any],
        evaluation_summary: Mapping[str, Any],
        boundary_summary: Mapping[str, Any],
        rag_snippets: Sequence[Mapping[str, str]],
    ) -> None:
        self.context = context
        self.evaluation_summary = evaluation_summary
        self.boundary_summary = boundary_summary
        self.rag_snippets = rag_snippets


def _serialize_summary(summary: tools.P3Summary | None) -> Mapping[str, Any] | None:
    if summary is None:
        return None
    return {
        "hv_score": summary.hv_score,
        "reference_point": list(summary.reference_point),
        "feasible_count": summary.feasible_count,
        "archive_size": summary.archive_size,
        "pareto_entries": [
            {
                **entry.as_mapping(),
                "design_hash": entry.design_hash,
                "stage": entry.stage,
            }
            for entry in summary.pareto_entries
        ],
    }


class PlanningAgent:
    """Wraps the planning-tier gate so runner cycles can rely on tool schemas + telemetry."""

    def __init__(
        self,
        *,
        config: ai_config.ModelConfig | None = None,
        rag_index: Path | str | None = None,
    ) -> None:
        self.config = config or ai_config.load_model_config()
        self.gate = agent_module.provision_model_tier(
            role="planning", config=self.config
        )
        self.rag_index = Path(rag_index or rag.DEFAULT_INDEX_PATH)
        self.last_context: Mapping[str, Any] | None = None

    def _hash_context(self, payload: Mapping[str, Any]) -> str:
        text = json.dumps(payload, sort_keys=True, separators=(",", ":"))
        return hashlib.sha256(text.encode("utf-8")).hexdigest()

    def _validate_tool_call(self, tool_name: str, arguments: Mapping[str, Any]) -> None:
        agent_module.validate_tool_call(self.config, self.gate.model_alias, tool_name)
        schema = tools_api.get_tool_schema(tool_name)
        if schema:
            parameters = schema.get("parameters", {})
            required = parameters.get("required", [])
            missing = [field for field in required if field not in arguments]
            if missing:
                raise ValueError(
                    f"Tool '{tool_name}' missing required arguments: {missing}"
                )
        context_hash = self._hash_context(arguments)
        print(
            f"[planner][tool-call] role={self.gate.model_alias} tool={tool_name} context_hash={context_hash}"
        )

    def retrieve_rag(self, query: str, *, k: int = 3) -> list[dict[str, str]]:
        payload: dict[str, Any] = {"query": query}
        payload["k"] = k
        self._validate_tool_call("retrieve_rag", payload)
        return tools.retrieve_rag(query, k=k, index_path=self.rag_index)

    def evaluate_p3(
        self,
        params: Mapping[str, Any],
        *,
        stage: str | None = None,
    ) -> Mapping[str, Any]:
        args = {
            "params": params,
            "problem": "p3",
        }
        if stage is not None:
            args["stage"] = stage
        self._validate_tool_call("evaluate_p3", args)
        try:
            return tools.evaluate_p3(params, stage=stage or "p3")
        except Exception as exc:  # pragma: no cover - smoke-run safety
            message = f"planning-stage evaluate_p3 failed: {exc}"
            print(f"[planner] {message}")
            return {
                "stage": stage or "p3",
                "error": message,
                "objective": None,
                "feasibility": None,
                "hv": None,
            }

    def make_boundary(
        self, params: Mapping[str, Any]
    ) -> tools.surface_rz_fourier.SurfaceRZFourier:
        args = {"params": params}
        self._validate_tool_call("make_boundary", args)
        return tools.make_boundary_from_params(params)

    def _build_template_params(
        self, template: ai_config.BoundaryTemplateConfig
    ) -> Mapping[str, Any]:
        n_poloidal = template.n_poloidal_modes
        n_toroidal = template.n_toroidal_modes
        center_idx = n_toroidal // 2
        r_cos = []
        z_sin = []
        for pol in range(n_poloidal):
            r_row = []
            z_row = []
            for tor in range(n_toroidal):
                r_val = (
                    template.base_major_radius
                    if pol == 0 and tor == center_idx
                    else 0.0
                )
                z_val = (
                    template.base_minor_radius
                    if pol == 1 and tor == center_idx and n_poloidal > 1
                    else 0.0
                )
                r_row.append(r_val)
                z_row.append(z_val)
            r_cos.append(r_row)
            z_sin.append(z_row)
        return {
            "r_cos": r_cos,
            "z_sin": z_sin,
            "n_field_periods": template.n_field_periods,
            "is_stellarator_symmetric": True,
        }

    def _build_context(
        self,
        *,
        cycle_index: int,
        budgets: ai_config.BudgetConfig,
        stage_history: Sequence[Mapping[str, Any]],
        last_summary: tools.P3Summary | None,
        evaluation_summary: Mapping[str, Any],
        boundary_summary: Mapping[str, Any],
        rag_snippets: Sequence[Mapping[str, str]],
    ) -> Mapping[str, Any]:
        context = {
            "cycle_index": cycle_index + 1,
            "planner_role": self.gate.model_alias,
            "budgets": asdict(budgets),
            "stage_history": list(stage_history),
            "previous_p3_summary": _serialize_summary(last_summary),
            "latest_evaluation": evaluation_summary,
            "current_boundary": boundary_summary,
            "rag_snippets": list(rag_snippets),
            "toolset": list(self.gate.allowed_tools),
        }
        self.last_context = context
        return context

    def plan_cycle(
        self,
        *,
        cfg: ai_config.ExperimentConfig,
        cycle_index: int,
        stage_history: Sequence[Mapping[str, Any]],
        last_summary: tools.P3Summary | None,
    ) -> PlanningOutcome:
        rag_snippets = self.retrieve_rag(
            f"Planning guidance for {cfg.problem.upper()} cycle {cycle_index + 1}",
            k=3,
        )
        params = self._build_template_params(cfg.boundary_template)
        evaluation = self.evaluate_p3(
            params,
            stage=cfg.fidelity_ladder.screen,
        )
        boundary = self.make_boundary(params)
        evaluation_summary = {
            "objective": evaluation.get("objective"),
            "feasibility": evaluation.get("feasibility"),
            "hv": evaluation.get("hv"),
            "stage": evaluation.get("stage"),
            "agent_stage_label": f"agent-cycle-{cycle_index + 1}",
        }
        boundary_summary = {
            "n_poloidal_modes": boundary.n_poloidal_modes,
            "n_toroidal_modes": boundary.n_toroidal_modes,
            "n_field_periods": boundary.n_field_periods,
            "stellarator_symmetric": boundary.is_stellarator_symmetric,
        }
        context = self._build_context(
            cycle_index=cycle_index,
            budgets=cfg.budgets,
            stage_history=stage_history,
            last_summary=last_summary,
            evaluation_summary=evaluation_summary,
            boundary_summary=boundary_summary,
            rag_snippets=rag_snippets,
        )
        return PlanningOutcome(
            context=context,
            evaluation_summary=evaluation_summary,
            boundary_summary=boundary_summary,
            rag_snippets=rag_snippets,
        )


================================================================================
File: runner.py
================================================================================

"""Runner that wires budgets, fidelity decisions, and minimal reporting (Tasks 4.1 + B.*)."""

from __future__ import annotations

import argparse
import json
import math
import os
import platform
import subprocess
import sys
import time
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
from dataclasses import asdict, dataclass, replace
from datetime import datetime
from pathlib import Path
from typing import Any, Iterable, Mapping, Protocol, Sequence, Set, Tuple

import numpy as np
import yaml

from ai_scientist import adapter
from ai_scientist import config as ai_config
from ai_scientist import memory
from ai_scientist import planner as ai_planner
from ai_scientist import rag
from ai_scientist import reporting
from ai_scientist import tools
from ai_scientist.optim.surrogate import SimpleSurrogateRanker
from constellaration.geometry import surface_rz_fourier as surface_module
from constellaration.initial_guess import generate_rotating_ellipse
from orchestration import adaptation as adaptation_helpers

FEASIBILITY_CUTOFF = getattr(tools, "_DEFAULT_RELATIVE_TOLERANCE", 1e-2)
P3_REFERENCE_POINT = getattr(tools, "_P3_REFERENCE_POINT", (1.0, 20.0))
MIN_SURROGATE_HISTORY = 4
_BOUNDARY_SEED_CACHE: dict[Path, dict[str, Any]] = {}


def _load_seed_boundary(path: Path) -> dict[str, Any]:
    resolved = path.resolve()
    cached = _BOUNDARY_SEED_CACHE.get(resolved)
    if cached is None:
        raw = json.loads(resolved.read_text(encoding="utf-8"))
        payload: dict[str, Any] = {
            "r_cos": np.asarray(raw["r_cos"], dtype=float),
            "z_sin": np.asarray(raw["z_sin"], dtype=float),
            "r_sin": np.asarray(raw["r_sin"], dtype=float)
            if raw.get("r_sin") is not None
            else None,
            "z_cos": np.asarray(raw["z_cos"], dtype=float)
            if raw.get("z_cos") is not None
            else None,
            "n_field_periods": int(
                raw.get("n_field_periods") or raw.get("nfp") or 1
            ),
            "is_stellarator_symmetric": bool(
                raw.get("is_stellarator_symmetric", True)
            ),
        }
        _BOUNDARY_SEED_CACHE[resolved] = payload
        cached = payload
    return {
        key: (np.array(value, copy=True) if isinstance(value, np.ndarray) else value)
        for key, value in cached.items()
    }


@dataclass(frozen=True)
class BudgetSnapshot:
    screen_evals_per_cycle: int
    promote_top_k: int
    max_high_fidelity_evals_per_cycle: int


@dataclass(frozen=True)
class CycleBudgetFeedback:
    hv_delta: float | None
    feasibility_rate: float | None
    cache_hit_rate: float | None


class BudgetController:
    def __init__(
        self,
        base_budgets: ai_config.BudgetConfig,
        adaptive_cfg: ai_config.AdaptiveBudgetConfig,
    ) -> None:
        self._base = base_budgets
        self._adaptive_cfg = adaptive_cfg
        self._last_feedback: CycleBudgetFeedback | None = None
        self._cache_stats: dict[str, dict[str, int]] = {}

    def snapshot(self) -> BudgetSnapshot:
        if not self._adaptive_cfg.enabled or self._last_feedback is None:
            return BudgetSnapshot(
                screen_evals_per_cycle=self._base.screen_evals_per_cycle,
                promote_top_k=self._base.promote_top_k,
                max_high_fidelity_evals_per_cycle=self._base.max_high_fidelity_evals_per_cycle,
            )
        return BudgetSnapshot(
            screen_evals_per_cycle=self._blend_budget(
                self._base.screen_evals_per_cycle,
                self._adaptive_cfg.screen_bounds,
                self._screen_score(self._last_feedback),
            ),
            promote_top_k=self._blend_budget(
                self._base.promote_top_k,
                self._adaptive_cfg.promote_top_k_bounds,
                self._promote_score(self._last_feedback),
            ),
            max_high_fidelity_evals_per_cycle=self._blend_budget(
                self._base.max_high_fidelity_evals_per_cycle,
                self._adaptive_cfg.high_fidelity_bounds,
                self._high_fidelity_score(self._last_feedback),
            ),
        )

    def capture_cache_hit_rate(
        self,
        stage: str,
        stats: Mapping[str, int] | None = None,
    ) -> float:
        stats = stats or tools.get_cache_stats(stage)
        previous = self._cache_stats.get(stage)
        delta_hits = stats.get("hits", 0)
        delta_misses = stats.get("misses", 0)
        if previous:
            delta_hits -= previous.get("hits", 0)
            delta_misses -= previous.get("misses", 0)
        self._cache_stats[stage] = {
            "hits": stats.get("hits", 0),
            "misses": stats.get("misses", 0),
        }
        total = max(0, delta_hits + delta_misses)
        if total <= 0:
            return 0.0
        return float(max(0.0, min(1.0, delta_hits / total)))

    def record_feedback(self, feedback: CycleBudgetFeedback) -> None:
        self._last_feedback = feedback

    def _blend_budget(
        self,
        base_value: int,
        bounds: ai_config.BudgetRangeConfig,
        score: float,
    ) -> int:
        constrained_base = max(bounds.min, min(bounds.max, base_value))
        if bounds.min >= bounds.max:
            return constrained_base
        clamped = max(0.0, min(1.0, score))
        mid = 0.5
        if clamped == mid:
            return constrained_base
        if clamped > mid:
            ratio = (clamped - mid) * 2.0
            increment = bounds.max - constrained_base
            return min(bounds.max, constrained_base + int(round(increment * ratio)))
        ratio = (mid - clamped) * 2.0
        decrement = constrained_base - bounds.min
        return max(bounds.min, constrained_base - int(round(decrement * ratio)))

    def _normalize(self, value: float | None, target: float) -> float:
        if value is None or target <= 0.0:
            return 0.0
        ratio = float(value) / float(target)
        return max(0.0, min(1.0, ratio))

    def _screen_score(self, feedback: CycleBudgetFeedback) -> float:
        progress = max(0.0, feedback.hv_delta or 0.0)
        normalized = self._normalize(progress, self._adaptive_cfg.hv_slope_reference)
        feasibility = self._normalize(
            feedback.feasibility_rate, self._adaptive_cfg.feasibility_target
        )
        return 1.0 - (0.6 * normalized + 0.4 * feasibility)

    def _promote_score(self, feedback: CycleBudgetFeedback) -> float:
        progress = max(0.0, feedback.hv_delta or 0.0)
        normalized = self._normalize(progress, self._adaptive_cfg.hv_slope_reference)
        feasibility = self._normalize(
            feedback.feasibility_rate, self._adaptive_cfg.feasibility_target
        )
        return 0.6 * normalized + 0.4 * feasibility

    def _high_fidelity_score(self, feedback: CycleBudgetFeedback) -> float:
        progress = max(0.0, feedback.hv_delta or 0.0)
        normalized = self._normalize(progress, self._adaptive_cfg.hv_slope_reference)
        cache = self._normalize(
            feedback.cache_hit_rate, self._adaptive_cfg.cache_hit_target
        )
        return 0.5 * normalized + 0.5 * cache


def _repo_relative(path: Path) -> str | None:
    allowed_prefixes = (
        "docs/",
        "constellaration/",
        "Jr.AI-Scientist/",
        "reports/",
        "tests/",
    )
    try:
        rel = path.resolve().relative_to(Path.cwd()).as_posix()
    except ValueError:
        return None
    for prefix in allowed_prefixes:
        if rel.startswith(prefix):
            return rel
    return None


class ProblemEvaluator(Protocol):
    def __call__(
        self,
        boundary_params: Mapping[str, Any],
        *,
        stage: str,
        use_cache: bool = True,
    ) -> dict[str, Any]: ...


class WorldModelLike(Protocol):
    def log_statement(
        self,
        experiment_id: int,
        cycle: int,
        stage: str,
        text: str,
        status: str,
        tool_name: str,
        tool_input: Mapping[str, Any],
        *,
        metrics_id: int | None = None,
        seed: int | None = None,
        git_sha: str,
        repro_cmd: str,
        created_at: str | None = None,
        commit: bool = True,
    ) -> int: ...


_PROBLEM_EVALUATORS: dict[str, tuple[str, ProblemEvaluator]] = {
    "p1": ("evaluate_p1", tools.evaluate_p1),
    "p2": ("evaluate_p2", tools.evaluate_p2),
    "p3": ("evaluate_p3", tools.evaluate_p3),
}


def _problem_evaluator(problem: str) -> ProblemEvaluator:
    try:
        return _PROBLEM_EVALUATORS[problem][1]
    except KeyError as exc:
        raise NotImplementedError(
            "Problem '%s' is not supported; choose one of %s."
            % (problem, ", ".join(sorted(_PROBLEM_EVALUATORS)))
        ) from exc


def _problem_tool_name(problem: str) -> str:
    try:
        return _PROBLEM_EVALUATORS[problem][0]
    except KeyError as exc:
        raise NotImplementedError(
            "Problem '%s' is not supported; choose one of %s."
            % (problem, ", ".join(sorted(_PROBLEM_EVALUATORS)))
        ) from exc


@dataclass
class RunnerCLIConfig:
    config_path: Path
    problem: str | None
    cycles: int | None
    memory_db: Path | None
    eval_budget: int | None
    workers: int | None
    pool_type: str | None
    screen_only: bool
    promote_only: bool
    slow: bool
    verbose: bool
    log_cache_stats: bool
    run_preset: str | None
    planner: str


def _build_argument_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description=(
            "AI Scientist runner (per docs/TASKS_CODEX_MINI.md:191-195). "
            "Set AI_SCIENTIST_PEFT=1 to load adapter bundles from reports/adapters."
        )
    )
    parser.add_argument(
        "--config",
        type=Path,
        default=ai_config.DEFAULT_EXPERIMENT_CONFIG_PATH,
        help="Path to the experiment configuration YAML (defaults to configs/experiment.yaml).",
    )
    parser.add_argument(
        "--problem",
        choices=["p1", "p2", "p3"],
        help=(
            "Problem identifier that overrides the config (p1=GeometricalProblem, "
            "p2=SimpleToBuildQIStellarator, p3=MHDStableQIStellarator)."
        ),
    )
    parser.add_argument(
        "--cycles",
        type=int,
        help="Number of governance cycles to run (overrides config; each cycle includes screening → reporting).",
    )
    parser.add_argument(
        "--memory-db",
        type=Path,
        help="Path to the shared SQLite world model (overrides config).",
    )
    parser.add_argument(
        "--eval-budget",
        type=int,
        help="Override the per-cycle screening budget (screen_evals_per_cycle).",
    )
    parser.add_argument(
        "--workers",
        type=int,
        help="Override n_workers from the config (also powers multiprocessing pools).",
    )
    parser.add_argument(
        "--pool-type",
        choices=["thread", "process"],
        help="Choose the executor pool type used when n_workers > 1.",
    )
    parser.add_argument(
        "--screen",
        action="store_true",
        help=(
            "Run only the screening stage (governance S1) and skip promotions. "
            "Cannot be combined with --promote or presets that advance directly to S2."
        ),
    )
    parser.add_argument(
        "--promote",
        action="store_true",
        help=(
            "Start governance in promote/refine mode (S2+) and report promotions. "
            "Cannot be combined with --screen or presets that force S1-only behavior."
        ),
    )
    parser.add_argument(
        "--slow",
        action="store_true",
        help="Throttle loop iterations for deterministic, long-wall-clock logging and traceability.",
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Emit additional runner diagnostics (candidate mixes, gating decisions).",
    )
    parser.add_argument(
        "--log-cache-stats",
        action="store_true",
        help="Write per-stage cache stats to reports/cache_stats.jsonl for Phase 5 observability.",
    )
    parser.add_argument(
        "--run-preset",
        type=str,
        help="Name of a preset from configs/run_presets.yaml that toggles --screen/--promote/--slow.",
    )
    parser.add_argument(
        "--planner",
        choices=["deterministic", "agent"],
        default="deterministic",
        help="Choose the planning driver (deterministic loop or Phase 3 agent).",
    )
    return parser


def parse_args(args: Sequence[str] | None = None) -> RunnerCLIConfig:
    parser = _build_argument_parser()
    namespace = parser.parse_args(args)
    if namespace.screen and namespace.promote:
        parser.error("--screen cannot be combined with --promote.")
    return RunnerCLIConfig(
        config_path=namespace.config,
        problem=namespace.problem,
        cycles=namespace.cycles,
        memory_db=namespace.memory_db,
        eval_budget=namespace.eval_budget,
        workers=namespace.workers,
        pool_type=namespace.pool_type,
        screen_only=bool(namespace.screen),
        promote_only=bool(namespace.promote),
        slow=bool(namespace.slow),
        verbose=bool(namespace.verbose),
        log_cache_stats=bool(namespace.log_cache_stats),
        run_preset=namespace.run_preset,
        planner=namespace.planner,
    )


def _validate_runtime_flags(runtime: RunnerCLIConfig) -> None:
    if runtime.screen_only and runtime.promote_only:
        raise ValueError(
            "--screen (S1-only) cannot be combined with promote-only mode (S2+) "
            f"(presets: {runtime.run_preset or '<none>'}). Remove one flag/preset."
        )


def _generate_candidate_params(
    template: ai_config.BoundaryTemplateConfig, seed: int
) -> dict[str, Any]:
    rng = np.random.default_rng(seed)
    seed_data: dict[str, Any] | None = None
    if template.seed_path is not None:
        seed_data = _load_seed_boundary(template.seed_path)
        r_cos = seed_data["r_cos"]
        z_sin = seed_data["z_sin"]
        r_sin = seed_data["r_sin"]
        z_cos = seed_data["z_cos"]
        n_field_periods = int(seed_data["n_field_periods"])
        is_stellarator_symmetric = bool(seed_data["is_stellarator_symmetric"])
    else:
        base_surface = generate_rotating_ellipse(
            aspect_ratio=4.0,
            elongation=1.5,
            rotational_transform=1.2,
            n_field_periods=template.n_field_periods,
        )
        max_poloidal = max(1, template.n_poloidal_modes - 1)
        max_toroidal = max(1, (template.n_toroidal_modes - 1) // 2)
        expanded = surface_module.set_max_mode_numbers(
            base_surface,
            max_poloidal_mode=max_poloidal,
            max_toroidal_mode=max_toroidal,
        )
        r_cos = np.asarray(expanded.r_cos, dtype=float)
        z_sin = np.asarray(expanded.z_sin, dtype=float)
        center_idx = r_cos.shape[1] // 2
        r_cos[0, center_idx] = template.base_major_radius
        if r_cos.shape[0] > 1:
            z_sin[1, center_idx] = template.base_minor_radius

        r_sin = None
        z_cos = None
        n_field_periods = template.n_field_periods
        is_stellarator_symmetric = True

    r_cos += rng.normal(scale=template.perturbation_scale, size=r_cos.shape)
    z_sin += rng.normal(scale=template.perturbation_scale / 2, size=z_sin.shape)
    if seed_data is not None:
        if r_sin is not None:
            r_sin += rng.normal(scale=template.perturbation_scale / 2, size=r_sin.shape)
        if z_cos is not None:
            z_cos += rng.normal(scale=template.perturbation_scale / 2, size=z_cos.shape)

    if is_stellarator_symmetric:
        n_cols = r_cos.shape[1]
        center_idx = n_cols // 2
        if center_idx > 0:
            r_cos[0, :center_idx] = 0.0
        z_sin[0, :] = 0.0

    params = {
        "r_cos": r_cos.tolist(),
        "z_sin": z_sin.tolist(),
        "n_field_periods": template.n_field_periods or n_field_periods,
        "is_stellarator_symmetric": is_stellarator_symmetric,
    }
    if r_sin is not None:
        params["r_sin"] = r_sin.tolist()
    if z_cos is not None:
        params["z_cos"] = z_cos.tolist()

    return {
        "seed": seed,
        "params": params,
        "design_hash": tools.design_hash(params),
    }


def _propose_p3_candidates_for_cycle(
    cfg: ai_config.ExperimentConfig,
    cycle_index: int,
    world_model: memory.WorldModel,
    experiment_id: int,
    *,
    screen_budget: int,
    total_candidates: int | None = None,
) -> tuple[list[Mapping[str, Any]], int, int]:
    """Blend constraint-aware sampling and random noise per roadmap Phase 1 guidance.

    If total_candidates is omitted, screen_budget drives the pool size.
    See /Users/suhjungdae/code/software/proxima_fusion/RL-feasible-designs/ai_scientist/roadmap.md for the Phase 1 candidate-generation recipe that introduced this helper.
    """

    pool_size = total_candidates if total_candidates is not None else screen_budget
    total_candidates = int(pool_size)
    if total_candidates <= 0:
        return [], 0, 0

    mix = cfg.proposal_mix
    ratio_sum = mix.constraint_ratio + mix.exploration_ratio
    if ratio_sum <= 0.0:
        sampler_target = 0
    else:
        sampler_target = int(
            round(total_candidates * (mix.constraint_ratio / ratio_sum))
        )
    sampler_target = min(sampler_target, total_candidates)

    stage_limit = max(total_candidates * 4, 16)
    stage_records = world_model.recent_stage_candidates(
        experiment_id=experiment_id,
        problem=cfg.problem,
        stage=cfg.fidelity_ladder.promote,
        limit=int(stage_limit),
    )

    if stage_records and sampler_target > 0:
        base_designs = [record[0] for record in stage_records]
        feasibilities = [max(0.0, float(record[1])) for record in stage_records]
        max_feas = max(feasibilities, default=0.0)
        normalized_distances = [
            0.0 if max_feas <= 0.0 else min(1.0, value / max_feas)
            for value in feasibilities
        ]
        rng_seed = cfg.random_seed + cycle_index + total_candidates
        sampler_params = tools.normalized_constraint_distance_sampler(
            base_designs,
            normalized_distances=normalized_distances,
            proposal_count=sampler_target,
            jitter_scale=mix.jitter_scale,
            rng=np.random.default_rng(rng_seed),
        )
    else:
        sampler_params = []

    candidate_seeds = [
        cfg.random_seed + cycle_index * total_candidates + i
        for i in range(total_candidates)
    ]
    seed_iter = iter(candidate_seeds)
    sampler_results: list[Mapping[str, Any]] = []
    for params in sampler_params:
        try:
            seed = next(seed_iter)
        except StopIteration:
            break
        sampler_results.append(
            {
                "seed": seed,
                "params": params,
                "design_hash": tools.design_hash(params),
            }
        )

    remaining = total_candidates - len(sampler_results)
    random_results: list[Mapping[str, Any]] = []
    for _ in range(remaining):
        seed = next(seed_iter)
        random_results.append(_generate_candidate_params(cfg.boundary_template, seed))

    candidates = sampler_results + random_results
    return candidates, len(sampler_results), len(random_results)


def _surrogate_candidate_pool_size(
    screen_budget: int,
    surrogate_pool_multiplier: float,
) -> int:
    if screen_budget <= 0:
        return 0
    multiplier = max(1.0, surrogate_pool_multiplier)
    proposed = int(math.ceil(screen_budget * multiplier))
    return max(proposed, screen_budget)


def _surrogate_rank_screen_candidates(
    cfg: ai_config.ExperimentConfig,
    screen_budget: int,
    candidates: list[Mapping[str, Any]],
    world_model: memory.WorldModel,
    *,
    verbose: bool = False,
) -> list[Mapping[str, Any]]:
    """Train SimpleSurrogateRanker on cached history and trim the pool (see /Users/suhjungdae/code/software/proxima_fusion/RL-feasible-designs/ai_scientist/roadmap.md)."""

    if not candidates or screen_budget <= 0:
        return candidates

    history = world_model.surrogate_training_data(target="hv", problem=cfg.problem)
    required_history = max(MIN_SURROGATE_HISTORY, screen_budget)
    if len(history) < required_history:
        return candidates

    metrics_list, target_values = zip(*history)
    ranker = SimpleSurrogateRanker()
    ranker.fit(metrics_list, target_values)

    pool_entries: list[Mapping[str, Any]] = []
    for idx, candidate in enumerate(candidates):
        pool_entries.append(
            {
                "candidate_params": candidate["params"],
                "__surrogate_candidate_index": idx,
            }
        )

    ranked = ranker.rank(pool_entries)
    selected: list[Mapping[str, Any]] = []
    seen: set[int] = set()
    needed = screen_budget
    for rank in ranked:
        idx = int(rank.metrics.get("__surrogate_candidate_index", -1))
        if idx < 0 or idx in seen:
            continue
        seen.add(idx)
        selected.append(candidates[idx])
        if len(selected) >= needed:
            break

    if not selected:
        return candidates[:needed]

    if verbose:
        dropped = len(candidates) - len(selected)
        print(
            f"[runner][surrogate] trained on {len(history)} rows, "
            f"selected {len(selected)}/{len(candidates)} candidates (dropped {dropped})"
        )

    return selected


def _time_exceeded(start: float, limit_minutes: float) -> bool:
    elapsed = time.perf_counter() - start
    return elapsed >= limit_minutes * 60


@dataclass
class CycleSummary:
    cycle: int
    objective: float | None
    feasibility: float | None
    hv: float | None
    stage: str


def _stage_rank(stage: str | None, promote_stage: str) -> int:
    if stage == promote_stage:
        return 2
    if stage:
        return 1
    return 0


def _feasibility_value(entry: Mapping[str, Any]) -> float:
    return float(entry["evaluation"].get("feasibility", float("inf")))


def _oriented_objective(entry: Mapping[str, Any]) -> float:
    evaluation = entry["evaluation"]
    objective = evaluation.get("objective")
    if objective is None:
        return float("inf")
    minimize = evaluation.get("minimize_objective", True)
    value = float(objective)
    return value if minimize else -value


def _prefer_entry(
    current: Mapping[str, Any],
    candidate: Mapping[str, Any],
    promote_stage: str,
) -> Mapping[str, Any]:
    current_rank = _stage_rank(current["evaluation"].get("stage"), promote_stage)
    candidate_rank = _stage_rank(candidate["evaluation"].get("stage"), promote_stage)
    if candidate_rank > current_rank:
        return candidate
    if candidate_rank < current_rank:
        return current

    current_feas = _feasibility_value(current)
    candidate_feas = _feasibility_value(candidate)
    if candidate_feas < current_feas:
        return candidate
    if candidate_feas > current_feas:
        return current

    current_obj = _oriented_objective(current)
    candidate_obj = _oriented_objective(candidate)
    if candidate_obj < current_obj:
        return candidate
    return current


def _close_metric(value_a: float | None, value_b: float | None) -> bool:
    if value_a is None or value_b is None:
        return value_a is None and value_b is None
    return math.isclose(value_a, value_b, rel_tol=1e-3, abs_tol=1e-3)


def _verify_best_claim(
    world_model: WorldModelLike,
    experiment_id: int,
    cycle_number: int,
    best_entry: Mapping[str, Any],
    best_eval: Mapping[str, Any],
    evaluation_fn: ProblemEvaluator,
    tool_name: str,
    best_seed: int,
    git_sha: str,
    reproduction_command: str,
    *,
    stage: str,
    metrics_id: int | None,
) -> str:
    tool_input = {"params": best_entry["params"], "stage": stage}
    replay_eval = evaluation_fn(
        best_entry["params"],
        stage=stage,
        use_cache=False,
    )
    differences: list[str] = []
    for metric in ("objective", "feasibility", "hv"):
        if not _close_metric(best_eval.get(metric), replay_eval.get(metric)):
            differences.append(metric)
    status = "SUPPORTED" if not differences else "REFUTED"
    statement_text = f"Replayed {tool_name} evaluation for design {best_entry.get('design_hash', '')[:8]} at stage {stage}."
    world_model.log_statement(
        experiment_id=experiment_id,
        cycle=cycle_number,
        stage=stage,
        text=statement_text,
        status=status,
        tool_name=tool_name,
        tool_input=tool_input,
        metrics_id=metrics_id,
        seed=best_seed,
        git_sha=git_sha,
        repro_cmd=reproduction_command,
    )
    print(
        f"[runner][verifier] statement status={status} differences={differences} for cycle {cycle_number}"
    )
    return status


def _latest_evaluations_by_design(
    aggregated: Sequence[Mapping[str, Any]], promote_stage: str
) -> dict[str, Mapping[str, Any]]:
    latest: dict[str, Mapping[str, Any]] = {}
    for entry in aggregated:
        design_hash = entry.get("design_hash")
        if not design_hash:
            continue
        existing = latest.get(design_hash)
        if existing is None:
            latest[design_hash] = entry
            continue
        latest[design_hash] = _prefer_entry(
            existing,
            entry,
            promote_stage,
        )
    return latest


def _extract_objectives(entry: Mapping[str, Any]) -> tuple[float, float]:
    metrics = entry["evaluation"]["metrics"]
    gradient = float(metrics["minimum_normalized_magnetic_gradient_scale_length"])
    aspect = float(metrics["aspect_ratio"])
    return gradient, aspect


def _crowding_distance(
    entries_by_design: Mapping[str, Mapping[str, Any]],
) -> dict[str, float]:
    if not entries_by_design:
        return {}
    values: list[tuple[str, float, float]] = []
    for design_hash, entry in entries_by_design.items():
        gradient, aspect = _extract_objectives(entry)
        values.append((design_hash, float(gradient), float(aspect)))
    distances = {design_hash: 0.0 for design_hash in entries_by_design}
    if len(values) <= 2:
        for design_hash in distances:
            distances[design_hash] = float("inf")
        return distances
    sorted_grad = sorted(values, key=lambda item: item[1], reverse=True)
    grad_values = [item[1] for item in sorted_grad]
    grad_span = max(max(grad_values) - min(grad_values), 1e-9)
    distances[sorted_grad[0][0]] = float("inf")
    distances[sorted_grad[-1][0]] = float("inf")
    for pos in range(1, len(sorted_grad) - 1):
        prev_val = sorted_grad[pos - 1][1]
        next_val = sorted_grad[pos + 1][1]
        distances[sorted_grad[pos][0]] += abs(next_val - prev_val) / grad_span

    sorted_aspect = sorted(values, key=lambda item: item[2], reverse=False)
    aspect_values = [item[2] for item in sorted_aspect]
    aspect_span = max(max(aspect_values) - min(aspect_values), 1e-9)
    distances[sorted_aspect[0][0]] = float("inf")
    distances[sorted_aspect[-1][0]] = float("inf")
    for pos in range(1, len(sorted_aspect) - 1):
        prev_val = sorted_aspect[pos - 1][2]
        next_val = sorted_aspect[pos + 1][2]
        distances[sorted_aspect[pos][0]] += abs(next_val - prev_val) / aspect_span
    return distances


def _rank_candidates_for_promotion(
    entries_by_design: Mapping[str, Mapping[str, Any]],
    promote_limit: int,
    reference_point: Tuple[float, float],
) -> list[Mapping[str, Any]]:
    if not entries_by_design:
        return []
    summary = tools.summarize_p3_candidates(
        list(entries_by_design.values()), reference_point=reference_point
    )
    ordered_hashes = [entry.design_hash for entry in summary.pareto_entries]
    ranked: list[Mapping[str, Any]] = [
        entries_by_design[h] for h in ordered_hashes if h in entries_by_design
    ]
    if len(ranked) >= promote_limit:
        return ranked[:promote_limit]
    remaining = {
        design_hash: entry
        for design_hash, entry in entries_by_design.items()
        if design_hash not in {entry.design_hash for entry in summary.pareto_entries}
    }
    crowding = _crowding_distance(remaining)

    def _sort_key(design_hash: str) -> tuple[float, float, float]:
        entry = remaining[design_hash]
        feas = _feasibility_value(entry)
        feasible_flag = 0.0 if feas <= FEASIBILITY_CUTOFF else 1.0
        return (
            feasible_flag,
            -crowding.get(design_hash, 0.0),
            feas,
        )

    for design_hash in sorted(remaining, key=_sort_key):
        ranked.append(remaining[design_hash])
        if len(ranked) >= promote_limit:
            break
    return ranked


def _persist_pareto_archive(
    *,
    world_model: memory.WorldModel,
    experiment_id: int,
    cycle_number: int,
    problem: str,
    entries_by_design: Mapping[str, Mapping[str, Any]],
    p3_summary: tools.P3Summary,
    git_sha: str,
    constellaration_sha: str,
) -> tuple[Set[str], dict[str, int]]:
    """Persist Phase 6 Pareto deliverables and return the logged design hashes."""

    logged_hashes: Set[str] = set()
    archive_rows: list[Mapping[str, Any]] = []
    metrics_by_hash: dict[str, int] = {}
    for entry in p3_summary.pareto_entries:
        design_hash = entry.design_hash
        latest = entries_by_design.get(design_hash)
        if latest is None:
            continue
        evaluation = latest["evaluation"]
        candidate_id, metrics_id = world_model.log_candidate(
            experiment_id=experiment_id,
            problem=problem,
            params=latest["params"],
            seed=int(latest.get("seed", -1)),
            status=evaluation.get("stage", "unknown"),
            evaluation=evaluation,
            design_hash=design_hash,
            commit=False,
        )
        logged_hashes.add(design_hash)
        metrics_by_hash[design_hash] = metrics_id
        settings_json = json.dumps(
            evaluation.get("settings", {}), separators=(",", ":")
        )
        archive_rows.append(
            {
                "design_hash": design_hash,
                "fidelity": evaluation.get("stage", "unknown"),
                "gradient": entry.gradient,
                "aspect": entry.aspect_ratio,
                "metrics_id": metrics_id,
                "git_sha": git_sha,
                "constellaration_sha": constellaration_sha,
                "settings_json": settings_json,
                "seed": int(latest.get("seed", -1)),
            }
        )
        world_model.upsert_pareto(experiment_id, candidate_id)
    if archive_rows:
        world_model.record_pareto_archive(
            experiment_id,
            cycle_number,
            archive_rows,
            commit=False,
        )
    return logged_hashes, metrics_by_hash


def _relative_objective_improvement(
    history: list[CycleSummary], lookback: int
) -> float:
    if len(history) <= lookback:
        return 0.0
    earlier = history[-lookback - 1].objective
    latest = history[-1].objective
    if earlier is None or latest is None:
        return 0.0
    diff = earlier - latest
    denom = abs(earlier) if abs(earlier) > 1e-6 else 1.0
    return float(diff / denom)


def _should_transition_s1_to_s2(
    history: list[CycleSummary], gate_cfg: ai_config.StageGateConfig
) -> bool:
    if not history:
        return False
    last = history[-1]
    if last.feasibility is not None:
        triggered = last.feasibility <= gate_cfg.s1_to_s2_feasibility_margin
        print(
            f"[runner][stage-gate] S1→S2 feasibility check: margin={last.feasibility:.5f} "
            f"<= {gate_cfg.s1_to_s2_feasibility_margin:.5f} -> {triggered}"
        )
        if triggered:
            return True
    improvement = _relative_objective_improvement(
        history, gate_cfg.s1_to_s2_lookback_cycles
    )
    triggered_improvement = improvement >= gate_cfg.s1_to_s2_objective_improvement
    print(
        f"[runner][stage-gate] S1→S2 objective improvement check: "
        f"{improvement:.4f} >= {gate_cfg.s1_to_s2_objective_improvement:.4f} -> {triggered_improvement}"
    )
    return triggered_improvement


def _should_transition_s2_to_s3(
    history: list[CycleSummary],
    gate_cfg: ai_config.StageGateConfig,
    governance_cfg: ai_config.GovernanceConfig,
    world_model: memory.WorldModel,
    experiment_id: int,
    current_cycle: int,
    total_cycles: int,
) -> bool:
    avg_delta = world_model.average_recent_hv_delta(
        experiment_id, governance_cfg.hv_lookback
    )
    if avg_delta is not None:
        triggered_delta = avg_delta <= gate_cfg.s2_to_s3_hv_delta
        print(
            f"[runner][stage-gate] S2→S3 average HV delta over "
            f"{governance_cfg.hv_lookback} cycles: {avg_delta:.4f} <= {gate_cfg.s2_to_s3_hv_delta:.4f} -> {triggered_delta}"
        )
        if triggered_delta:
            return True
    else:
        print(
            f"[runner][stage-gate] insufficient HV delta history ({len(history)} cycles) "
            f"to evaluate lookback={governance_cfg.hv_lookback}; deferring promotion"
        )
    exhausted = current_cycle >= total_cycles
    print(
        f"[runner][stage-gate] S2→S3 budget check: cycle={current_cycle} >= total={total_cycles} -> {exhausted}"
    )
    return exhausted


def _process_worker_initializer() -> None:
    """Limit OpenMP threads inside process workers (Phase 5 observability safeguard)."""
    os.environ["OMP_NUM_THREADS"] = "1"


def _evaluate_stage(
    candidates: Iterable[Mapping[str, Any]],
    stage: str,
    budgets: ai_config.BudgetConfig,
    cycle_start: float,
    evaluate_fn: ProblemEvaluator,
    *,
    sleep_per_eval: float = 0.0,
) -> list[dict[str, Any]]:
    """Evaluate candidates at a given fidelity, respecting wall-clock budget."""

    results: list[dict[str, Any]] = []
    wall_limit = budgets.wall_clock_minutes

    if budgets.n_workers <= 1:
        for candidate in candidates:
            if _time_exceeded(cycle_start, wall_limit):
                break
            params = candidate["params"]
            design_id = candidate.get("design_hash") or tools.design_hash(params)
            evaluation = evaluate_fn(params, stage=stage)
            results.append(
                {
                    "params": params,
                    "evaluation": evaluation,
                    "seed": int(candidate["seed"]),
                    "design_hash": design_id,
                }
            )
            if sleep_per_eval > 0:
                time.sleep(sleep_per_eval)
        return results

    future_payloads = {}
    executor_cls = (
        ThreadPoolExecutor if budgets.pool_type == "thread" else ProcessPoolExecutor
    )
    executor_kwargs: dict[str, Any] = {"max_workers": budgets.n_workers}
    if executor_cls is ProcessPoolExecutor:
        executor_kwargs["initializer"] = _process_worker_initializer
    with executor_cls(**executor_kwargs) as executor:
        for candidate in candidates:
            if _time_exceeded(cycle_start, wall_limit):
                break
            design_id = candidate.get("design_hash")
            future = executor.submit(evaluate_fn, candidate["params"], stage=stage)
            future_payloads[future] = (candidate, design_id)

        for future in as_completed(future_payloads):
            candidate, design_id = future_payloads[future]
            exc = future.exception()
            if exc is not None:
                design_hash = design_id or tools.design_hash(candidate["params"])
                print(
                    f"[runner][stage-eval] Failed evaluation for design {design_hash} "
                    f"(seed={candidate['seed']} stage={stage}): {exc}"
                )
                continue

            results.append(
                {
                    "params": candidate["params"],
                    "evaluation": future.result(),
                    "seed": int(candidate["seed"]),
                    "design_hash": design_id or tools.design_hash(candidate["params"]),
                }
            )
    return results


def _cache_stats_log_path(report_dir: Path | str) -> Path:
    return Path(report_dir) / "cache_stats.jsonl"


def _maybe_log_cache_stats(
    runtime: RunnerCLIConfig | None,
    cfg: ai_config.ExperimentConfig,
    cycle_index: int,
    stage: str,
    stats: Mapping[str, int],
) -> None:
    """Emit per-cycle cache stats for Phase 5 observability (see ai_scientist/roadmap.md & ai_scientist/improvement-plan.md)."""
    if not (runtime and runtime.log_cache_stats):
        return
    entry = {
        "cycle": cycle_index + 1,
        "stage": stage,
        "timestamp": datetime.utcnow().isoformat(),
        "stats": stats,
    }
    log_path = _cache_stats_log_path(cfg.reporting_dir)
    log_path.parent.mkdir(parents=True, exist_ok=True)
    with log_path.open("a", encoding="utf-8") as handle:
        handle.write(json.dumps(entry, separators=(",", ":")) + "\n")


def _run_cycle(
    cfg: ai_config.ExperimentConfig,
    cycle_index: int,
    world_model: memory.WorldModel,
    experiment_id: int,
    governance_stage: str,
    git_sha: str,
    constellaration_sha: str,
    *,
    runtime: RunnerCLIConfig | None = None,
    budget_controller: BudgetController,
) -> tuple[Path | None, dict[str, Any] | None, tools.P3Summary | None]:
    tool_name = _problem_tool_name(cfg.problem)
    base_evaluate = _problem_evaluator(cfg.problem)
    evaluate_fn = adapter.with_peft(base_evaluate, tool_name=tool_name)
    cycle_start = time.perf_counter()
    cycle_number = cycle_index + 1
    sleep_per_eval = 0.05 if runtime and runtime.slow else 0.0
    screen_only = bool(runtime and runtime.screen_only)
    budget_snapshot = budget_controller.snapshot()
    active_budgets = replace(
        cfg.budgets,
        screen_evals_per_cycle=budget_snapshot.screen_evals_per_cycle,
        promote_top_k=budget_snapshot.promote_top_k,
        max_high_fidelity_evals_per_cycle=budget_snapshot.max_high_fidelity_evals_per_cycle,
    )
    if cfg.adaptive_budgets.enabled and runtime and runtime.verbose:
        print(
            f"[runner][budget] cycle={cycle_number} override "
            f"screen={budget_snapshot.screen_evals_per_cycle} "
            f"promote_top_k={budget_snapshot.promote_top_k} "
            f"max_high_fidelity={budget_snapshot.max_high_fidelity_evals_per_cycle}"
        )
    cache_hit_rate = 0.0
    pool_size = _surrogate_candidate_pool_size(
        budget_snapshot.screen_evals_per_cycle,
        cfg.proposal_mix.surrogate_pool_multiplier,
    )
    sampler_count = 0
    random_count = 0
    if pool_size <= 0:
        if runtime and runtime.verbose:
            print(
                f"[runner][cycle={cycle_number}] screen budget zero; skipping candidate generation"
            )
        candidate_pool: list[Mapping[str, Any]] = []
        candidates: list[Mapping[str, Any]] = []
    else:
        candidate_pool, sampler_count, random_count = _propose_p3_candidates_for_cycle(
            cfg,
            cycle_index,
            world_model,
            experiment_id,
            screen_budget=budget_snapshot.screen_evals_per_cycle,
            total_candidates=pool_size,
        )
        if runtime and runtime.verbose:
            print(
                f"[runner][cycle={cycle_number}] candidate mix (pool={len(candidate_pool)}): sampler={sampler_count} random={random_count}"
            )
        candidates = _surrogate_rank_screen_candidates(
            cfg,
            budget_snapshot.screen_evals_per_cycle,
            candidate_pool,
            world_model,
            verbose=bool(runtime and runtime.verbose),
        )

    screen_stage = cfg.fidelity_ladder.screen
    if candidates:
        screen_results = _evaluate_stage(
            candidates,
            stage=screen_stage,
            budgets=active_budgets,
            cycle_start=cycle_start,
            evaluate_fn=evaluate_fn,
            sleep_per_eval=sleep_per_eval,
        )
    else:
        screen_results = []
    screen_cache_stats = tools.get_cache_stats(screen_stage)
    cache_hit_rate = budget_controller.capture_cache_hit_rate(
        screen_stage, stats=screen_cache_stats
    )
    _maybe_log_cache_stats(runtime, cfg, cycle_index, screen_stage, screen_cache_stats)

    screen_design_map = _latest_evaluations_by_design(screen_results, screen_stage)
    screen_summary = tools.summarize_p3_candidates(
        list(screen_design_map.values()), reference_point=P3_REFERENCE_POINT
    )
    allow_promote = (
        screen_summary.feasible_count >= cfg.governance.min_feasible_for_promotion
    )
    promote_limit = 0
    to_promote: list[Mapping[str, Any]] = []
    if allow_promote and screen_design_map:
        prioritized_screen = _rank_candidates_for_promotion(
            screen_design_map, active_budgets.promote_top_k, P3_REFERENCE_POINT
        )
        promote_limit = min(
            active_budgets.promote_top_k,
            active_budgets.max_high_fidelity_evals_per_cycle,
            len(screen_design_map),
        )
        to_promote = prioritized_screen[:promote_limit]
    elif not allow_promote and screen_design_map and runtime and runtime.verbose:
        print(
            "[runner][stage-gate] insufficient feasible screen results "
            f"({screen_summary.feasible_count} < {cfg.governance.min_feasible_for_promotion}); skipping promotions"
        )

    promote_stage = cfg.fidelity_ladder.promote
    promote_results: list[dict[str, Any]] = []
    if not screen_only and promote_limit > 0:
        promote_results = _evaluate_stage(
            to_promote,
            stage=promote_stage,
            budgets=active_budgets,
            cycle_start=cycle_start,
            evaluate_fn=evaluate_fn,
            sleep_per_eval=sleep_per_eval,
        )
        promote_cache_stats = tools.get_cache_stats(promote_stage)
        _maybe_log_cache_stats(
            runtime, cfg, cycle_index, promote_stage, promote_cache_stats
        )
    elif screen_only:
        print("[runner] screen-only flag active; skipping promotion evaluations.")

    aggregated = screen_results + promote_results
    if not aggregated:
        world_model.record_stage_history(
            experiment_id=experiment_id,
            cycle=cycle_number,
            stage=governance_stage,
        )
        return None, None, None

    latest_by_design = _latest_evaluations_by_design(
        aggregated, cfg.fidelity_ladder.promote
    )
    if not latest_by_design:
        world_model.record_stage_history(
            experiment_id=experiment_id,
            cycle=cycle_number,
            stage=governance_stage,
        )
        return None, None, None

    p3_summary = tools.summarize_p3_candidates(
        list(latest_by_design.values()), reference_point=P3_REFERENCE_POINT
    )
    total_designs = len(latest_by_design)
    feasibility_rate = float(p3_summary.feasible_count) / float(max(1, total_designs))
    p3_summary_path = adaptation_helpers.write_p3_summary(
        base_dir=cfg.reporting_dir,
        cycle=cycle_number,
        summary=p3_summary,
    )

    best_entry = min(latest_by_design.values(), key=_oriented_objective)
    best_eval = dict(best_entry["evaluation"])
    metrics_payload = best_eval.setdefault("metrics", {})
    metrics_payload["cycle_hv"] = p3_summary.hv_score
    best_eval["cycle_hv"] = p3_summary.hv_score
    best_eval["design_hash"] = best_entry.get("design_hash", "")
    cycle_duration = time.perf_counter() - cycle_start
    best_seed = int(best_entry.get("seed", cfg.random_seed))
    previous_baseline = world_model.previous_best_hv(experiment_id, cycle_number)
    current_hv = float(p3_summary.hv_score)
    hv_delta = (
        current_hv - previous_baseline if previous_baseline is not None else current_hv
    )
    budget_controller.record_feedback(
        CycleBudgetFeedback(
            hv_delta=hv_delta,
            feasibility_rate=feasibility_rate,
            cache_hit_rate=cache_hit_rate,
        )
    )
    best_metrics_id: int | None = None
    logged_hashes: Set[str] = set()
    config_snapshot = dict(
        _serialize_experiment_config(cfg, constellaration_sha=constellaration_sha)
    )
    config_snapshot["cycle_seed"] = best_seed
    with world_model.transaction():
        world_model.record_cycle(
            experiment_id=experiment_id,
            cycle_number=cycle_number,
            screen_evals=len(screen_results),
            promoted_evals=len(promote_results),
            high_fidelity_evals=len(promote_results),
            wall_seconds=cycle_duration,
            best_params=best_entry["params"],
            best_evaluation=best_eval,
            seed=best_seed,
            log_best_candidate=False,
            problem=cfg.problem,
            commit=False,
        )
        world_model.record_cycle_hv(
            experiment_id=experiment_id,
            cycle_number=cycle_number,
            hv_score=p3_summary.hv_score,
            reference_point=p3_summary.reference_point,
            pareto_entries=[entry.as_mapping() for entry in p3_summary.pareto_entries],
            n_feasible=p3_summary.feasible_count,
            n_archive=p3_summary.archive_size,
            hv_lookback=cfg.governance.hv_lookback,
            commit=False,
        )
        logged_hashes, metrics_by_hash = _persist_pareto_archive(
            world_model=world_model,
            experiment_id=experiment_id,
            cycle_number=cycle_number,
            problem=cfg.problem,
            entries_by_design=latest_by_design,
            p3_summary=p3_summary,
            git_sha=git_sha,
            constellaration_sha=constellaration_sha,
        )
        if (
            best_entry.get("design_hash")
            and best_entry["design_hash"] not in logged_hashes
        ):
            _, metrics_id = world_model.log_candidate(
                experiment_id=experiment_id,
                problem=cfg.problem,
                params=best_entry["params"],
                seed=best_seed,
                status=best_eval.get("stage", "unknown"),
                evaluation=best_eval,
                design_hash=best_entry["design_hash"],
                commit=False,
            )
            best_metrics_id = metrics_id
        world_model.record_deterministic_snapshot(
            experiment_id=experiment_id,
            cycle_number=cycle_number,
            snapshot=config_snapshot,
            constellaration_sha=constellaration_sha,
            seed=best_seed,
            commit=False,
        )
    if best_metrics_id is None:
        best_metrics_id = metrics_by_hash.get(best_entry.get("design_hash", ""))
    repro_command = (
        f"python -m ai_scientist.runner --config {cfg.source_config} --problem {cfg.problem} "
        f"--cycles {cfg.cycles} --eval-budget {active_budgets.screen_evals_per_cycle} "
        f"--workers {active_budgets.n_workers} --pool-type {active_budgets.pool_type}"
    )
    env_block = (
        f"- Python: {sys.version.splitlines()[0]}\n"
        f"- Platform: {platform.platform()}\n"
        f"- Executable: {sys.executable}\n"
        f"- Host: {platform.node()}\n"
        f"- CPU: {platform.processor() or 'unknown'} | cores: {os.cpu_count() or 'unknown'}\n"
    )
    pareto_entries = p3_summary.pareto_entries
    if pareto_entries:
        pareto_lines = "\n".join(
            f"- design {entry.design_hash[:8]} seed={entry.seed} stage={entry.stage}: "
            f"gradient={entry.gradient:.4f}, aspect={entry.aspect_ratio:.4f}, "
            f"feasibility={entry.feasibility:.4f}"
            for entry in pareto_entries
        )
    else:
        pareto_lines = "- none (pareto front empty)"
    if pareto_entries:
        replay_entry = pareto_entries[0]
        reproduction_snippet = (
            "```bash\n"
            "python - <<'PY'\n"
            "import json, sqlite3\n"
            "from ai_scientist import tools\n"
            f"conn = sqlite3.connect('{cfg.memory_db}')\n"
            "row = conn.execute(\n"
            '    "SELECT params_json FROM candidates WHERE design_hash = ? ORDER BY id DESC LIMIT 1",\n'
            f"    ('{replay_entry.design_hash}',),\n"
            ").fetchone()\n"
            "assert row, 'Design hash not found in world model'\n"
            "params = json.loads(row[0])\n"
            f"print(tools.{tool_name}(params, stage='{replay_entry.stage or cfg.fidelity_ladder.promote}'))\n"
            "PY\n"
            "```\n"
        )
    else:
        reproduction_snippet = (
            "No Pareto archive entries available to replay this cycle.\n"
        )
    stage_label = best_eval.get("stage") or cfg.fidelity_ladder.promote
    reproduction_steps = [
        repro_command,
        f"git checkout {git_sha}",
        f"(cd constellaration && git checkout {constellaration_sha})",
        f"tools.{tool_name}(params, stage='{stage_label}')",
    ]
    tool_input = {"params": best_entry["params"], "stage": stage_label}
    tool_input_hash = memory.hash_payload(tool_input)
    statement_status = _verify_best_claim(
        world_model=world_model,
        experiment_id=experiment_id,
        cycle_number=cycle_number,
        best_entry=best_entry,
        best_eval=best_eval,
        evaluation_fn=evaluate_fn,
        tool_name=tool_name,
        best_seed=best_seed,
        git_sha=git_sha,
        reproduction_command=repro_command,
        stage=stage_label,
        metrics_id=best_metrics_id,
    )
    preference_pairs_path = adaptation_helpers.append_preference_pair(
        base_dir=cfg.reporting_dir,
        cycle=cycle_number,
        stage=stage_label,
        status=statement_status,
        tool_name=tool_name,
        tool_input_hash=tool_input_hash,
        reproduction_command=repro_command,
        metrics=best_eval.get("metrics", {}),
        design_hash=best_entry.get("design_hash"),
        problem=cfg.problem,
        seed=best_seed,
    )
    trajectory_path = adaptation_helpers.append_trajectory_entry(
        base_dir=cfg.reporting_dir,
        cycle=cycle_number,
        stage=stage_label,
        seed=best_seed,
        tool_name=tool_name,
        tool_input_hash=tool_input_hash,
        reproduction_steps=reproduction_steps,
        reproduction_snippet=reproduction_snippet,
        reproduction_command=repro_command,
        params=best_entry["params"],
        metrics=best_eval.get("metrics", {}),
        problem=cfg.problem,
        design_hash=best_entry.get("design_hash", ""),
    )
    preference_pairs_anchor = _repo_relative(preference_pairs_path)
    p3_summary_anchor = _repo_relative(p3_summary_path)
    trajectory_anchor = _repo_relative(trajectory_path)
    baseline_display = (
        f"{previous_baseline:.6f}" if previous_baseline is not None else "n/a"
    )
    preference_pairs_display = preference_pairs_anchor or preference_pairs_path.name
    trajectory_display = trajectory_anchor or trajectory_path.name
    p3_summary_display = p3_summary_anchor or p3_summary_path.name
    hv_text = (
        f"Cycle {cycle_number} hypervolume {current_hv:.6f} vs baseline "
        f"{baseline_display} (delta {hv_delta:+.6f}) recorded in cycle_hv "
        f"and adaptation logs {preference_pairs_display} and {trajectory_display} "
        f"with summary {p3_summary_display} (docs/TASKS_CODEX_MINI.md:238; docs/MASTER_PLAN_AI_SCIENTIST.md:226-247)."
    )
    hv_tool_input = {
        "cycle": cycle_number,
        "stage": stage_label,
        "current_hv": current_hv,
        "baseline_hv": previous_baseline,
        "delta": hv_delta,
        "preference_pairs_anchor": preference_pairs_anchor,
        "p3_summary_anchor": p3_summary_anchor,
        "trajectory_anchor": trajectory_anchor,
    }
    world_model.log_statement(
        experiment_id=experiment_id,
        cycle=cycle_number,
        stage=stage_label,
        text=hv_text,
        status="PENDING",
        tool_name="hv_delta_comparison",
        tool_input=hv_tool_input,
        metrics_id=best_metrics_id,
        seed=best_seed,
        git_sha=git_sha,
        repro_cmd=repro_command,
    )
    adaptation_helpers.append_preference_pair(
        base_dir=cfg.reporting_dir,
        cycle=cycle_number,
        stage=stage_label,
        status=statement_status,
        tool_name=tool_name,
        tool_input_hash=tool_input_hash,
        reproduction_command=repro_command,
        metrics=best_eval.get("metrics", {}),
        design_hash=best_entry.get("design_hash"),
        problem=cfg.problem,
        seed=best_seed,
    )
    statements = world_model.statements_for_cycle(experiment_id, cycle_number)
    figure_path = reporting.save_pareto_figure(
        p3_summary.pareto_entries,
        cfg.reporting_dir,
        title=cfg.problem,
        cycle_index=cycle_index,
    )
    figure_paths = [figure_path] if figure_path else []
    metrics_payload = best_eval.get("metrics", {})
    metrics_path = adaptation_helpers.write_metrics_snapshot(
        base_dir=cfg.reporting_dir,
        cycle=cycle_number,
        metrics=metrics_payload,
    )
    artifact_entries: list[tuple[str, Path]] = [("metrics_snapshot", metrics_path)]
    world_model.log_artifact(
        experiment_id=experiment_id,
        path=metrics_path,
        kind="metrics_snapshot",
    )
    artifact_entries.append(("p3_summary", p3_summary_path))
    world_model.log_artifact(
        experiment_id=experiment_id,
        path=p3_summary_path,
        kind="p3_summary",
    )
    artifact_entries.append(("preference_pairs", preference_pairs_path))
    world_model.log_artifact(
        experiment_id=experiment_id,
        path=preference_pairs_path,
        kind="preference_pairs",
    )
    artifact_entries.append(("trajectory_entry", trajectory_path))
    world_model.log_artifact(
        experiment_id=experiment_id,
        path=trajectory_path,
        kind="trajectory_entry",
    )
    if figure_path:
        artifact_entries.append(("pareto_figure", figure_path))
        world_model.log_artifact(
            experiment_id=experiment_id,
            path=figure_path,
            kind="pareto_figure",
        )
    world_model.record_stage_history(
        experiment_id=experiment_id,
        cycle=cycle_number,
        stage=governance_stage,
    )
    stage_history_entries = world_model.stage_history(experiment_id)
    adaptation_figures = reporting.collect_adaptation_figures(cfg.reporting_dir)
    anchor_candidates = (
        ("preference_pairs", preference_pairs_anchor),
        ("p3_summary", p3_summary_anchor),
        ("trajectory", trajectory_anchor),
    )
    positioning_artifacts = {
        name: anchor for name, anchor in anchor_candidates if anchor is not None
    }
    if not positioning_artifacts:
        positioning_artifacts = None
    references = [
        "docs/TASKS_CODEX_MINI.md:200-248",
        "docs/TASKS_CODEX_MINI.md:206-238",
        "docs/MASTER_PLAN_AI_SCIENTIST.md:247-368",
    ]
    for anchor in (preference_pairs_anchor, p3_summary_anchor, trajectory_anchor):
        if anchor:
            references.append(anchor)
    reproduction_steps = [
        repro_command,
        f"git checkout {git_sha}",
        f"(cd constellaration && git checkout {constellaration_sha})",
        f"tools.{tool_name}(params, stage='{stage_label}')",
    ]
    content = reporting.build_cycle_report(
        cycle_index=cycle_index,
        problem=cfg.problem,
        screened=len(screen_results),
        promoted=len(promote_results),
        governance_stage=governance_stage,
        best_metrics=best_eval["metrics"],
        config_snapshot=config_snapshot,
        reproduction_steps=reproduction_steps,
        reproduction_snippet=reproduction_snippet,
        environment_block=env_block,
        pareto_lines=pareto_lines,
        p3_summary={
            "hv_score": p3_summary.hv_score,
            "reference_point": p3_summary.reference_point,
            "feasible_count": p3_summary.feasible_count,
            "archive_size": p3_summary.archive_size,
        },
        statements=statements,
        references=references,
        positioning_artifacts=positioning_artifacts,
        stage_history=stage_history_entries,
        artifact_entries=artifact_entries,
        adaptation_figures=adaptation_figures,
        figure_paths=figure_paths,
        out_dir=cfg.reporting_dir,
    )

    title = f"{cfg.problem}_cycle_{cycle_index + 1}"
    report_path = reporting.write_report(title, content, out_dir=cfg.reporting_dir)
    return report_path, best_eval, p3_summary


def run(
    cfg: ai_config.ExperimentConfig, runtime: RunnerCLIConfig | None = None
) -> None:
    index_status = rag.ensure_index()
    runtime_label = (
        f"screen_only={runtime.screen_only} promote_only={runtime.promote_only} "
        f"log_cache_stats={runtime.log_cache_stats} slow={runtime.slow} "
        f"planner={runtime.planner} preset={runtime.run_preset or 'none'}"
        if runtime
        else "default"
    )
    print(
        f"[runner] RAG index ready: {index_status.chunks_indexed} chunks ({index_status.index_path}); runtime={runtime_label}"
    )
    tools.clear_evaluation_cache()
    planner_mode = (
        runtime.planner.lower() if runtime and runtime.planner else "deterministic"
    )
    planning_agent = ai_planner.PlanningAgent() if planner_mode == "agent" else None
    budget_controller = BudgetController(cfg.budgets, cfg.adaptive_budgets)
    last_p3_summary: tools.P3Summary | None = None
    with memory.WorldModel(cfg.memory_db) as world_model:
        git_sha = _resolve_git_sha()
        constellaration_sha = _resolve_git_sha("constellaration")
        experiment_id = world_model.start_experiment(
            _serialize_experiment_config(cfg, constellaration_sha=constellaration_sha),
            git_sha,
            constellaration_sha=constellaration_sha,
        )
        governance_stage = "s1"
        if runtime and runtime.promote_only:
            governance_stage = "s2"
            print("[runner] promote-only flag engaged; starting governance in S2.")
        stage_history: list[CycleSummary] = []
        last_best_objective: float | None = None
        for idx in range(cfg.cycles):
            print(
                f"[runner] starting cycle {idx + 1} stage={governance_stage.upper()} "
                f"screen_budget={cfg.budgets.screen_evals_per_cycle}"
            )
            if planning_agent:
                stage_records = world_model.stage_history(experiment_id)
                stage_payload = [
                    {
                        "cycle": entry.cycle,
                        "stage": entry.stage,
                        "selected_at": entry.selected_at,
                    }
                    for entry in stage_records
                ]
                plan_outcome = planning_agent.plan_cycle(
                    cfg=cfg,
                    cycle_index=idx,
                    stage_history=stage_payload,
                    last_summary=last_p3_summary,
                )
                context_snapshot = json.dumps(plan_outcome.context, indent=2)
                print(f"[planner][cycle={idx + 1}] context:\n{context_snapshot}")
            report_path, best_eval, p3_summary = _run_cycle(
                cfg,
                idx,
                world_model,
                experiment_id,
                governance_stage,
                git_sha,
                constellaration_sha,
                runtime=runtime,
                budget_controller=budget_controller,
            )
            last_p3_summary = p3_summary
            if report_path:
                print(f"[runner] cycle {idx + 1} report saved to {report_path}")
            else:
                print(f"[runner] cycle {idx + 1} aborted (wall-clock or budget).")
            summary = CycleSummary(
                cycle=idx + 1,
                objective=best_eval.get("objective") if best_eval else None,
                feasibility=best_eval.get("feasibility") if best_eval else None,
                hv=best_eval.get("cycle_hv") if best_eval else None,
                stage=governance_stage,
            )
            stage_history.append(summary)
            if best_eval:
                current_objective = best_eval.get("objective")
                reward_diff = 0.0
                if current_objective is not None and last_best_objective is not None:
                    reward_diff = float(current_objective) - float(last_best_objective)
                adaptation_helpers.append_preference_record(
                    base_dir=cfg.reporting_dir,
                    cycle=idx + 1,
                    stage=governance_stage,
                    candidate_hash=best_eval.get("design_hash", "") or "",
                    reward_diff=reward_diff,
                )
                if current_objective is not None:
                    last_best_objective = float(current_objective)
            next_stage = governance_stage
            if governance_stage == "s1":
                if _should_transition_s1_to_s2(stage_history, cfg.stage_gates):
                    next_stage = "s2"
                    print(
                        f"[runner][stage-gate] governance stage advanced to S2 after cycle {idx + 1}"
                    )
            elif governance_stage == "s2":
                if _should_transition_s2_to_s3(
                    stage_history,
                    cfg.stage_gates,
                    cfg.governance,
                    world_model,
                    experiment_id,
                    idx + 1,
                    cfg.cycles,
                ):
                    next_stage = "s3"
                    print(
                        f"[runner][stage-gate] governance stage advanced to S3 after cycle {idx + 1}"
                    )
            governance_stage = next_stage
        batch_summary_path = _export_batch_reports(cfg.reporting_dir, stage_history)
        world_model.log_artifact(
            experiment_id=experiment_id,
            path=batch_summary_path,
            kind="batch_summary",
        )
        usage = world_model.budget_usage(experiment_id)
        print(
            f"[runner] logged {usage.screen_evals} screen + {usage.promoted_evals} promote evaluations ("
            f"{usage.high_fidelity_evals} high-fidelity) into {cfg.memory_db}",
        )


def _serialize_experiment_config(
    cfg: ai_config.ExperimentConfig, constellaration_sha: str | None = None
) -> dict[str, Any]:
    boundary_template = asdict(cfg.boundary_template)
    seed_path = boundary_template.get("seed_path")
    if seed_path is not None:
        boundary_template["seed_path"] = str(seed_path)
    return {
        "problem": cfg.problem,
        "cycles": cfg.cycles,
        "random_seed": cfg.random_seed,
        "budgets": asdict(cfg.budgets),
        "adaptive_budgets": asdict(cfg.adaptive_budgets),
        "proposal_mix": asdict(cfg.proposal_mix),
        "fidelity_ladder": asdict(cfg.fidelity_ladder),
        "boundary_template": boundary_template,
        "stage_gates": asdict(cfg.stage_gates),
        "governance": asdict(cfg.governance),
        "source_config": str(cfg.source_config),
        "reporting_dir": str(cfg.reporting_dir),
        "memory_db": str(cfg.memory_db),
        "constellaration_sha": constellaration_sha or "unknown",
    }


_RUN_PRESETS_PATH = Path("configs/run_presets.yaml")


def _load_run_presets(path: Path | str | None = None) -> dict[str, dict[str, bool]]:
    target = Path(path or _RUN_PRESETS_PATH)
    if not target.exists():
        return {}
    raw = yaml.safe_load(target.read_text(encoding="utf-8")) or {}
    presets: dict[str, dict[str, bool]] = {}
    for key, values in raw.items():
        if not isinstance(values, dict):
            continue
        presets[key] = {
            "screen_only": bool(values.get("screen_only", False)),
            "promote_only": bool(values.get("promote_only", False)),
            "slow": bool(values.get("slow", False)),
        }
    return presets


def _apply_run_preset(cli: RunnerCLIConfig) -> RunnerCLIConfig:
    preset_name = cli.run_preset or os.getenv("AI_SCIENTIST_RUN_PRESET")
    if not preset_name:
        return cli
    presets = _load_run_presets()
    preset = presets.get(preset_name)
    if preset is None:
        raise ValueError(
            "Unknown run preset '%s'; available presets are %s."
            % (preset_name, ", ".join(sorted(presets or ["<none>"])))
        )
    return replace(
        cli,
        screen_only=cli.screen_only or preset["screen_only"],
        promote_only=cli.promote_only or preset["promote_only"],
        slow=cli.slow or preset["slow"],
    )


def _export_batch_reports(
    report_dir: Path | str, history: Sequence[CycleSummary]
) -> Path:
    base_path = Path(report_dir)
    figures_dir = base_path / "figures"
    stage_dir = figures_dir / "batch_stage_summaries"
    figures_dir.mkdir(parents=True, exist_ok=True)
    stage_dir.mkdir(parents=True, exist_ok=True)
    stage_entries: dict[str, list[CycleSummary]] = {}
    for cycle_summary in history:
        stage_entries.setdefault(cycle_summary.stage, []).append(cycle_summary)
    stage_refs: dict[str, dict[str, Any]] = {}
    for stage, entries in stage_entries.items():
        objectives = [
            entry.objective for entry in entries if entry.objective is not None
        ]
        feasibilities = [
            entry.feasibility for entry in entries if entry.feasibility is not None
        ]
        hv_values = [entry.hv for entry in entries if entry.hv is not None]
        stage_payload = {
            "stage": stage,
            "cycles": len(entries),
            "best_objective": max(objectives) if objectives else None,
            "best_feasibility": min(feasibilities) if feasibilities else None,
            "max_hv": max(hv_values) if hv_values else None,
            "entries": [
                {
                    "cycle": entry.cycle,
                    "objective": entry.objective,
                    "feasibility": entry.feasibility,
                    "hv": entry.hv,
                }
                for entry in entries
            ],
        }
        stage_path = stage_dir / f"{stage}_summary.json"
        stage_path.write_text(json.dumps(stage_payload, indent=2), encoding="utf-8")
        stage_refs[stage] = {
            "cycles": len(entries),
            "path": str(stage_path.resolve()),
        }
    summary_payload = {
        "generated_at": datetime.utcnow().isoformat(),
        "total_cycles": len(history),
        "stage_files": stage_refs,
    }
    summary_path = figures_dir / "batch_summary.json"
    summary_path.write_text(json.dumps(summary_payload, indent=2), encoding="utf-8")
    return summary_path


def _resolve_git_sha(repo_path: str | None = None) -> str:
    try:
        completed = subprocess.run(
            ["git", "-C", repo_path, "rev-parse", "HEAD"]
            if repo_path
            else ["git", "rev-parse", "HEAD"],
            check=True,
            capture_output=True,
            text=True,
        )
        return completed.stdout.strip()
    except (subprocess.SubprocessError, FileNotFoundError):
        return "unknown"


def main() -> None:
    try:
        cli = _apply_run_preset(parse_args())
        _validate_runtime_flags(cli)
    except ValueError as exc:
        print(f"[runner] invalid CLI flags: {exc}", file=sys.stderr)
        raise SystemExit(2) from exc
    experiment = ai_config.load_experiment_config(cli.config_path)
    if cli.problem:
        experiment = replace(experiment, problem=cli.problem)
    if cli.cycles:
        experiment = replace(experiment, cycles=cli.cycles)
    if cli.memory_db:
        experiment = replace(experiment, memory_db=cli.memory_db)
    if cli.eval_budget is not None:
        experiment = replace(
            experiment,
            budgets=replace(experiment.budgets, screen_evals_per_cycle=cli.eval_budget),
        )
    if cli.workers is not None:
        experiment = replace(
            experiment,
            budgets=replace(experiment.budgets, n_workers=cli.workers),
        )
    if cli.pool_type is not None:
        experiment = replace(
            experiment,
            budgets=replace(experiment.budgets, pool_type=cli.pool_type),
        )
    if cli.slow:
        experiment = replace(
            experiment,
            budgets=replace(
                experiment.budgets,
                wall_clock_minutes=experiment.budgets.wall_clock_minutes * 1.5,
            ),
        )
    preset_label = cli.run_preset or os.getenv("AI_SCIENTIST_RUN_PRESET") or "none"
    print(
        f"[runner] starting problem={experiment.problem} cycles={experiment.cycles} "
        f"screen_budget={experiment.budgets.screen_evals_per_cycle} "
        f"screen_only={cli.screen_only} promote_only={cli.promote_only} "
        f"log_cache_stats={cli.log_cache_stats} slow={cli.slow} preset={preset_label}"
    )
    run(experiment, runtime=cli)


if __name__ == "__main__":
    main()


================================================================================
File: tools_api.py
================================================================================

"""OpenAI-style tool catalog for K2 models (Wave 8 checklist: docs/TASKS_CODEX_MINI.md:157-190)."""

from __future__ import annotations

from typing import Any, Mapping, Sequence

ToolSchema = Mapping[str, Any]

_TOOL_DEFINITIONS: Sequence[ToolSchema] = (
    {
        "name": "make_boundary",
        "description": (
            "Build a SurfaceRZFourier boundary before submitting it to any evaluator. "
            "Input matches ai_scientist.tools.make_boundary_from_params and keeps r/z coefficients, symmetry, and nfp. "
            "Reference: docs/TASKS_CODEX_MINI.md:157-190."
        ),
        "parameters": {
            "type": "object",
            "properties": {
                "params": {
                    "type": "object",
                    "description": "Dictionary with r_cos/z_sin arrays, n_field_periods (aka nfp), and optional symmetry flag.",
                    "additionalProperties": True,
                }
            },
            "required": ["params"],
            "additionalProperties": False,
        },
    },
    {
        "name": "evaluate_p1",
        "description": (
            "Evaluate the low-fidelity P1 problem (minimize max elongation). "
            "Caller must provide the same params dict used in make_boundary."
        ),
        "parameters": {
            "type": "object",
            "properties": {
                "params": {"type": "object", "description": "Surface coefficients."},
                "problem": {
                    "type": "string",
                    "enum": ["p1"],
                    "description": "Explicitly declare the target problem.",
                },
                "stage": {
                    "type": "string",
                    "description": "Screening stage (default 'screen').",
                },
            },
            "required": ["params"],
            "additionalProperties": False,
        },
    },
    {
        "name": "evaluate_p2",
        "description": (
            "Run the high-fidelity QI-focused P2 evaluation via forward_model(ConstellarationSettings.default_high_fidelity())."
        ),
        "parameters": {
            "type": "object",
            "properties": {
                "params": {"type": "object", "description": "Boundary spec."},
                "problem": {
                    "type": "string",
                    "enum": ["p2"],
                    "description": "Explicit problem identifier.",
                },
                "stage": {"type": "string", "description": "Call tag (default 'p2')."},
            },
            "required": ["params"],
            "additionalProperties": False,
        },
    },
    {
        "name": "evaluate_p3",
        "description": (
            "Run the P3 metrics (aspect ratio + gradient) with high-fidelity settings and return hv-ready metrics."
        ),
        "parameters": {
            "type": "object",
            "properties": {
                "params": {"type": "object", "description": "Boundary parameters."},
                "problem": {
                    "type": "string",
                    "enum": ["p3"],
                    "description": "Explicit problem identifier.",
                },
                "stage": {"type": "string", "description": "Stage tag (default 'p3')."},
            },
            "required": ["params"],
            "additionalProperties": False,
        },
    },
    {
        "name": "retrieve_rag",
        "description": (
            "Fetch K relevant chunks from the ai_scientist/rag_index.db knowledge index."
        ),
        "parameters": {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "Search query that guides retrieval.",
                },
                "k": {
                    "type": "integer",
                    "minimum": 1,
                    "description": "Number of snippets to return (default 3).",
                },
            },
            "required": ["query"],
            "additionalProperties": False,
        },
    },
    {
        "name": "log_citation",
        "description": (
            "Register a doc citation for the current report draft (anchor optional, but include repo path)."
        ),
        "parameters": {
            "type": "object",
            "properties": {
                "source_path": {
                    "type": "string",
                    "description": "File path, e.g. docs/MASTER_PLAN_AI_SCIENTIST.md",
                },
                "anchor": {
                    "type": "string",
                    "description": "Section or line anchor for the claim.",
                },
                "quote": {
                    "type": "string",
                    "description": "Textual quote or paraphrase.",
                },
            },
            "required": ["source_path", "quote"],
            "additionalProperties": False,
        },
    },
    {
        "name": "write_report",
        "description": (
            "Generate or extend a Markdown report with structured sections and references (docs/TASKS_CODEX_MINI.md:157-190)."
        ),
        "parameters": {
            "type": "object",
            "properties": {
                "title": {"type": "string", "description": "Document title."},
                "sections": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "heading": {"type": "string"},
                            "body": {"type": "string"},
                        },
                        "required": ["heading", "body"],
                        "additionalProperties": False,
                    },
                },
                "references": {
                    "type": "array",
                    "items": {"type": "string"},
                },
            },
            "required": ["title", "sections"],
            "additionalProperties": False,
        },
    },
)

TOOL_SCHEMA_BY_NAME = {schema["name"]: schema for schema in _TOOL_DEFINITIONS}


def list_tool_schemas() -> tuple[ToolSchema, ...]:
    """Return all declared OpenAI-style tool definitions."""

    return tuple(_TOOL_DEFINITIONS)


def get_tool_schema(name: str) -> ToolSchema | None:
    """Lookup a schema by tool name."""

    return TOOL_SCHEMA_BY_NAME.get(name)


================================================================================
File: config.py
================================================================================

"""Configuration helpers for the AI Scientist orchestration (Tasks 0.2 + B.*)."""

from __future__ import annotations

from dataclasses import dataclass
import os
from pathlib import Path
from typing import Any, Mapping, Tuple

import yaml

DEFAULT_EXPERIMENT_CONFIG_PATH = Path("configs/experiment.example.yaml")
DEFAULT_MODEL_CONFIG_PATH = Path("configs/model.yaml")
DEFAULT_MEMORY_DB_PATH = Path("reports/ai_scientist.sqlite")


def load(path: str | Path | None = None) -> dict[str, Any]:
    """Return the raw YAML mapping for a given configuration file."""

    target = Path(path) if path is not None else DEFAULT_EXPERIMENT_CONFIG_PATH
    with target.open("r", encoding="utf-8") as handle:
        return yaml.safe_load(handle) or {}


@dataclass(frozen=True)
class AgentGateConfig:
    model_alias: str
    allowed_tools: Tuple[str, ...]
    system_prompt: str | None
    provider_model: str | None


@dataclass(frozen=True)
class ProviderConfig:
    name: str
    base_url: str
    chat_path: str
    auth_env: str
    default_model: str | None
    extra_headers: Tuple[tuple[str, str], ...]


@dataclass(frozen=True)
class ModelConfig:
    base_url: str
    instruct_model: str
    thinking_model: str
    request_timeout_seconds: int
    rate_limit_per_minute: int
    context_length: int
    dtype: str
    tensor_parallel: int
    default_provider: str
    providers: Tuple["ProviderConfig", ...]
    agent_gates: Tuple[AgentGateConfig, ...]

    def get_provider(self, name: str | None = None) -> "ProviderConfig":
        alias = (name or self.default_provider).lower()
        for provider in self.providers:
            if provider.name.lower() == alias:
                return provider
        raise ValueError(f"model provider '{alias}' is not configured")


def _env_override(key: str, default: str) -> str:
    value = os.getenv(key)
    if value is None or value == "":
        return default
    return value


def load_model_config(path: str | Path | None = None) -> ModelConfig:
    payload = load(path or DEFAULT_MODEL_CONFIG_PATH)
    model_data = payload.get("model", {})
    agent_gates_payload = model_data.get("agent_gates") or {}
    agent_gates = tuple(
        _agent_gate_config_from_dict(alias, gate_data)
        for alias, gate_data in agent_gates_payload.items()
    )
    provider_data = model_data.get("providers") or {}
    providers = tuple(
        _provider_config_from_dict(name, data) for name, data in provider_data.items()
    )
    default_provider = str(
        model_data.get("provider", providers[0].name if providers else "openrouter")
    )
    default_provider = _env_override("MODEL_PROVIDER", default_provider)
    instruct_alias = str(model_data.get("instruct_model", "kimi-k2-instruct"))
    instruct_alias = _env_override("AI_SCIENTIST_INSTRUCT_MODEL", instruct_alias)
    thinking_alias = str(model_data.get("thinking_model", "kimi-k2-thinking"))
    thinking_alias = _env_override("AI_SCIENTIST_THINKING_MODEL", thinking_alias)
    return ModelConfig(
        base_url=str(model_data.get("base_url", "http://localhost:8000")),
        instruct_model=instruct_alias,
        thinking_model=thinking_alias,
        request_timeout_seconds=int(model_data.get("request_timeout_seconds", 60)),
        rate_limit_per_minute=int(model_data.get("rate_limit_per_minute", 600)),
        context_length=int(model_data.get("context_length", 8192)),
        dtype=str(model_data.get("dtype", "bf16")),
        tensor_parallel=int(model_data.get("tensor_parallel", 1)),
        default_provider=default_provider,
        providers=providers,
        agent_gates=agent_gates,
    )


@dataclass(frozen=True)
class BudgetConfig:
    screen_evals_per_cycle: int
    promote_top_k: int
    max_high_fidelity_evals_per_cycle: int
    wall_clock_minutes: float
    n_workers: int
    pool_type: str


@dataclass(frozen=True)
class BudgetRangeConfig:
    min: int
    max: int


@dataclass(frozen=True)
class AdaptiveBudgetConfig:
    enabled: bool
    hv_slope_reference: float
    feasibility_target: float
    cache_hit_target: float
    screen_bounds: BudgetRangeConfig
    promote_top_k_bounds: BudgetRangeConfig
    high_fidelity_bounds: BudgetRangeConfig


@dataclass(frozen=True)
class FidelityLadder:
    screen: str
    promote: str


@dataclass(frozen=True)
class BoundaryTemplateConfig:
    n_poloidal_modes: int
    n_toroidal_modes: int
    n_field_periods: int
    base_major_radius: float
    base_minor_radius: float
    perturbation_scale: float
    seed_path: Path | None = None


@dataclass(frozen=True)
class StageGateConfig:
    s1_to_s2_feasibility_margin: float
    s1_to_s2_objective_improvement: float
    s1_to_s2_lookback_cycles: int
    s2_to_s3_hv_delta: float
    s2_to_s3_lookback_cycles: int


@dataclass(frozen=True)
class GovernanceConfig:
    min_feasible_for_promotion: int
    hv_lookback: int


@dataclass(frozen=True)
class ProposalMixConfig:
    constraint_ratio: float
    exploration_ratio: float
    jitter_scale: float
    surrogate_pool_multiplier: float = 2.0


@dataclass(frozen=True)
class ExperimentConfig:
    problem: str
    cycles: int
    random_seed: int
    budgets: BudgetConfig
    adaptive_budgets: AdaptiveBudgetConfig
    fidelity_ladder: FidelityLadder
    boundary_template: BoundaryTemplateConfig
    stage_gates: StageGateConfig
    governance: GovernanceConfig
    proposal_mix: ProposalMixConfig
    reporting_dir: Path
    memory_db: Path
    source_config: Path


def _boundary_template_from_dict(
    data: Mapping[str, Any] | None,
) -> BoundaryTemplateConfig:
    config = data or {}
    seed_path = config.get("seed_path")
    return BoundaryTemplateConfig(
        n_poloidal_modes=int(config.get("n_poloidal_modes", 3)),
        n_toroidal_modes=int(config.get("n_toroidal_modes", 5)),
        n_field_periods=int(config.get("n_field_periods", 1)),
        base_major_radius=float(config.get("base_major_radius", 1.5)),
        base_minor_radius=float(config.get("base_minor_radius", 0.5)),
        perturbation_scale=float(config.get("perturbation_scale", 0.05)),
        seed_path=Path(seed_path) if seed_path else None,
    )


def _stage_gate_config_from_dict(
    data: Mapping[str, Any] | None,
) -> StageGateConfig:
    config = data or {}
    return StageGateConfig(
        s1_to_s2_feasibility_margin=float(
            config.get("s1_to_s2_feasibility_margin", 0.01)
        ),
        s1_to_s2_objective_improvement=float(
            config.get("s1_to_s2_objective_improvement", 0.02)
        ),
        s1_to_s2_lookback_cycles=int(config.get("s1_to_s2_lookback_cycles", 3)),
        s2_to_s3_hv_delta=float(config.get("s2_to_s3_hv_delta", 0.01)),
        s2_to_s3_lookback_cycles=int(config.get("s2_to_s3_lookback_cycles", 3)),
    )


def _governance_config_from_dict(
    data: Mapping[str, Any] | None,
    *,
    default_hv_lookback: int,
) -> GovernanceConfig:
    config = data or {}
    min_feasible = int(config.get("min_feasible_for_promotion", 1))
    min_feasible = max(0, min_feasible)
    hv_lookback = int(config.get("hv_lookback", default_hv_lookback))
    hv_lookback = max(1, hv_lookback)
    return GovernanceConfig(
        min_feasible_for_promotion=min_feasible,
        hv_lookback=hv_lookback,
    )


def _agent_gate_config_from_dict(
    alias: str, data: Mapping[str, Any] | None
) -> AgentGateConfig:
    config = data or {}
    allowed = tuple(str(item) for item in config.get("allowed_tools", []))
    prompt = config.get("system_prompt")
    provider_model = config.get("provider_model")
    return AgentGateConfig(
        model_alias=str(alias),
        allowed_tools=allowed,
        system_prompt=str(prompt) if prompt is not None else None,
        provider_model=(
            str(provider_model) if provider_model is not None else None
        ),
    )


def _extra_headers_from_dict(
    data: Mapping[str, Any] | None,
) -> Tuple[tuple[str, str], ...]:
    config = data or {}
    return tuple((str(key), str(value)) for key, value in config.items())


def _provider_config_from_dict(
    name: str, data: Mapping[str, Any] | None
) -> ProviderConfig:
    config = data or {}
    return ProviderConfig(
        name=str(name),
        base_url=str(config.get("base_url", "")),
        chat_path=str(config.get("chat_path", "/v1/chat/completions")),
        auth_env=str(config.get("auth_env", "")),
        default_model=(
            str(config.get("default_model"))
            if config.get("default_model") is not None
            else None
        ),
        extra_headers=_extra_headers_from_dict(config.get("extra_headers")),
    )


def _proposal_mix_from_dict(
    data: Mapping[str, Any] | None,
) -> ProposalMixConfig:
    config = data or {}
    constraint_ratio = float(config.get("constraint_ratio", 0.7))
    exploration_ratio = float(config.get("exploration_ratio", 0.3))
    return ProposalMixConfig(
        constraint_ratio=constraint_ratio,
        exploration_ratio=exploration_ratio,
        jitter_scale=float(config.get("jitter_scale", 0.01)),
        surrogate_pool_multiplier=float(config.get("surrogate_pool_multiplier", 2.0)),
    )


def _budget_range_from_dict(
    data: Mapping[str, Any] | None,
    *,
    default_value: int,
) -> BudgetRangeConfig:
    config = data or {}
    min_val = int(config.get("min", default_value))
    max_val = int(config.get("max", default_value))
    if min_val > max_val:
        min_val, max_val = max_val, min_val
    min_val = max(0, min_val)
    max_val = max(max_val, min_val)
    return BudgetRangeConfig(min=min_val, max=max_val)


def _adaptive_budget_config_from_dict(
    data: Mapping[str, Any] | None,
    *,
    base_budgets: BudgetConfig,
) -> AdaptiveBudgetConfig:
    config = data or {}
    enabled = bool(config.get("enabled", False))
    hv_reference = float(config.get("hv_slope_reference", 0.05))
    feasibility_target = float(config.get("feasibility_target", 0.5))
    cache_hit_target = float(config.get("cache_hit_target", 0.3))
    screen_bounds = _budget_range_from_dict(
        config.get("screen_evals_per_cycle"),
        default_value=base_budgets.screen_evals_per_cycle,
    )
    promote_bounds = _budget_range_from_dict(
        config.get("promote_top_k"),
        default_value=base_budgets.promote_top_k,
    )
    high_fidelity_bounds = _budget_range_from_dict(
        config.get("max_high_fidelity_evals_per_cycle"),
        default_value=base_budgets.max_high_fidelity_evals_per_cycle,
    )
    return AdaptiveBudgetConfig(
        enabled=enabled,
        hv_slope_reference=max(1e-6, hv_reference),
        feasibility_target=max(1e-6, feasibility_target),
        cache_hit_target=max(1e-6, cache_hit_target),
        screen_bounds=screen_bounds,
        promote_top_k_bounds=promote_bounds,
        high_fidelity_bounds=high_fidelity_bounds,
    )


def load_experiment_config(path: str | Path | None = None) -> ExperimentConfig:
    config_path = Path(path) if path is not None else DEFAULT_EXPERIMENT_CONFIG_PATH
    payload = load(config_path)
    budgets = payload.get("budgets", {})
    fidelity = payload.get("fidelity_ladder", {})
    stage_gates = _stage_gate_config_from_dict(payload.get("stage_gates"))
    governance = _governance_config_from_dict(
        payload.get("governance"),
        default_hv_lookback=stage_gates.s2_to_s3_lookback_cycles,
    )
    budget_config = BudgetConfig(
        screen_evals_per_cycle=int(budgets.get("screen_evals_per_cycle", 1)),
        promote_top_k=int(budgets.get("promote_top_k", 1)),
        max_high_fidelity_evals_per_cycle=int(
            budgets.get("max_high_fidelity_evals_per_cycle", 1)
        ),
        wall_clock_minutes=float(budgets.get("wall_clock_minutes", 5.0)),
        n_workers=int(budgets.get("n_workers", 1)),
        pool_type=str(budgets.get("pool_type", "process")),
    )
    return ExperimentConfig(
        problem=str(payload.get("problem", "p1")),
        cycles=int(payload.get("cycles", 1)),
        random_seed=int(payload.get("random_seed", 0)),
        budgets=budget_config,
        adaptive_budgets=_adaptive_budget_config_from_dict(
            payload.get("adaptive_budgets"),
            base_budgets=budget_config,
        ),
        fidelity_ladder=FidelityLadder(
            screen=str(fidelity.get("screen", "screen")),
            promote=str(fidelity.get("promote", "promote")),
        ),
        boundary_template=_boundary_template_from_dict(
            payload.get("boundary_template")
        ),
        stage_gates=stage_gates,
        governance=governance,
        proposal_mix=_proposal_mix_from_dict(payload.get("proposal_mix")),
        reporting_dir=Path(payload.get("reporting_dir", "reports")),
        memory_db=Path(payload.get("memory_db", DEFAULT_MEMORY_DB_PATH)),
        source_config=config_path,
    )


================================================================================
File: guards.py
================================================================================

"""Repository guardrails for the AI Scientist stack."""

from __future__ import annotations

from pathlib import Path
from typing import Sequence


REQUIRED_PATHS: Sequence[Path] = (
    Path("AGENTS.md"),
    Path("docs/TASKS_CODEX_MINI.md"),
    Path("docs/MASTER_PLAN_AI_SCIENTIST.md"),
    Path("configs/model.yaml"),
    Path("ai_scientist/__init__.py"),
    Path("ai_scientist/runner.py"),
)


class GuardViolation(Exception):
    """Raised when a repository guardrail is violated."""


def verify() -> None:
    """Ensure the repository satisfies the declared guardrails."""

    missing = [str(path) for path in REQUIRED_PATHS if not path.exists()]
    if missing:
        raise GuardViolation(f"missing required paths: {', '.join(missing)}")

    tasks = Path("docs/TASKS_CODEX_MINI.md").read_text()
    if "Task 0.3" not in tasks:
        raise GuardViolation("docs/TASKS_CODEX_MINI.md must mention Task 0.3")

    master_plan = Path("docs/MASTER_PLAN_AI_SCIENTIST.md").read_text()
    if "Score-hacking guardrails" not in master_plan:
        raise GuardViolation(
            "MASTER_PLAN_AI_SCIENTIST.md must mention score guardrails"
        )


================================================================================
File: model_endpoint.py
================================================================================

"""Minimal OpenAI-style K2 endpoint to satisfy Phase 1 model-serving (docs/MASTER_PLAN_AI_SCIENTIST.md:99-110, docs/TASKS_CODEX_MINI.md:247-368)."""

from __future__ import annotations

import json
import logging
import threading
from contextlib import contextmanager
from dataclasses import dataclass
from http.server import BaseHTTPRequestHandler
from socketserver import TCPServer, ThreadingMixIn
from typing import Iterator
from urllib.parse import ParseResult, urlparse, urlunparse

from ai_scientist.config import ModelConfig, load_model_config

_LOGGER = logging.getLogger(__name__)


@dataclass(frozen=True)
class EndpointMetadata:
    """Metadata that mirrors the Phase 1 serving config."""

    context_length: int
    dtype: str
    tensor_parallel: int


@dataclass(frozen=True)
class ModelEndpoint:
    """Information about the reachable K2 endpoint."""

    url: str
    metadata: EndpointMetadata
    provider_name: str
    chat_path: str


class _ThreadedServer(ThreadingMixIn, TCPServer):
    """TCP server that handles requests in threads and reuses its address."""

    allow_reuse_address = True


class _Handler(BaseHTTPRequestHandler):
    metadata: EndpointMetadata
    model_alias: str
    scheme: str
    chat_path: str

    def log_message(
        self, *_: object, **__: object
    ) -> None:  # pragma: no cover - suppress noise
        return

    def do_GET(self) -> None:
        if self.path not in {"/health", "/healthz"}:
            self.send_error(404)
            return
        payload = {
            "status": "ready",
            "model": self.model_alias,
            "metadata": self.metadata.__dict__,
        }
        self._respond(200, payload)

    def do_POST(self) -> None:
        if self.path != self.chat_path:
            self.send_error(404)
            return
        length = int(self.headers.get("Content-Length", 0))
        body = self.rfile.read(length) if length else b"{}"
        payload = json.loads(body.decode("utf-8"))
        response = {
            "id": "mock-k2",
            "object": "chat.completion",
            "model": self.model_alias,
            "choices": [
                {
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": {
                            "type": "tool_call",
                            "tool": payload.get("tool_call", {}).get("name"),
                            "confirmation": "endpoint-ready",
                        },
                    },
                    "finish_reason": "function_call",
                    "function_call": payload.get("tool_call"),
                }
            ],
            "usage": {
                "context_length": self.metadata.context_length,
                "dtype": self.metadata.dtype,
                "tensor_parallel": self.metadata.tensor_parallel,
            },
        }
        self._respond(200, response)

    def _respond(self, status: int, payload: dict) -> None:
        data = json.dumps(payload, separators=(",", ":")).encode("utf-8")
        self.send_response(status)
        self.send_header("Content-Type", "application/json")
        self.send_header("Content-Length", str(len(data)))
        self.end_headers()
        self.wfile.write(data)


def _build_handler(
    metadata: EndpointMetadata, model_alias: str, scheme: str, chat_path: str
) -> type[_Handler]:
    handler = type(
        "_ModelHandler",
        (_Handler,),
        {
            "metadata": metadata,
            "model_alias": model_alias,
            "scheme": scheme,
            "chat_path": chat_path,
        },
    )
    return handler


def _normalize_base_url(base_url: str) -> ParseResult:
    parsed = urlparse(base_url)
    scheme = parsed.scheme or "http"
    netloc = parsed.netloc or parsed.path
    if not netloc:
        netloc = "127.0.0.1"
    return parsed._replace(
        scheme=scheme, netloc=netloc, path="", params="", query="", fragment=""
    )


@contextmanager
def run_model_endpoint(
    config: ModelConfig | None = None,
    provider_name: str | None = None,
) -> Iterator[ModelEndpoint]:
    """Start a lightweight K2 endpoint and yield its URL plus metadata."""

    resolved = config or load_model_config()
    provider = resolved.get_provider(provider_name)
    parsed = _normalize_base_url(resolved.base_url)
    host = parsed.hostname or "127.0.0.1"
    port = parsed.port or 0
    metadata = EndpointMetadata(
        context_length=resolved.context_length,
        dtype=resolved.dtype,
        tensor_parallel=resolved.tensor_parallel,
    )
    handler = _build_handler(
        metadata, resolved.instruct_model, parsed.scheme, provider.chat_path
    )
    with _ThreadedServer((host, port), handler) as server:
        thread = threading.Thread(target=server.serve_forever, daemon=True)
        thread.start()
        live_netloc = f"{server.server_address[0]}:{server.server_address[1]}"
        endpoint_url = urlunparse((parsed.scheme, live_netloc, "", "", "", ""))
        _LOGGER.info(
            "Launched K2 endpoint %s (context=%d, dtype=%s, tp=%d)",
            endpoint_url,
            metadata.context_length,
            metadata.dtype,
            metadata.tensor_parallel,
        )
        try:
            yield ModelEndpoint(
                url=endpoint_url,
                metadata=metadata,
                provider_name=provider.name,
                chat_path=provider.chat_path,
            )
        finally:
            server.shutdown()
            thread.join()


__all__ = ["EndpointMetadata", "ModelEndpoint", "run_model_endpoint"]


================================================================================
File: memory.py
================================================================================

"""SQLite-backed world model for AI Scientist budgeting + logging."""

from __future__ import annotations

import hashlib
import json
import sqlite3
from contextlib import contextmanager
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any, Mapping, Sequence


@dataclass
class PropertyGraph:
    nodes: dict[str, Mapping[str, Any]] = field(default_factory=dict)
    edges: list[tuple[str, str, Mapping[str, Any]]] = field(default_factory=list)

    def add_node(self, node_id: str, **attrs: Any) -> None:
        self.nodes[node_id] = attrs

    def add_edge(self, src: str, dst: str, **attrs: Any) -> None:
        self.edges.append((src, dst, attrs))

    def has_node(self, node_id: str) -> bool:
        return node_id in self.nodes


@dataclass(frozen=True)
class StatementRecord:
    id: int
    experiment_id: int
    cycle: int
    stage: str
    text: str
    status: str
    metrics_id: int | None
    tool_name: str
    tool_input_hash: str
    seed: int | None
    git_sha: str
    repro_cmd: str
    created_at: str


@dataclass(frozen=True)
class StageHistoryEntry:
    cycle: int
    stage: str
    selected_at: str


SCHEMA = """
PRAGMA journal_mode=WAL;

CREATE TABLE IF NOT EXISTS experiments (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    started_at TEXT NOT NULL,
    config_json TEXT NOT NULL,
    git_sha TEXT NOT NULL,
    constellaration_sha TEXT NOT NULL,
    notes TEXT
);

CREATE TABLE IF NOT EXISTS candidates (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    experiment_id INTEGER NOT NULL,
    problem TEXT NOT NULL,
    params_json TEXT NOT NULL,
    seed INTEGER NOT NULL,
    status TEXT NOT NULL,
    design_hash TEXT NOT NULL,
    FOREIGN KEY(experiment_id) REFERENCES experiments(id)
);

CREATE TABLE IF NOT EXISTS metrics (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    candidate_id INTEGER NOT NULL,
    raw_json TEXT NOT NULL,
    feasibility REAL NOT NULL,
    objective REAL,
    hv REAL,
    is_feasible INTEGER NOT NULL,
    FOREIGN KEY(candidate_id) REFERENCES candidates(id)
);

CREATE TABLE IF NOT EXISTS pareto (
    experiment_id INTEGER NOT NULL,
    candidate_id INTEGER NOT NULL,
    PRIMARY KEY (experiment_id, candidate_id),
    FOREIGN KEY(experiment_id) REFERENCES experiments(id),
    FOREIGN KEY(candidate_id) REFERENCES candidates(id)
);

CREATE TABLE IF NOT EXISTS citations (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    experiment_id INTEGER NOT NULL,
    source_path TEXT NOT NULL,
    anchor TEXT,
    quote TEXT,
    FOREIGN KEY(experiment_id) REFERENCES experiments(id)
);

CREATE TABLE IF NOT EXISTS artifacts (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    experiment_id INTEGER NOT NULL,
    path TEXT NOT NULL,
    kind TEXT NOT NULL,
    FOREIGN KEY(experiment_id) REFERENCES experiments(id)
);

CREATE TABLE IF NOT EXISTS budgets (
    experiment_id INTEGER NOT NULL,
    cycle INTEGER NOT NULL,
    screen_evals INTEGER NOT NULL,
    promoted_evals INTEGER NOT NULL,
    high_fidelity_evals INTEGER NOT NULL,
    wall_seconds REAL NOT NULL,
    best_objective REAL,
    best_feasibility REAL,
    best_score REAL,
    best_stage TEXT,
    PRIMARY KEY (experiment_id, cycle),
    FOREIGN KEY(experiment_id) REFERENCES experiments(id)
);

CREATE TABLE IF NOT EXISTS cycle_stats (
    experiment_id INTEGER NOT NULL,
    cycle INTEGER NOT NULL,
    hv_score REAL NOT NULL,
    reference_point TEXT NOT NULL,
    pareto_json TEXT NOT NULL,
    PRIMARY KEY (experiment_id, cycle),
    FOREIGN KEY(experiment_id) REFERENCES experiments(id)
);

CREATE TABLE IF NOT EXISTS cycle_hv (
    experiment_id INTEGER NOT NULL,
    cycle INTEGER NOT NULL,
    hv_value REAL NOT NULL,
    hv_delta REAL,
    hv_delta_moving_avg REAL,
    n_feasible INTEGER NOT NULL,
    n_archive INTEGER NOT NULL,
    PRIMARY KEY (experiment_id, cycle),
    FOREIGN KEY(experiment_id) REFERENCES experiments(id)
);

CREATE TABLE IF NOT EXISTS pareto_archive (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    experiment_id INTEGER NOT NULL,
    cycle INTEGER NOT NULL,
    design_hash TEXT NOT NULL,
    fidelity TEXT NOT NULL,
    gradient REAL NOT NULL,
    aspect REAL NOT NULL,
    metrics_id INTEGER,
    git_sha TEXT,
    constellaration_sha TEXT,
    settings_json TEXT NOT NULL,
    seed INTEGER NOT NULL,
    UNIQUE(experiment_id, cycle, design_hash, fidelity),
    FOREIGN KEY(experiment_id) REFERENCES experiments(id),
    FOREIGN KEY(metrics_id) REFERENCES metrics(id)
);

CREATE TABLE IF NOT EXISTS statements (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    experiment_id INTEGER NOT NULL,
    cycle INTEGER NOT NULL,
    stage TEXT NOT NULL,
    text TEXT NOT NULL,
    status TEXT NOT NULL DEFAULT 'PENDING',
    metrics_id INTEGER,
    tool_name TEXT NOT NULL,
    tool_input_hash TEXT NOT NULL,
    seed INTEGER,
    git_sha TEXT NOT NULL,
    repro_cmd TEXT NOT NULL,
    created_at TEXT NOT NULL,
    FOREIGN KEY(experiment_id) REFERENCES experiments(id),
    FOREIGN KEY(metrics_id) REFERENCES metrics(id),
    UNIQUE(experiment_id, cycle, tool_input_hash)
);

CREATE TABLE IF NOT EXISTS stage_history (
    experiment_id INTEGER NOT NULL,
    cycle INTEGER NOT NULL,
    stage TEXT NOT NULL,
    selected_at TEXT NOT NULL,
    PRIMARY KEY (experiment_id, cycle),
    FOREIGN KEY(experiment_id) REFERENCES experiments(id)
);

CREATE TABLE IF NOT EXISTS deterministic_snapshots (
    experiment_id INTEGER NOT NULL,
    cycle INTEGER NOT NULL,
    snapshot_json TEXT NOT NULL,
    constellaration_sha TEXT NOT NULL,
    seed INTEGER NOT NULL,
    created_at TEXT NOT NULL,
    PRIMARY KEY (experiment_id, cycle),
    FOREIGN KEY(experiment_id) REFERENCES experiments(id)
);
"""

DEFAULT_RELATIVE_TOLERANCE = 1e-2


@dataclass(frozen=True)
class BudgetUsage:
    screen_evals: int
    promoted_evals: int
    high_fidelity_evals: int
    wall_seconds: float


def _normalize_to_json(value: Any) -> Any:
    if isinstance(value, (str, int, float, bool)) or value is None:
        return value
    if isinstance(value, Mapping):
        return {str(key): _normalize_to_json(val) for key, val in value.items()}
    if isinstance(value, (list, tuple)):
        return [_normalize_to_json(val) for val in value]
    return str(value)


def _hash_payload(payload: Mapping[str, Any]) -> str:
    serialized = json.dumps(
        _normalize_to_json(payload), sort_keys=True, separators=(",", ":")
    )
    return hashlib.sha256(serialized.encode("utf-8")).hexdigest()


def hash_payload(payload: Mapping[str, Any]) -> str:
    """Return a deterministic digest that mirrors the statements table hashing."""

    return _hash_payload(payload)


class WorldModel:
    """Simple SQLite wrapper for experiments, candidates, and budgets."""

    def __init__(self, path: str | Path) -> None:
        db_path = Path(path)
        init_db(db_path)
        self.db_path = db_path
        self._conn: sqlite3.Connection = sqlite3.connect(
            str(self.db_path), check_same_thread=False
        )
        self._conn.row_factory = sqlite3.Row

    def __enter__(self) -> "WorldModel":
        return self

    def __exit__(self, exc_type, exc, tb) -> None:  # pragma: no cover - context helper
        self.close()

    def close(self) -> None:
        self._conn.close()

    @contextmanager
    def transaction(self):
        """Context manager that wraps multiple writes in a single atomic transaction."""

        try:
            self._conn.execute("BEGIN")
            yield
        except Exception:
            self._conn.rollback()
            raise
        else:
            self._conn.commit()

    def start_experiment(
        self,
        config_payload: Mapping[str, Any],
        git_sha: str,
        constellaration_sha: str | None = None,
        notes: str | None = None,
    ) -> int:
        payload = json.dumps(_normalize_to_json(config_payload), separators=(",", ":"))
        cursor = self._conn.execute(
            "INSERT INTO experiments (started_at, config_json, git_sha, constellaration_sha, notes) VALUES (?, ?, ?, ?, ?)",
            (
                datetime.utcnow().isoformat(),
                payload,
                git_sha,
                constellaration_sha or "unknown",
                notes,
            ),
        )
        lastrowid = cursor.lastrowid
        assert lastrowid is not None
        self._conn.commit()
        return lastrowid

    def record_cycle(
        self,
        experiment_id: int,
        cycle_number: int,
        screen_evals: int,
        promoted_evals: int,
        high_fidelity_evals: int,
        wall_seconds: float,
        best_params: Mapping[str, Any],
        best_evaluation: Mapping[str, Any],
        seed: int,
        problem: str,
        *,
        log_best_candidate: bool = True,
        commit: bool = True,
    ) -> None:
        payload = (
            experiment_id,
            cycle_number,
            screen_evals,
            promoted_evals,
            high_fidelity_evals,
            wall_seconds,
            best_evaluation.get("objective"),
            best_evaluation.get("feasibility"),
            best_evaluation.get("score"),
            best_evaluation.get("stage", ""),
        )

        def _write() -> None:
            self._conn.execute(
                "INSERT OR REPLACE INTO budgets (experiment_id, cycle, screen_evals, promoted_evals, high_fidelity_evals, wall_seconds, best_objective, best_feasibility, best_score, best_stage) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)",
                payload,
            )
            if log_best_candidate:
                self.log_candidate(
                    experiment_id=experiment_id,
                    problem=problem,
                    params=best_params,
                    seed=seed,
                    status=best_evaluation.get("stage", "unknown"),
                    evaluation=best_evaluation,
                    design_hash=best_evaluation.get("design_hash", ""),
                    commit=False,
                )

        if commit:
            with self._conn:
                _write()
        else:
            _write()

    def record_deterministic_snapshot(
        self,
        experiment_id: int,
        cycle_number: int,
        snapshot: Mapping[str, Any],
        *,
        constellaration_sha: str,
        seed: int,
        created_at: str | None = None,
        commit: bool = True,
    ) -> None:
        payload = json.dumps(_normalize_to_json(snapshot), separators=(",", ":"))
        timestamp = created_at or datetime.utcnow().isoformat()

        def _write() -> None:
            self._conn.execute(
                """
                INSERT OR REPLACE INTO deterministic_snapshots
                (experiment_id, cycle, snapshot_json, constellaration_sha, seed, created_at)
                VALUES (?, ?, ?, ?, ?, ?)
                """,
                (
                    experiment_id,
                    cycle_number,
                    payload,
                    constellaration_sha,
                    seed,
                    timestamp,
                ),
            )

        if commit:
            with self._conn:
                _write()
        else:
            _write()

    def record_cycle_hv(
        self,
        experiment_id: int,
        cycle_number: int,
        hv_score: float,
        reference_point: Sequence[float],
        pareto_entries: Sequence[Mapping[str, float]],
        *,
        n_feasible: int,
        n_archive: int,
        hv_lookback: int | None = None,
        commit: bool = True,
    ) -> None:
        snapshot = {
            "reference_point": list(reference_point),
            "pareto": [_normalize_to_json(entry) for entry in pareto_entries],
        }

        payload_stats = (
            experiment_id,
            cycle_number,
            hv_score,
            json.dumps(list(reference_point)),
            json.dumps(snapshot, separators=(",", ":")),
        )

        prev_row = self._conn.execute(
            "SELECT hv_value FROM cycle_hv WHERE experiment_id = ? ORDER BY cycle DESC LIMIT 1",
            (experiment_id,),
        ).fetchone()
        previous_hv = float(prev_row["hv_value"]) if prev_row else None
        hv_delta: float | None = None
        if previous_hv is not None:
            hv_delta = float(abs(hv_score - previous_hv))
        hv_delta_moving_avg: float | None = None
        if hv_delta is not None and hv_lookback is not None and hv_lookback > 0:
            lookback_limit = hv_lookback - 1
            delta_rows = []
            if lookback_limit > 0:
                delta_rows = self._conn.execute(
                    "SELECT hv_delta FROM cycle_hv WHERE experiment_id = ? AND hv_delta IS NOT NULL ORDER BY cycle DESC LIMIT ?",
                    (experiment_id, lookback_limit),
                ).fetchall()
            recent_deltas = [
                row["hv_delta"] for row in delta_rows if row["hv_delta"] is not None
            ]
            moving_window = [hv_delta] + recent_deltas[:lookback_limit]
            if len(moving_window) >= hv_lookback:
                hv_delta_moving_avg = float(sum(moving_window) / len(moving_window))

        payload_hv = (
            experiment_id,
            cycle_number,
            hv_score,
            hv_delta,
            hv_delta_moving_avg,
            n_feasible,
            n_archive,
        )

        def _write() -> None:
            self._conn.execute(
                "INSERT OR REPLACE INTO cycle_stats (experiment_id, cycle, hv_score, reference_point, pareto_json) VALUES (?, ?, ?, ?, ?)",
                payload_stats,
            )
            self._conn.execute(
                "INSERT OR REPLACE INTO cycle_hv (experiment_id, cycle, hv_value, hv_delta, hv_delta_moving_avg, n_feasible, n_archive) VALUES (?, ?, ?, ?, ?, ?, ?)",
                payload_hv,
            )

        if commit:
            with self._conn:
                _write()
        else:
            _write()

    def average_recent_hv_delta(
        self,
        experiment_id: int,
        lookback: int,
    ) -> float | None:
        if lookback <= 0:
            return None
        rows = self._conn.execute(
            "SELECT hv_delta FROM cycle_hv WHERE experiment_id = ? AND hv_delta IS NOT NULL ORDER BY cycle DESC LIMIT ?",
            (experiment_id, lookback),
        ).fetchall()
        deltas = [row["hv_delta"] for row in rows if row["hv_delta"] is not None]
        if len(deltas) < lookback:
            return None
        return float(sum(deltas) / len(deltas))

    def log_candidate(
        self,
        experiment_id: int,
        problem: str,
        params: Mapping[str, Any],
        seed: int,
        status: str,
        evaluation: Mapping[str, Any],
        *,
        design_hash: str,
        commit: bool = True,
    ) -> tuple[int, int]:
        params_json = json.dumps(_normalize_to_json(params), separators=(",", ":"))
        cursor = self._conn.execute(
            "INSERT INTO candidates (experiment_id, problem, params_json, seed, status, design_hash) VALUES (?, ?, ?, ?, ?, ?)",
            (experiment_id, problem, params_json, seed, status, design_hash),
        )
        candidate_id = cursor.lastrowid
        assert candidate_id is not None
        metrics_id = self.log_metrics(
            candidate_id,
            evaluation.get("metrics", {}),
            feasibility=float(evaluation.get("feasibility", 0.0)),
            objective=evaluation.get("objective"),
            hv=evaluation.get("hv"),
            commit=False,
        )
        if commit:
            self._conn.commit()
        return candidate_id, metrics_id

    def log_artifact(
        self,
        experiment_id: int,
        path: str | Path,
        kind: str,
        *,
        commit: bool = True,
    ) -> None:
        """Record artifacts such as metrics snapshots or Pareto figures."""

        self._conn.execute(
            "INSERT INTO artifacts (experiment_id, path, kind) VALUES (?, ?, ?)",
            (experiment_id, str(path), kind),
        )
        if commit:
            self._conn.commit()

    def log_statement(
        self,
        experiment_id: int,
        cycle: int,
        stage: str,
        text: str,
        status: str,
        tool_name: str,
        tool_input: Mapping[str, Any],
        *,
        metrics_id: int | None = None,
        seed: int | None = None,
        git_sha: str,
        repro_cmd: str,
        created_at: str | None = None,
        commit: bool = True,
    ) -> int:
        digest = _hash_payload(tool_input)
        timestamp = created_at or datetime.utcnow().isoformat()
        cursor = self._conn.execute(
            """
            INSERT INTO statements
            (experiment_id, cycle, stage, text, status, metrics_id, tool_name, tool_input_hash, seed, git_sha, repro_cmd, created_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """,
            (
                experiment_id,
                cycle,
                stage,
                text,
                status,
                metrics_id,
                tool_name,
                digest,
                int(seed) if seed is not None else None,
                git_sha,
                repro_cmd,
                timestamp,
            ),
        )
        statement_id = cursor.lastrowid
        assert statement_id is not None
        if commit:
            self._conn.commit()
        return statement_id

    def statements_for_cycle(
        self, experiment_id: int, cycle: int
    ) -> list[StatementRecord]:
        rows = self._conn.execute(
            """
            SELECT *
            FROM statements
            WHERE experiment_id = ? AND cycle = ?
            ORDER BY id ASC
            """,
            (experiment_id, cycle),
        ).fetchall()
        history: list[StatementRecord] = []
        for row in rows:
            history.append(
                StatementRecord(
                    id=row["id"],
                    experiment_id=row["experiment_id"],
                    cycle=row["cycle"],
                    stage=row["stage"],
                    text=row["text"],
                    status=row["status"],
                    metrics_id=row["metrics_id"],
                    tool_name=row["tool_name"],
                    tool_input_hash=row["tool_input_hash"],
                    seed=row["seed"],
                    git_sha=row["git_sha"],
                    repro_cmd=row["repro_cmd"],
                    created_at=row["created_at"],
                )
            )
        return history

    def stage_history(self, experiment_id: int) -> list[StageHistoryEntry]:
        rows = self._conn.execute(
            """
            SELECT cycle, stage, selected_at
            FROM stage_history
            WHERE experiment_id = ?
            ORDER BY cycle ASC
            """,
            (experiment_id,),
        ).fetchall()
        return [
            StageHistoryEntry(
                cycle=row["cycle"],
                stage=row["stage"],
                selected_at=row["selected_at"],
            )
            for row in rows
        ]

    def record_stage_history(
        self,
        experiment_id: int,
        cycle: int,
        stage: str,
        *,
        selected_at: str | None = None,
        commit: bool = True,
    ) -> None:
        timestamp = selected_at or datetime.utcnow().isoformat()
        self._conn.execute(
            """
            INSERT OR REPLACE INTO stage_history (experiment_id, cycle, stage, selected_at)
            VALUES (?, ?, ?, ?)
            """,
            (experiment_id, cycle, stage, timestamp),
        )
        if commit:
            self._conn.commit()

    def recent_stage_candidates(
        self,
        experiment_id: int,
        problem: str,
        stage: str,
        *,
        limit: int = 64,
    ) -> list[tuple[Mapping[str, Any], float]]:
        """Return parameters + feasibility for the most recent candidates of a given stage."""

        rows = self._conn.execute(
            """
            SELECT c.params_json, m.feasibility
            FROM candidates c
            JOIN metrics m ON m.candidate_id = c.id
            WHERE c.experiment_id = ? AND c.problem = ? AND c.status = ?
            ORDER BY m.id DESC
            LIMIT ?
            """,
            (experiment_id, problem, stage, limit),
        ).fetchall()
        records: list[tuple[Mapping[str, Any], float]] = []
        for row in rows:
            raw_params = row["params_json"]
            feasibility = row["feasibility"]
            if feasibility is None:
                continue
            params = json.loads(raw_params)
            records.append((params, float(feasibility)))
        return records

    def previous_best_hv(self, experiment_id: int, cycle_number: int) -> float | None:
        row = self._conn.execute(
            """
            SELECT MAX(hv_value) AS max_hv
            FROM cycle_hv
            WHERE experiment_id = ? AND cycle < ?
            """,
            (experiment_id, cycle_number),
        ).fetchone()
        if row is None:
            return None
        max_hv = row["max_hv"]
        return float(max_hv) if max_hv is not None else None

    def log_metrics(
        self,
        candidate_id: int,
        metrics_payload: Mapping[str, Any],
        *,
        feasibility: float | None = None,
        objective: float | None = None,
        hv: float | None = None,
        commit: bool = True,
    ) -> int:
        payload = json.dumps(_normalize_to_json(metrics_payload), separators=(",", ":"))
        feasibility_value = float(
            feasibility
            if feasibility is not None
            else metrics_payload.get("feasibility", 0.0)
        )

        def _coerce_float(value: Any | None) -> float | None:
            if value is None:
                return None
            return float(value)

        objective_value = (
            _coerce_float(objective)
            if objective is not None
            else _coerce_float(metrics_payload.get("objective"))
        )
        hv_value = (
            _coerce_float(hv)
            if hv is not None
            else _coerce_float(metrics_payload.get("hv"))
        )
        cursor = self._conn.execute(
            "INSERT INTO metrics (candidate_id, raw_json, feasibility, objective, hv, is_feasible) VALUES (?, ?, ?, ?, ?, ?)",
            (
                candidate_id,
                payload,
                feasibility_value,
                objective_value,
                hv_value,
                1 if feasibility_value <= DEFAULT_RELATIVE_TOLERANCE else 0,
            ),
        )
        metrics_id = cursor.lastrowid
        assert metrics_id is not None
        if commit:
            self._conn.commit()
        return metrics_id

    def record_pareto_archive(
        self,
        experiment_id: int,
        cycle_number: int,
        entries: Sequence[Mapping[str, Any]],
        *,
        commit: bool = True,
    ) -> None:
        def _write() -> None:
            for entry in entries:
                self._conn.execute(
                    """
                    INSERT OR REPLACE INTO pareto_archive
                    (experiment_id, cycle, design_hash, fidelity, gradient, aspect, metrics_id, git_sha, constellaration_sha, settings_json, seed)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """,
                    (
                        experiment_id,
                        cycle_number,
                        entry["design_hash"],
                        entry["fidelity"],
                        float(entry["gradient"]),
                        float(entry["aspect"]),
                        entry.get("metrics_id"),
                        entry.get("git_sha"),
                        entry.get("constellaration_sha"),
                        entry.get("settings_json"),
                        int(entry.get("seed", -1)),
                    ),
                )

        if commit:
            with self._conn:
                _write()
        else:
            _write()

    def budget_usage(self, experiment_id: int) -> BudgetUsage:
        row = self._conn.execute(
            "SELECT COALESCE(SUM(screen_evals), 0), COALESCE(SUM(promoted_evals), 0), COALESCE(SUM(high_fidelity_evals), 0), COALESCE(SUM(wall_seconds), 0.0) FROM budgets WHERE experiment_id = ?",
            (experiment_id,),
        ).fetchone()
        return BudgetUsage(
            screen_evals=int(row[0]),
            promoted_evals=int(row[1]),
            high_fidelity_evals=int(row[2]),
            wall_seconds=float(row[3]),
        )

    def cycles_completed(self, experiment_id: int) -> int:
        row = self._conn.execute(
            "SELECT COUNT(*) FROM budgets WHERE experiment_id = ?",
            (experiment_id,),
        ).fetchone()
        return int(row[0])

    def upsert_pareto(
        self, experiment_id: int, candidate_id: int, *, commit: bool = True
    ) -> None:
        self._conn.execute(
            "INSERT OR REPLACE INTO pareto (experiment_id, candidate_id) VALUES (?, ?)",
            (experiment_id, candidate_id),
        )
        if commit:
            self._conn.commit()

    def to_networkx(self, experiment_id: int) -> PropertyGraph:
        graph = PropertyGraph()
        experiment = self._conn.execute(
            "SELECT * FROM experiments WHERE id = ?",
            (experiment_id,),
        ).fetchone()
        if experiment is None:
            raise ValueError(f"experiment {experiment_id} does not exist")
        exp_node = f"experiment:{experiment_id}"
        graph.add_node(
            exp_node,
            type="experiment",
            started_at=experiment["started_at"],
            git_sha=experiment["git_sha"],
            notes=experiment["notes"],
        )
        candidate_rows = self._conn.execute(
            "SELECT * FROM candidates WHERE experiment_id = ?",
            (experiment_id,),
        ).fetchall()
        for candidate in candidate_rows:
            candidate_node = f"candidate:{candidate['id']}"
            graph.add_node(
                candidate_node,
                type="candidate",
                problem=candidate["problem"],
                status=candidate["status"],
                seed=candidate["seed"],
                params=candidate["params_json"],
            )
            graph.add_edge(exp_node, candidate_node, relation="contains")
        metrics_rows = self._conn.execute(
            "SELECT m.*, c.problem FROM metrics m JOIN candidates c ON m.candidate_id = c.id WHERE c.experiment_id = ?",
            (experiment_id,),
        ).fetchall()
        for metrics in metrics_rows:
            metrics_node = f"metrics:{metrics['id']}"
            graph.add_node(
                metrics_node,
                type="metrics",
                feasibility=metrics["feasibility"],
                objective=metrics["objective"],
                hv=metrics["hv"],
                raw=metrics["raw_json"],
                problem=metrics["problem"],
            )
            candidate_node = f"candidate:{metrics['candidate_id']}"
            graph.add_edge(candidate_node, metrics_node, relation="evaluated_as")
        citations = self._conn.execute(
            "SELECT * FROM citations WHERE experiment_id = ?",
            (experiment_id,),
        ).fetchall()
        for citation in citations:
            citation_node = f"citation:{citation['id']}"
            graph.add_node(
                citation_node,
                type="citation",
                source_path=citation["source_path"],
                anchor=citation["anchor"],
                quote=citation["quote"],
            )
            graph.add_edge(exp_node, citation_node, relation="cites")
        artifacts = self._conn.execute(
            "SELECT * FROM artifacts WHERE experiment_id = ?",
            (experiment_id,),
        ).fetchall()
        for artifact in artifacts:
            artifact_node = f"artifact:{artifact['id']}"
            graph.add_node(
                artifact_node,
                type="artifact",
                path=artifact["path"],
                kind=artifact["kind"],
            )
            graph.add_edge(exp_node, artifact_node, relation="produces")
        budgets = self._conn.execute(
            "SELECT * FROM budgets WHERE experiment_id = ?",
            (experiment_id,),
        ).fetchall()
        for budget in budgets:
            budget_node = f"budget:{experiment_id}:{budget['cycle']}"
            graph.add_node(
                budget_node,
                type="budget",
                cycle=budget["cycle"],
                screen_evals=budget["screen_evals"],
                promoted_evals=budget["promoted_evals"],
                high_fidelity_evals=budget["high_fidelity_evals"],
                wall_seconds=budget["wall_seconds"],
                best_stage=budget["best_stage"],
            )
            graph.add_edge(exp_node, budget_node, relation="cycle")
        stage_rows = self._conn.execute(
            "SELECT * FROM stage_history WHERE experiment_id = ?",
            (experiment_id,),
        ).fetchall()
        for stage_row in stage_rows:
            stage_node = f"stage:{experiment_id}:{stage_row['cycle']}"
            graph.add_node(
                stage_node,
                type="stage",
                cycle=stage_row["cycle"],
                stage=stage_row["stage"],
                selected_at=stage_row["selected_at"],
            )
            graph.add_edge(exp_node, stage_node, relation="governance")
        pareto_rows = self._conn.execute(
            "SELECT candidate_id FROM pareto WHERE experiment_id = ?",
            (experiment_id,),
        ).fetchall()
        for pareto in pareto_rows:
            candidate_node = f"candidate:{pareto['candidate_id']}"
            if graph.has_node(candidate_node):
                graph.add_edge(exp_node, candidate_node, relation="pareto_member")
        return graph

    def surrogate_training_data(
        self,
        *,
        target: str = "hv",
        problem: str | None = None,
    ) -> list[tuple[Mapping[str, Any], float]]:
        """Return cached metrics + target values usable by the surrogate ranker."""

        allowed_targets = {"hv", "objective", "feasibility"}
        target_column = target if target in allowed_targets else "hv"
        rows = self._conn.execute(
            """
            SELECT c.problem, c.params_json, m.raw_json, m.hv, m.objective, m.feasibility
            FROM metrics m
            JOIN candidates c ON m.candidate_id = c.id
            ORDER BY m.id ASC
            """
        ).fetchall()
        history: list[tuple[Mapping[str, Any], float]] = []
        for row in rows:
            if problem is not None and row["problem"] != problem:
                continue
            value = row[target_column]
            if value is None:
                continue
            metrics_payload = json.loads(row["raw_json"])
            try:
                params_payload = json.loads(row["params_json"])
            except (TypeError, ValueError):
                params_payload = None
            if params_payload is not None:
                metrics_payload["candidate_params"] = params_payload
            history.append((metrics_payload, float(value)))
        return history


def init_db(path: str | Path) -> None:
    db_path = Path(path)
    db_path.parent.mkdir(parents=True, exist_ok=True)
    con = sqlite3.connect(str(db_path))
    try:
        con.executescript(SCHEMA)
        try:
            con.execute(
                "ALTER TABLE candidates ADD COLUMN design_hash TEXT NOT NULL DEFAULT ''"
            )
        except sqlite3.OperationalError:
            pass
        try:
            con.execute(
                "ALTER TABLE experiments ADD COLUMN constellaration_sha TEXT NOT NULL DEFAULT 'unknown'"
            )
        except sqlite3.OperationalError:
            pass
        try:
            con.execute("ALTER TABLE cycle_hv ADD COLUMN hv_delta REAL")
        except sqlite3.OperationalError:
            pass
        try:
            con.execute("ALTER TABLE cycle_hv ADD COLUMN hv_delta_moving_avg REAL")
        except sqlite3.OperationalError:
            pass
        con.commit()
    finally:
        con.close()


================================================================================
File: adapter.py
================================================================================

"""PEFT/LoRA adapter integration for Wave 7 adaptation hooks (docs/WAVE_7_ADAPTATION.md).

Set ``AI_SCIENTIST_PEFT=1`` to activate adapter loading before tool calls and queue updates
in ``reports/adapters/{tool}/{stage}/adapter.safetensors`` / ``reports/adapters/queue.jsonl``."""

from __future__ import annotations

import json
import logging
import os
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any, Callable, Mapping, Protocol

_LOGGER = logging.getLogger(__name__)
_ENV_VAR = "AI_SCIENTIST_PEFT"
_ADAPTERS_ROOT = Path("reports") / "adapters"
_QUEUE_PATH = _ADAPTERS_ROOT / "queue.jsonl"
_PERSIST_DIR_ENV = "AI_SCIENTIST_ADAPTER_PERSIST_DIR"


AdapterLoader = Callable[[Path, str, str], bool]
AdapterPersistHandler = Callable[[Path, str, str], bool]

_REGISTERED_LOADERS: list[tuple[str, AdapterLoader]] = []
_REGISTERED_PERSISTERS: list[tuple[str, AdapterPersistHandler]] = []


def register_adapter_loader(name: str, loader: AdapterLoader) -> None:
    """Register a callable that can load adapters for the runner (HF PEFT, ggml, etc.)."""

    _REGISTERED_LOADERS.append((name, loader))


def register_adapter_persist_handler(name: str, handler: AdapterPersistHandler) -> None:
    """Register a callable that persists adapters produced during a cycle run."""

    _REGISTERED_PERSISTERS.append((name, handler))


def _adapter_bundle_path(tool_name: str, stage: str) -> Path:
    normalized_stage = stage.lower().strip()
    return _ADAPTERS_ROOT / tool_name / normalized_stage / "adapter.safetensors"


def _ensure_adapter_directory(path: Path) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)


def _queue_entry(
    tool_name: str, stage: str, adapter_path: Path, backend: str | None, status: str
) -> dict[str, Any]:
    return {
        "tool": tool_name,
        "stage": stage,
        "adapter_path": adapter_path.as_posix(),
        "backend": backend,
        "status": status,
        "timestamp": datetime.utcnow().replace(microsecond=0).isoformat() + "Z",
    }


def _append_to_queue(entry: Mapping[str, Any]) -> None:
    _ensure_adapter_directory(_QUEUE_PATH)
    with _QUEUE_PATH.open("a", encoding="utf-8") as handle:
        handle.write(json.dumps(entry))
        handle.write("\n")


def record_adapter_refresh(
    tool_name: str,
    stage: str,
    *,
    backend: str | None = None,
    status: str = "refreshed",
    adapter_path: Path | None = None,
) -> None:
    """Expose queue logging so offline adapters can annotate backend refreshes."""

    bundle_path = adapter_path or _adapter_bundle_path(tool_name, stage)
    entry = _queue_entry(tool_name, stage, bundle_path, backend, status)
    _append_to_queue(entry)


def _try_load_adapter(adapter_path: Path, tool_name: str, stage: str) -> str | None:
    for backend_name, loader in _REGISTERED_LOADERS:
        if loader(adapter_path, tool_name, stage):
            return backend_name
    return None


def _try_persist_adapter(adapter_path: Path, tool_name: str, stage: str) -> str | None:
    for backend_name, handler in _REGISTERED_PERSISTERS:
        if handler(adapter_path, tool_name, stage):
            return backend_name
    return None


def _json_metadata_loader(adapter_path: Path, tool_name: str, stage: str) -> bool:
    """Load JSON-based adapter bundles for inspection."""

    try:
        raw = adapter_path.read_text(encoding="utf-8").strip()
    except (FileNotFoundError, OSError) as exc:  # pragma: no cover - upstream guard
        _LOGGER.debug(
            "JSON adapter loader missing %s:%s (%s)",
            tool_name,
            stage,
            exc,
        )
        return False

    if not raw:
        _LOGGER.debug("JSON adapter %s:%s is empty", tool_name, stage)
        return True

    try:
        payload = json.loads(raw)
    except json.JSONDecodeError:
        _LOGGER.warning(
            "JSON adapter %s:%s failed to parse; treating as raw bytes",
            tool_name,
            stage,
        )
        return True

    _LOGGER.info(
        "Loaded JSON adapter %s:%s summary=%s",
        tool_name,
        stage,
        {
            "entry_count": payload.get("preference_pair_count"),
            "dataset_path": payload.get("dataset_path"),
        },
    )
    return True


def _staged_adapter_persist(adapter_path: Path, tool_name: str, stage: str) -> bool:
    """Promote staged adapter bundles when a staging directory is configured."""

    persist_root = os.getenv(_PERSIST_DIR_ENV)
    if not persist_root:
        return False

    normalized_stage = stage.lower().strip()
    staged_path = (
        Path(persist_root) / tool_name / normalized_stage / "adapter.safetensors"
    )
    if not staged_path.exists():
        return False

    _ensure_adapter_directory(adapter_path)
    staged_path.replace(adapter_path)
    _LOGGER.info(
        "Persisted staged adapter %s:%s from %s",
        tool_name,
        stage,
        staged_path,
    )
    return True


class ProblemEvaluator(Protocol):
    """Lightweight protocol that mirrors the evaluator interface used in runner.py."""

    def __call__(
        self,
        boundary_params: Mapping[str, Any],
        *,
        stage: str,
        use_cache: bool = True,
    ) -> dict[str, Any]: ...


@dataclass(frozen=True)
class AdapterState:
    """Tracks which LoRA weights were loaded/applied so Wave 7 can replay the stack."""

    loaded: dict[str, str] = field(default_factory=dict)
    updates: list[str] = field(default_factory=list)

    def load_lora_weights(self, label: str, stage: str) -> None:
        """Record when a LoRA bundle is staged for the current tool/stage."""
        bundle_path = _adapter_bundle_path(label, stage)
        if not bundle_path.exists():
            self.loaded[f"{label}:{stage}"] = "missing"
            _LOGGER.debug(
                "Adapter bundle not found for %s:%s at %s",
                label,
                stage,
                bundle_path,
            )
            return

        backend_name = _try_load_adapter(bundle_path, label, stage)
        status = (
            f"{backend_name}:{bundle_path.as_posix()}"
            if backend_name
            else f"ready:{bundle_path.as_posix()}"
        )
        self.loaded[f"{label}:{stage}"] = status
        _LOGGER.debug(
            "AdapterState.load_lora_weights label=%s stage=%s status=%s",
            label,
            stage,
            status,
        )

    def push_updates(self, label: str, stage: str) -> None:
        """Log when downstream adapters propagated updates."""
        bundle_path = _adapter_bundle_path(label, stage)
        persisted_backend = _try_persist_adapter(bundle_path, label, stage)
        status = f"persisted:{persisted_backend}" if persisted_backend else "queued"

        if not persisted_backend:
            entry = _queue_entry(label, stage, bundle_path, None, status)
            _append_to_queue(entry)

        self.updates.append(f"{label}:{stage}:{status}")
        _LOGGER.debug(
            "AdapterState.push_updates label=%s stage=%s status=%s path=%s",
            label,
            stage,
            status,
            bundle_path,
        )


adapter_state = AdapterState()


def is_peft_enabled() -> bool:
    """Return True when the Wave 7 PEFT toggle is set in the environment."""

    return os.getenv(_ENV_VAR, "0").lower() in {"1", "true", "yes"}


def prepare_peft_hook(tool_name: str, stage: str) -> None:
    """Hook point that loads LoRA weights before a tool call."""

    if not is_peft_enabled():
        return
    _LOGGER.info("Preparing PEFT hook tool=%s stage=%s", tool_name, stage)
    adapter_state.load_lora_weights(tool_name, stage)


def apply_lora_updates(tool_name: str, stage: str) -> None:
    """Hook point that pushes LoRA updates after a tool call."""

    if not is_peft_enabled():
        return
    _LOGGER.info("Applying LoRA updates tool=%s stage=%s", tool_name, stage)
    adapter_state.push_updates(tool_name, stage)


def with_peft(evaluate_fn: ProblemEvaluator, tool_name: str) -> ProblemEvaluator:
    """Return either the evaluator unchanged or a PEFT-wrapped callable."""

    if not is_peft_enabled():
        return evaluate_fn

    def _wrapped(
        boundary_params: Mapping[str, Any],
        *,
        stage: str,
        use_cache: bool = True,
    ) -> dict[str, Any]:
        prepare_peft_hook(tool_name, stage)
        result = evaluate_fn(boundary_params, stage=stage, use_cache=use_cache)
        apply_lora_updates(tool_name, stage)
        return result

    return _wrapped


register_adapter_loader("json_metadata", _json_metadata_loader)
register_adapter_persist_handler("staged_adapter", _staged_adapter_persist)


================================================================================
File: tools.py
================================================================================

"""Physics tool wrappers for the ConStellaration AI Scientist."""

from __future__ import annotations

import hashlib
import json
import math
from collections import defaultdict
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Callable, Dict, Mapping, Sequence, Tuple

import numpy as np
from pymoo.indicators import hv as pymoo_hv

from ai_scientist import rag
from constellaration import forward_model
from constellaration.geometry import surface_rz_fourier

_DEFAULT_RELATIVE_TOLERANCE = 1e-2
_CANONICAL_PRECISION = 1e-8
_EVALUATION_CACHE: Dict[Tuple[str, str], Dict[str, Any]] = {}
_CACHE_STATS: Dict[str, Dict[str, int]] = defaultdict(lambda: {"hits": 0, "misses": 0})
_P3_REFERENCE_POINT: Tuple[float, float] = (1.0, 20.0)


@dataclass(frozen=True)
class BoundaryParams:
    """Container for surface parameters that may evolve in future waves."""

    params: Mapping[str, Any]


@dataclass(frozen=True)
class P3Summary:
    """Compact summary of the per-cycle P3 pareto front and hypervolume."""

    hv_score: float
    reference_point: Tuple[float, float]
    feasible_count: int
    archive_size: int
    pareto_entries: Tuple["ParetoEntry", ...]


@dataclass(frozen=True)
class ParetoEntry:
    design_hash: str
    seed: int
    stage: str
    gradient: float
    aspect_ratio: float
    objective: float
    feasibility: float

    def as_mapping(self) -> Mapping[str, float]:
        return {
            "seed": float(self.seed),
            "gradient": self.gradient,
            "aspect_ratio": self.aspect_ratio,
            "objective": self.objective,
            "feasibility": self.feasibility,
        }


def _quantize_float(value: float) -> float:
    return float(round(value / _CANONICAL_PRECISION) * _CANONICAL_PRECISION)


def _canonicalize_value(value: Any) -> Any:
    if isinstance(value, Mapping):
        return {k: _canonicalize_value(v) for k, v in sorted(value.items())}
    if isinstance(value, np.ndarray):
        return _canonicalize_value(value.tolist())
    if isinstance(value, (list, tuple)):
        return [_canonicalize_value(v) for v in value]
    if isinstance(value, float):
        return _quantize_float(value)
    if isinstance(value, (int, str, bool)) or value is None:
        return value
    return str(value)


def _hash_params(params: Mapping[str, Any]) -> str:
    return design_hash(params)


def design_hash(params: Mapping[str, Any] | BoundaryParams) -> str:
    params_map = _ensure_mapping(params)
    normalized = _canonicalize_value(params_map)
    digest = json.dumps(normalized, sort_keys=True, separators=(",", ":"))
    return hashlib.sha256(digest.encode("utf-8")).hexdigest()


def _ensure_mapping(params: Mapping[str, Any] | BoundaryParams) -> Mapping[str, Any]:
    if isinstance(params, BoundaryParams):
        return params.params
    return params


def _evaluate_cached_stage(
    boundary_params: Mapping[str, Any] | BoundaryParams,
    *,
    stage: str,
    compute: Callable[[Mapping[str, Any]], Dict[str, Any]],
    use_cache: bool = True,
) -> Dict[str, Any]:
    params_map = _ensure_mapping(boundary_params)
    stage_lower = stage.lower()
    cache_key = (stage_lower, _hash_params(params_map))
    stats = _CACHE_STATS[stage_lower]
    if use_cache:
        cached = _EVALUATION_CACHE.get(cache_key)
        if cached is not None:
            stats["hits"] += 1
            return cached

    stats["misses"] += 1
    result = compute(params_map)
    if use_cache:
        _EVALUATION_CACHE[cache_key] = result
    return result


def _settings_for_stage(
    stage: str, *, skip_qi: bool = False
) -> forward_model.ConstellarationSettings:
    stage_lower = stage.lower()
    if stage_lower == "promote":
        settings = forward_model.ConstellarationSettings.default_high_fidelity_skip_qi()
    elif (
        stage_lower.startswith("p2")
        or stage_lower.startswith("p3")
        or stage_lower == "high_fidelity"
    ):
        settings = forward_model.ConstellarationSettings.default_high_fidelity()
    else:
        settings = forward_model.ConstellarationSettings()

    if skip_qi:
        return settings.model_copy(
            update={
                "boozer_preset_settings": None,
                "qi_settings": None,
            }
        )
    return settings


def _normalize_between_bounds(
    value: float, lower_bound: float, upper_bound: float
) -> float:
    assert lower_bound < upper_bound
    normalized = (value - lower_bound) / (upper_bound - lower_bound)
    return float(np.clip(normalized, 0.0, 1.0))


def _log10_or_large(value: float | None) -> float:
    if value is None or value <= 0.0:
        return 10.0
    return float(math.log10(value))


def _gradient_score(metrics: forward_model.ConstellarationMetrics) -> float:
    gradient = float(metrics.minimum_normalized_magnetic_gradient_scale_length)
    aspect = float(metrics.aspect_ratio)
    return float(gradient / max(1.0, aspect))


def _p2_feasibility(metrics: forward_model.ConstellarationMetrics) -> float:
    violations = [
        metrics.aspect_ratio - 10.0,
        max(0.0, 0.25 - metrics.edge_rotational_transform_over_n_field_periods),
        metrics.edge_magnetic_mirror_ratio - 0.2,
        max(0.0, metrics.max_elongation - 5.0),
        max(0.0, _log10_or_large(metrics.qi) + 4.0),
    ]
    return float(max(violations + [0.0]))


def _p3_feasibility(metrics: forward_model.ConstellarationMetrics) -> float:
    flux_violation = 0.0
    if metrics.flux_compression_in_regions_of_bad_curvature is not None:
        flux_violation = max(
            0.0, metrics.flux_compression_in_regions_of_bad_curvature - 0.9
        )

    constraints = [
        max(0.0, 0.25 - metrics.edge_rotational_transform_over_n_field_periods),
        max(0.0, metrics.edge_magnetic_mirror_ratio - 0.25),
        max(0.0, -metrics.vacuum_well),
        flux_violation,
        max(0.0, _log10_or_large(metrics.qi) + 3.5),
    ]
    return float(max(constraints + [0.0]))


def _objective_vector(metrics: Mapping[str, Any]) -> Tuple[float, float]:
    gradient = float(metrics["minimum_normalized_magnetic_gradient_scale_length"])
    aspect = float(metrics["aspect_ratio"])
    return -gradient, aspect


def _extract_p3_point(metrics: Mapping[str, Any]) -> Tuple[float, float]:
    vector = _objective_vector(metrics)
    return -vector[0], vector[1]


def _dominates(a: Tuple[float, float], b: Tuple[float, float]) -> bool:
    """Return True if objective a Pareto dominates b (higher gradient, lower aspect)."""

    higher_gradient = a[0] >= b[0]
    lower_aspect = a[1] <= b[1]
    strict = a[0] > b[0] or a[1] < b[1]
    return higher_gradient and lower_aspect and strict


def _hypervolume_minimization(
    vectors: Sequence[Tuple[float, float]],
    reference_point: Tuple[float, float],
) -> float:
    if not vectors:
        return 0.0
    indicator = pymoo_hv.Hypervolume(ref_point=np.asarray(reference_point, dtype=float))
    output = indicator(np.asarray(vectors, dtype=float))
    return float(output if output is not None else 0.0)


def summarize_p3_candidates(
    candidates: Sequence[Mapping[str, Any] | dict[str, Any]],
    *,
    reference_point: Tuple[float, float] = _P3_REFERENCE_POINT,
) -> P3Summary:
    """Produce the hypervolume score and all non-dominated seeds for a candidate batch."""

    @dataclass(frozen=True)
    class _P3Entry:
        gradient: float
        aspect: float
        seed: int
        evaluation: Mapping[str, Any]
        feasibility: float
        design_hash: str
        design_hash: str

    entries: list[_P3Entry] = []
    for candidate in candidates:
        design_id = candidate.get("design_hash")
        if design_id is None:
            design_id = design_hash(candidate.get("params", {}))
        design_id = str(design_id)
        eval_metrics = candidate["evaluation"]["metrics"]
        gradient, aspect = _extract_p3_point(eval_metrics)
        seed = int(candidate.get("seed", -1))
        feasibility = float(candidate["evaluation"]["feasibility"])
        entries.append(
            _P3Entry(
                design_hash=design_id,
                gradient=gradient,
                aspect=aspect,
                seed=seed,
                evaluation=candidate["evaluation"],
                feasibility=feasibility,
            )
        )

    hv_vectors: list[Tuple[float, float]] = []
    for entry in entries:
        if entry.feasibility > _DEFAULT_RELATIVE_TOLERANCE:
            continue
        hv_vectors.append((-entry.gradient, entry.aspect))

    pareto_entries: list[ParetoEntry] = []
    for current_index, entry in enumerate(entries):
        if entry.feasibility > _DEFAULT_RELATIVE_TOLERANCE:
            continue
        point = (entry.gradient, entry.aspect)
        dominated = False
        for other_index, other in enumerate(entries):
            if other_index == current_index:
                continue
            if other.feasibility > _DEFAULT_RELATIVE_TOLERANCE:
                continue
            if _dominates((other.gradient, other.aspect), point):
                dominated = True
                break
        if dominated:
            continue
        pareto_entries.append(
            ParetoEntry(
                design_hash=entry.design_hash,
                seed=entry.seed,
                stage=str(entry.evaluation.get("stage", "")),
                gradient=entry.gradient,
                aspect_ratio=entry.aspect,
                objective=float(entry.evaluation["objective"]),
                feasibility=entry.feasibility,
            )
        )

    pareto_entries.sort(key=lambda item: (-item.gradient, item.aspect_ratio))
    return P3Summary(
        hv_score=_hypervolume_minimization(hv_vectors, reference_point),
        reference_point=reference_point,
        feasible_count=sum(
            1 for entry in entries if entry.feasibility <= _DEFAULT_RELATIVE_TOLERANCE
        ),
        archive_size=len(pareto_entries),
        pareto_entries=tuple(pareto_entries),
    )


def retrieve_rag(
    query: str, *, k: int = 3, index_path: Path | str | None = None
) -> list[dict[str, str]]:
    """Expose RAG retrieval via the ai_scientist/rag_index.db index (Phase 3)."""

    index = Path(index_path) if index_path is not None else rag.DEFAULT_INDEX_PATH
    return rag.retrieve(query=query, k=k, index_path=index)


def make_boundary_from_params(
    params: Mapping[str, Any] | BoundaryParams,
) -> surface_rz_fourier.SurfaceRZFourier:
    """Construct a SurfaceRZFourier boundary from a simple parameter dictionary."""

    params_map = _ensure_mapping(params)
    payload: dict[str, Any] = {
        "r_cos": np.asarray(params_map["r_cos"], dtype=float),
        "z_sin": np.asarray(params_map["z_sin"], dtype=float),
        "is_stellarator_symmetric": bool(
            params_map.get("is_stellarator_symmetric", True)
        ),
        "n_field_periods": int(params_map.get("n_field_periods", 1)),
    }

    if "r_sin" in params_map:
        payload["r_sin"] = np.asarray(params_map["r_sin"], dtype=float)
    if "z_cos" in params_map:
        payload["z_cos"] = np.asarray(params_map["z_cos"], dtype=float)
    if "nfp" in params_map:
        payload.setdefault("n_field_periods", int(params_map["nfp"]))

    return surface_rz_fourier.SurfaceRZFourier(**payload)


def _constraint_metrics(
    metrics: forward_model.ConstellarationMetrics,
) -> Tuple[float, float]:
    targets = np.array([4.0, -0.5, 0.3], dtype=float)
    violations = np.array(
        [
            metrics.aspect_ratio - targets[0],
            metrics.average_triangularity - targets[1],
            targets[2] - metrics.edge_rotational_transform_over_n_field_periods,
        ],
        dtype=float,
    )
    normalized = violations / np.abs(targets)
    feasibility = float(np.max(np.maximum(normalized, 0.0)))
    return feasibility, float(np.max(normalized))


def evaluate_p1(
    boundary_params: Mapping[str, Any] | BoundaryParams,
    *,
    stage: str = "screen",
    use_cache: bool = True,
) -> Dict[str, Any]:
    """Run a P1-style evaluation and cache results by stage."""

    def compute(params_map: Mapping[str, Any]) -> Dict[str, Any]:
        boundary = make_boundary_from_params(params_map)
        settings = _settings_for_stage(stage, skip_qi=True)
        metrics, _ = forward_model.forward_model(boundary, settings=settings)
        feasibility, _ = _constraint_metrics(metrics)
        score = 0.0
        if feasibility <= _DEFAULT_RELATIVE_TOLERANCE:
            normalized = _normalize_between_bounds(
                value=metrics.max_elongation, lower_bound=1.0, upper_bound=10.0
            )
            score = 1.0 - normalized

        return {
            "stage": stage.lower(),
            "objective": float(metrics.max_elongation),
            "minimize_objective": True,
            "feasibility": feasibility,
            "score": score,
            "metrics": metrics.model_dump(),
            "settings": settings.model_dump(),
        }

    return _evaluate_cached_stage(
        boundary_params, stage=stage, compute=compute, use_cache=use_cache
    )


def evaluate_p2(
    boundary_params: Mapping[str, Any] | BoundaryParams,
    *,
    stage: str = "p2",
    use_cache: bool = True,
) -> Dict[str, Any]:
    """Run a high-fidelity evaluator for the P2 (QI) problem."""

    def compute(params_map: Mapping[str, Any]) -> Dict[str, Any]:
        boundary = make_boundary_from_params(params_map)
        settings = _settings_for_stage(stage)
        metrics, _ = forward_model.forward_model(boundary, settings=settings)
        feasibility = _p2_feasibility(metrics)
        gradient = float(metrics.minimum_normalized_magnetic_gradient_scale_length)
        score = _gradient_score(metrics)

        return {
            "stage": stage.lower(),
            "objective": gradient,
            "minimize_objective": False,
            "feasibility": feasibility,
            "score": score,
            "hv": float(max(0.0, gradient - 1.0)),
            "metrics": metrics.model_dump(),
            "settings": settings.model_dump(),
        }

    return _evaluate_cached_stage(
        boundary_params, stage=stage, compute=compute, use_cache=use_cache
    )


def evaluate_p3(
    boundary_params: Mapping[str, Any] | BoundaryParams,
    *,
    stage: str = "p3",
    use_cache: bool = True,
) -> Dict[str, Any]:
    """Run a high-fidelity evaluator for the P3 (multi-objective) problem."""

    def compute(params_map: Mapping[str, Any]) -> Dict[str, Any]:
        boundary = make_boundary_from_params(params_map)
        settings = _settings_for_stage(stage)
        metrics, _ = forward_model.forward_model(boundary, settings=settings)
        feasibility = _p3_feasibility(metrics)
        score = _gradient_score(metrics)

        return {
            "stage": stage.lower(),
            "objective": float(metrics.aspect_ratio),
            "minimize_objective": True,
            "feasibility": feasibility,
            "score": score,
            "hv": float(
                max(
                    0.0, metrics.minimum_normalized_magnetic_gradient_scale_length - 1.0
                )
            ),
            "metrics": metrics.model_dump(),
            "settings": settings.model_dump(),
        }

    return _evaluate_cached_stage(
        boundary_params, stage=stage, compute=compute, use_cache=use_cache
    )


def evaluate_p3_set(
    boundary_specs: Sequence[Mapping[str, Any] | BoundaryParams],
    *,
    stage: str = "p3",
    reference_point: Tuple[float, float] = _P3_REFERENCE_POINT,
) -> Dict[str, Any]:
    """Evaluate a batch of P3 boundaries and compute the set-level hypervolume."""

    stage_lower = stage.lower()
    if not boundary_specs:
        return {
            "stage": stage_lower,
            "objectives": [],
            "feasibilities": [],
            "hv_score": 0.0,
            "metrics_list": [],
        }

    evaluations: list[Dict[str, Any]] = []
    hv_vectors: list[Tuple[float, float]] = []

    for candidate in boundary_specs:
        evaluation = evaluate_p3(candidate, stage=stage)
        metrics = evaluation["metrics"]
        feasibility = float(evaluation["feasibility"])

        if feasibility <= _DEFAULT_RELATIVE_TOLERANCE:
            hv_vectors.append(_objective_vector(metrics))

        evaluations.append(evaluation)

    hv_score = _hypervolume_minimization(hv_vectors, reference_point)
    return {
        "stage": stage_lower,
        "objectives": [
            {
                "aspect_ratio": float(eval_["metrics"]["aspect_ratio"]),
                "gradient": float(
                    eval_["metrics"][
                        "minimum_normalized_magnetic_gradient_scale_length"
                    ]
                ),
                "objective": eval_["objective"],
            }
            for eval_ in evaluations
        ],
        "feasibilities": [float(eval_["feasibility"]) for eval_ in evaluations],
        "hv_score": hv_score,
        "metrics_list": [eval_["metrics"] for eval_ in evaluations],
    }


def normalized_constraint_distance_sampler(
    base_designs: Sequence[Mapping[str, Sequence[float] | float]],
    *,
    normalized_distances: Sequence[float],
    proposal_count: int,
    jitter_scale: float = 0.01,
    rng: np.random.Generator | None = None,
) -> list[Mapping[str, float | Sequence[float]]]:
    """Constraint-aware sampler for Task X.6 (docs/TASKS_CODEX_MINI.md:233).

    Designs with smaller normalized constraint distances are preferred so the curriculum
    nudges proposals toward near-feasible regions.
    """

    if proposal_count <= 0:
        return []

    if rng is None:
        rng = np.random.default_rng()

    total_candidates = len(base_designs)
    if total_candidates == 0:
        return []

    distances = np.asarray(normalized_distances, dtype=float)
    if distances.shape[0] != total_candidates:
        raise ValueError("normalized_distances must align with base_designs")

    clipped = np.clip(distances, 0.0, 1.0)
    weights = (1.0 - clipped) + 1e-3
    weights_sum = float(np.sum(weights))
    if weights_sum <= 0.0:
        weights = np.ones_like(weights)
        weights_sum = float(weights.size)

    probabilities = (weights / weights_sum).astype(float)
    chosen_indices = rng.choice(total_candidates, size=proposal_count, p=probabilities)
    proposals: list[Mapping[str, float | Sequence[float]]] = []

    for idx in chosen_indices:
        candidate = base_designs[idx]
        perturbed: dict[str, float | Sequence[float]] = {}
        for key, value in candidate.items():
            array = np.asarray(value, dtype=float)
            jitter = rng.normal(scale=jitter_scale, size=array.shape)
            proposal_array = array + jitter
            if proposal_array.shape == ():
                perturbed[key] = float(proposal_array)
            else:
                perturbed[key] = proposal_array.tolist()
        proposals.append(perturbed)

    return proposals


def get_cache_stats(stage: str) -> Mapping[str, int]:
    """Return hit/miss counts for a given stage."""

    return _CACHE_STATS[stage.lower()].copy()


def clear_evaluation_cache() -> None:
    """Reset the P1 evaluation cache and stats (useful for tests)."""

    _EVALUATION_CACHE.clear()
    _CACHE_STATS.clear()


================================================================================
File: __init__.py
================================================================================

"""AI Scientist orchestration package (skeleton).

This package hosts agent-facing wrappers and orchestration code to interact with
the ConStellaration physics stack without modifying code under
`constellaration/`.

Modules intentionally start minimal so Codex-mini sized tasks can extend them.
"""

from . import adapter
from . import agent
from . import config
from . import memory
from . import model_endpoint
from . import model_provider
from . import rag
from . import reporting
from . import planner
from . import runner
from . import tools
from . import tools_api
from . import prompts

__all__ = [
    "adapter",
    "agent",
    "config",
    "memory",
    "model_endpoint",
    "model_provider",
    "planner",
    "rag",
    "reporting",
    "runner",
    "tools",
    "tools_api",
    "prompts",
]


================================================================================
File: prompts.py
================================================================================

"""Planner prompts grounded in live problem specs + governance docs."""

from __future__ import annotations

from dataclasses import dataclass
from typing import Mapping

from constellaration import problems as problem_module

_SOURCE_PATH = "constellaration/src/constellaration/problems.py"


@dataclass(frozen=True)
class ConstraintSpec:
    name: str
    operator: str
    value: float
    description: str


@dataclass(frozen=True)
class ObjectiveSpec:
    name: str
    direction: str  # "min" | "max"
    description: str


@dataclass(frozen=True)
class ProblemSpec:
    key: str
    constraints: tuple[ConstraintSpec, ...]
    objectives: tuple[ObjectiveSpec, ...]
    score_description: str
    source: str = _SOURCE_PATH

    def prompt_block(self) -> str:
        constraint_lines = "\n".join(
            f"- {c.name} {c.operator} {c.value:g} ({c.description})"
            for c in self.constraints
        )
        objective_lines = "\n".join(
            f"- {obj.direction.upper()} {obj.name}: {obj.description}"
            for obj in self.objectives
        )
        return (
            f"Constraints ({self.source}):\n{constraint_lines}\n"
            f"Objectives:\n{objective_lines}\n"
            f"Scoring: {self.score_description}\n"
        )


def _geometrical_spec() -> ProblemSpec:
    instance = problem_module.GeometricalProblem()
    return ProblemSpec(
        key="p1",
        constraints=(
            ConstraintSpec(
                "aspect_ratio",
                "<=",
                instance._aspect_ratio_upper_bound,
                "controls width vs. height",
            ),
            ConstraintSpec(
                "average_triangularity",
                "<=",
                instance._average_triangularity_upper_bound,
                "keeps indentations bounded toward circular shapes",
            ),
            ConstraintSpec(
                "edge_rotational_transform_over_n_field_periods",
                ">=",
                instance._edge_rotational_transform_over_n_field_periods_lower_bound,
                "ensures sufficient edge winding per field period",
            ),
        ),
        objectives=(
            ObjectiveSpec(
                "max_elongation",
                "min",
                "lower elongation yields more circular, feasible geometries",
            ),
        ),
        score_description=(
            "Normalized max elongation between 1.0 (ideal) and 10.0 (poor)."
        ),
    )


def _simple_qi_spec() -> ProblemSpec:
    instance = problem_module.SimpleToBuildQIStellarator()
    return ProblemSpec(
        key="p2",
        constraints=(
            ConstraintSpec(
                "aspect_ratio",
                "<=",
                instance._aspect_ratio_upper_bound,
                "limits width/height for coilability",
            ),
            ConstraintSpec(
                "edge_rotational_transform_over_n_field_periods",
                ">=",
                instance._edge_rotational_transform_over_n_field_periods_lower_bound,
                "preserves transform per field period",
            ),
            ConstraintSpec(
                "log10(QI residual)",
                "<=",
                instance._log10_qi_upper_bound,
                "bounds quasi-isodynamic residuals for transport",
            ),
            ConstraintSpec(
                "edge_magnetic_mirror_ratio",
                "<=",
                instance._edge_magnetic_mirror_ratio_upper_bound,
                "keeps boundary field variation manageable",
            ),
            ConstraintSpec(
                "max_elongation",
                "<=",
                instance._max_elongation_upper_bound,
                "prevents extreme vertical stretching",
            ),
        ),
        objectives=(
            ObjectiveSpec(
                "minimum_normalized_magnetic_gradient_scale_length",
                "max",
                "higher gradients correlate with easier-to-build coils",
            ),
        ),
        score_description=(
            "Linear score over the minimum normalized magnetic gradient scale length "
            "(0.0 poor → 1.0 optimal)."
        ),
    )


def _mhd_qi_spec() -> ProblemSpec:
    instance = problem_module.MHDStableQIStellarator()
    return ProblemSpec(
        key="p3",
        constraints=(
            ConstraintSpec(
                "edge_rotational_transform_over_n_field_periods",
                ">=",
                instance._edge_rotational_transform_over_n_field_periods_lower_bound,
                "ensures winding per period",
            ),
            ConstraintSpec(
                "log10(QI residual)",
                "<=",
                instance._log10_qi_upper_bound,
                "keeps QI residual small for confinement",
            ),
            ConstraintSpec(
                "edge_magnetic_mirror_ratio",
                "<=",
                instance._edge_magnetic_mirror_ratio_upper_bound,
                "limits mirror ratio at the edge",
            ),
            ConstraintSpec(
                "flux_compression_in_regions_of_bad_curvature",
                "<=",
                instance._flux_compression_in_regions_of_bad_curvature_upper_bound,
                "controls turbulent transport proxy",
            ),
            ConstraintSpec(
                "vacuum_well",
                ">=",
                instance._vacuum_well_lower_bound,
                "maintains ideal-MHD stability margin",
            ),
        ),
        objectives=(
            ObjectiveSpec(
                "minimum_normalized_magnetic_gradient_scale_length",
                "max",
                "larger gradients improve QI performance",
            ),
            ObjectiveSpec(
                "aspect_ratio",
                "min",
                "smaller aspect ratios favor compact machines",
            ),
        ),
        score_description=(
            "Hypervolume of feasible (-gradient, aspect_ratio) points relative to [1.0, 20.0]."
        ),
    )


_PROBLEM_SPECS: Mapping[str, ProblemSpec] = {
    "p1": _geometrical_spec(),
    "p2": _simple_qi_spec(),
    "p3": _mhd_qi_spec(),
}

PHASE_GUIDANCE = (
    "Phase 6 requires computing/archiving the Pareto front + hypervolume each cycle; "
    "Phase 9 acceptance demands feasible P1–P3 designs, reproducible logs, and cited claims "
    "(docs/MASTER_PLAN_AI_SCIENTIST.md)."
)

BUDGET_REMINDER = "Respect Wave B budget guardrails (docs/TASKS_CODEX_MINI.md) when narrating promotions."

TOOL_REMINDER = "Use ai_scientist.tools_api schemas (Wave 8) and cite repositories instead of line numbers."

REPRO_PROMPT = (
    "Each report needs deterministic reproduction steps: git SHAs (repo + constellaration), "
    "seed, fidelity, settings dump, and a rerun command for an archived design."
)


def get_problem_spec(problem: str) -> ProblemSpec:
    key = problem.lower()
    if key not in _PROBLEM_SPECS:
        raise KeyError(f"unknown problem spec '{problem}'")
    return _PROBLEM_SPECS[key]


def build_problem_prompt(problem: str, stage: str) -> str:
    spec = get_problem_spec(problem)
    return (
        f"Planning for {problem.upper()} at stage '{stage}'.\n"
        f"{spec.prompt_block()}"
        f"{PHASE_GUIDANCE}\n"
        f"{BUDGET_REMINDER}\n"
        f"{TOOL_REMINDER}\n"
        f"{REPRO_PROMPT}\n"
    )


def annotate_solution_summary(summary: str) -> str:
    return (
        f"Summary (tie back to constraints/objectives + cite {_SOURCE_PATH}): {summary}"
    )


================================================================================
File: model_provider.py
================================================================================

"""Helpers for building OpenAI-style calls that match OpenRouter, Moonshot, and StreamLake APIs."""

from __future__ import annotations

import json
import logging
import os
from dataclasses import dataclass
from typing import Any, Mapping, Sequence
from urllib.error import HTTPError, URLError
from urllib.request import Request, urlopen

from ai_scientist.config import ProviderConfig

_LOGGER = logging.getLogger(__name__)
_DEFAULT_TIMEOUT_SECONDS = 60.0


@dataclass(frozen=True)
class ChatRequest:
    path: str
    headers: Mapping[str, str]
    body: Mapping[str, Any]


@dataclass(frozen=True)
class ChatResponse:
    status_code: int
    body: Mapping[str, Any]


def _resolve_auth_header(provider: ProviderConfig) -> str:
    token = os.getenv(provider.auth_env)
    if not token:
        placeholder = provider.auth_env or provider.name.upper()
        token = f"LOCAL-{placeholder}"
    return f"Bearer {token}"


def build_chat_request(
    provider: ProviderConfig,
    tool_call: Mapping[str, Any],
    *,
    messages: Sequence[Mapping[str, str]] | None = None,
    model: str | None = None,
) -> ChatRequest:
    """Return the per-provider path, headers, and body for a chat completion call."""

    headers: dict[str, str] = {
        "Authorization": _resolve_auth_header(provider),
        "Content-Type": "application/json",
    }
    for key, value in provider.extra_headers:
        headers[key] = value
    payload = {
        "model": model or provider.default_model,
        "messages": list(messages)
        if messages
        else [{"role": "user", "content": "tool request"}],
        "tool_call": tool_call,
    }
    _LOGGER.info(
        "Built chat request provider=%s path=%s model=%s tool=%s",
        provider.name,
        provider.chat_path,
        payload["model"],
        tool_call.get("name"),
    )
    return ChatRequest(path=provider.chat_path, headers=headers, body=payload)


def invoke_chat_completion(
    provider: ProviderConfig,
    tool_call: Mapping[str, Any],
    *,
    messages: Sequence[Mapping[str, str]] | None = None,
    model: str | None = None,
    base_url_override: str | None = None,
    timeout: float | None = None,
) -> ChatResponse:
    """Send a chat completion request to the configured provider and return the decoded response."""

    chat_request = build_chat_request(provider, tool_call, messages=messages, model=model)
    base_url = (base_url_override or provider.base_url or "").rstrip("/")
    if not base_url:
        raise ValueError(f"Provider '{provider.name}' is missing a base_url")
    url = f"{base_url}{chat_request.path}"
    data = json.dumps(chat_request.body, separators=(",", ":")).encode("utf-8")
    request = Request(url, data=data, headers=dict(chat_request.headers), method="POST")
    request_timeout = timeout or _DEFAULT_TIMEOUT_SECONDS
    try:
        with urlopen(request, timeout=request_timeout) as response:
            payload = response.read()
            status = getattr(response, "status", response.getcode())
    except HTTPError as exc:  # pragma: no cover - exercised with live providers
        error_body = exc.read().decode("utf-8", "replace") if exc.fp else ""
        message = (
            f"Provider '{provider.name}' returned HTTP {exc.code} for {url}: {error_body}"
        )
        raise RuntimeError(message) from exc
    except URLError as exc:  # pragma: no cover - exercised with live providers
        raise RuntimeError(f"Failed to reach provider '{provider.name}' at {url}: {exc}") from exc

    body_text = payload.decode("utf-8") if payload else "{}"
    try:
        parsed = json.loads(body_text) if body_text.strip() else {}
    except json.JSONDecodeError as exc:  # pragma: no cover - malformed upstream payload
        raise RuntimeError(f"Provider '{provider.name}' returned invalid JSON: {body_text}") from exc
    _LOGGER.info(
        "provider=%s status=%s finish_reason=%s",
        provider.name,
        status,
        parsed.get("choices", [{}])[0].get("finish_reason"),
    )
    return ChatResponse(status_code=status, body=parsed)


__all__ = ["ChatRequest", "ChatResponse", "build_chat_request", "invoke_chat_completion"]


================================================================================
File: rag.py
================================================================================

"""Local retrieval helpers for AI Scientist Wave 3 (RAG)."""

from __future__ import annotations

import os
import re
import sqlite3
from collections import defaultdict
from dataclasses import dataclass
from difflib import SequenceMatcher
from pathlib import Path
from typing import Iterable, List, Sequence

DEFAULT_INDEX_PATH = Path("ai_scientist/rag_index.db")
DEFAULT_INDEX_SOURCES = (
    "2511.02824v2.md",
    "ConStellaration Fusion Challenge_ Benchmarks and Solution Strategies.md",
)
INDEX_TABLE_NAME = "rag_references"
META_TABLE_NAME = "rag_index_meta"


@dataclass(frozen=True)
class DocumentChunk:
    source: str
    anchor: str
    chunk: str
    start_line: int
    end_line: int


@dataclass(frozen=True)
class IndexSummary:
    index_path: str
    chunks_indexed: int


@dataclass(frozen=True)
class SourceMeta:
    source: str
    mtime: float
    chunk_count: int


def _tokenize(text: str) -> List[str]:
    tokens = re.findall(r"\b\w+\b", text.lower())
    return [token for token in tokens if len(token) > 1]


def _file_mtime(path: Path) -> float | None:
    try:
        return path.stat().st_mtime
    except OSError:
        return None


def _write_metadata(
    conn: sqlite3.Connection,
    sources: Sequence[str],
    chunk_counts: dict[str, int],
) -> None:
    conn.execute(f"DELETE FROM {META_TABLE_NAME}")
    report_lines = []
    for source in sources:
        mtime = _file_mtime(Path(source)) or 0.0
        conn.execute(
            f"INSERT INTO {META_TABLE_NAME} (source, mtime, chunk_count) VALUES (?, ?, ?)",
            (source, mtime, chunk_counts.get(source, 0)),
        )
        report_lines.append(
            f"{source}: {chunk_counts.get(source, 0)} chunks (mtime={mtime:.0f})"
        )
    print(
        "[rag] indexed sources:\n" + "\n".join(f"  - {line}" for line in report_lines)
    )


def _ensure_index(conn: sqlite3.Connection) -> None:
    conn.execute(
        f"""
        CREATE TABLE IF NOT EXISTS {INDEX_TABLE_NAME} (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            source TEXT NOT NULL,
            anchor TEXT,
            chunk TEXT NOT NULL,
            start_line INTEGER NOT NULL,
            end_line INTEGER NOT NULL,
            tokens TEXT NOT NULL
        )
        """
    )
    conn.execute(
        f"""
        CREATE TABLE IF NOT EXISTS {META_TABLE_NAME} (
            source TEXT PRIMARY KEY,
            mtime REAL NOT NULL,
            chunk_count INTEGER NOT NULL
        )
        """
    )


def _iter_document_chunks(source: str, lines: Sequence[str]) -> Iterable[DocumentChunk]:
    anchor = ""
    chunk: List[str] = []
    chunk_start = 1

    def flush(end_line: int) -> DocumentChunk | None:
        if not chunk:
            return None
        text = " ".join(line.strip() for line in chunk if line.strip())
        if not text:
            return None
        return DocumentChunk(
            source=source,
            anchor=anchor,
            chunk=text,
            start_line=chunk_start,
            end_line=end_line,
        )

    for line_number, raw in enumerate(lines, start=1):
        stripped = raw.strip()
        if stripped.startswith("#"):
            anchor = stripped.lstrip("#").strip()
            if chunk:
                flushed = flush(line_number - 1)
                if flushed:
                    chunk.clear()
                    chunk_start = line_number
                    yield flushed
        if not chunk:
            chunk_start = line_number
        chunk.append(raw)
        if stripped == "":
            flushed = flush(line_number)
            if flushed:
                chunk.clear()
                chunk_start = line_number + 1
                yield flushed
    flushed = flush(len(lines))
    if flushed:
        yield flushed


def ensure_index(
    sources: Sequence[str] | None = None,
    index_path: Path | str | None = None,
    *,
    force_rebuild: bool = False,
) -> IndexSummary:
    """Ensure an index exists and reuse it unless a rebuild is requested."""

    index_path = Path(index_path) if index_path is not None else DEFAULT_INDEX_PATH
    sources = tuple(sources or DEFAULT_INDEX_SOURCES)
    env_force = os.environ.get("AI_SCIENTIST_RAG_FORCE_REBUILD", "").lower() in {
        "1",
        "true",
        "yes",
    }
    should_rebuild = force_rebuild or env_force or not index_path.exists()

    if not should_rebuild and index_path.exists():
        conn = sqlite3.connect(index_path)
        conn.row_factory = sqlite3.Row
        try:
            _ensure_index(conn)
            existing_meta = {
                row["source"]: SourceMeta(
                    source=row["source"],
                    mtime=row["mtime"],
                    chunk_count=int(row["chunk_count"]),
                )
                for row in conn.execute(
                    f"SELECT source, mtime, chunk_count FROM {META_TABLE_NAME}"
                )
            }
            needs_rebuild = False
            current_total = 0
            for source in sources:
                path = Path(source)
                meta = existing_meta.get(source)
                if path.exists():
                    current_mtime = _file_mtime(path)
                    if (
                        meta is None
                        or current_mtime is None
                        or meta.mtime != current_mtime
                    ):
                        needs_rebuild = True
                        break
                    current_total += meta.chunk_count
                else:
                    if meta is not None and meta.chunk_count > 0:
                        needs_rebuild = True
                        break
            if not needs_rebuild and current_total > 0:
                print(f"[rag] reusing index {index_path} ({current_total} chunks)")
                return IndexSummary(str(index_path), current_total)
            print("[rag] source changes detected; rebuilding index")
            should_rebuild = True
        finally:
            conn.close()
    if should_rebuild:
        return build_index(sources, index_path)
    return IndexSummary(str(index_path), 0)


def build_index(
    sources: Sequence[str],
    index_path: Path | str | None = None,
) -> IndexSummary:
    """Persist a simple sqlite-backed index derived from Markdown sources."""

    index_path = Path(index_path) if index_path is not None else DEFAULT_INDEX_PATH
    index_path.parent.mkdir(parents=True, exist_ok=True)
    conn = sqlite3.connect(index_path)
    conn.row_factory = sqlite3.Row
    try:
        _ensure_index(conn)
        conn.execute(f"DELETE FROM {INDEX_TABLE_NAME}")
        total_chunks = 0
        chunk_counts: dict[str, int] = defaultdict(int)
        for source in sources:
            path = Path(source)
            if not path.exists():
                continue
            lines = path.read_text(encoding="utf-8").splitlines()
            for chunk in _iter_document_chunks(source, lines):
                tokens = " ".join(_tokenize(chunk.chunk))
                conn.execute(
                    f"INSERT INTO {INDEX_TABLE_NAME} (source, anchor, chunk, start_line, end_line, tokens) VALUES (?, ?, ?, ?, ?, ?)",
                    (
                        chunk.source,
                        chunk.anchor,
                        chunk.chunk,
                        chunk.start_line,
                        chunk.end_line,
                        tokens,
                    ),
                )
                chunk_counts[source] += 1
                total_chunks += 1
        _write_metadata(conn, sources, chunk_counts)
        conn.commit()
    finally:
        conn.close()
    return IndexSummary(str(index_path), total_chunks)


def _fuzzy_similarity(a: str, b: str) -> float:
    if not a or not b:
        return 0.0
    return SequenceMatcher(None, a.lower(), b.lower()).ratio()


def retrieve(
    query: str,
    k: int = 3,
    index_path: Path | str | None = None,
    similarity_weight: float = 0.5,
) -> List[dict[str, str]]:
    """Retrieve the top-k chunks using token overlap + fuzzy matching."""

    index_path = Path(index_path) if index_path is not None else DEFAULT_INDEX_PATH
    if not index_path.exists():
        return []
    tokens = set(_tokenize(query))
    if not tokens:
        return []
    conn = sqlite3.connect(index_path)
    conn.row_factory = sqlite3.Row
    try:
        rows = conn.execute(f"SELECT * FROM {INDEX_TABLE_NAME}").fetchall()
    finally:
        conn.close()
    scored: List[tuple[float, sqlite3.Row]] = []
    for row in rows:
        row_tokens = set(str(row["tokens"]).split())
        overlap_score = len(tokens & row_tokens)
        if overlap_score == 0 and similarity_weight <= 0:
            continue
        similarity_score = _fuzzy_similarity(query, row["chunk"])
        score = overlap_score + similarity_weight * similarity_score
        if score == 0:
            continue
        scored.append((score, row))
    scored.sort(key=lambda item: (-item[0], item[1]["source"], item[1]["start_line"]))
    results: List[dict[str, str]] = []
    for _, row in scored[:k]:
        results.append(
            {
                "source": row["source"],
                "anchor": row["anchor"],
                "chunk": row["chunk"],
                "start_line": str(row["start_line"]),
                "end_line": str(row["end_line"]),
            }
        )
    return results


================================================================================
File: agent.py
================================================================================

"""Client gating helpers so K2-Instruct/K2-Thinking emit valid tool calls (docs/TASKS_CODEX_MINI.md:157-190)."""

from __future__ import annotations

import logging
from dataclasses import dataclass
from typing import Tuple

from ai_scientist.config import ModelConfig, load_model_config
from ai_scientist.tools_api import TOOL_SCHEMA_BY_NAME

_LOGGER = logging.getLogger(__name__)

_ROLE_ALIAS_MAP = {
    "screen": lambda cfg: cfg.instruct_model,
    "short_loop": lambda cfg: cfg.instruct_model,
    "prompt": lambda cfg: cfg.instruct_model,
    "planning": lambda cfg: cfg.thinking_model,
    "report": lambda cfg: cfg.thinking_model,
    "verification": lambda cfg: cfg.thinking_model,
}


@dataclass(frozen=True)
class AgentGate:
    model_alias: str
    allowed_tools: Tuple[str, ...]
    system_prompt: str
    provider_model: str

    def allows(self, tool_name: str) -> bool:
        return tool_name in self.allowed_tools


def gates_from_config(config: ModelConfig) -> tuple[AgentGate, ...]:
    return tuple(
        AgentGate(
            model_alias=gate.model_alias,
            allowed_tools=tuple(gate.allowed_tools),
            system_prompt=gate.system_prompt or "",
            provider_model=gate.provider_model or gate.model_alias,
        )
        for gate in config.agent_gates
    )


def gate_for_model(config: ModelConfig, model_alias: str) -> AgentGate | None:
    for gate in gates_from_config(config):
        if gate.model_alias == model_alias:
            return gate
    return None


def _resolve_alias_for_role(role: str | None, config: ModelConfig) -> str:
    normalized = (role or "short_loop").lower()
    resolver = _ROLE_ALIAS_MAP.get(normalized)
    if resolver:
        return resolver(config)
    return config.instruct_model


def provision_model_tier(
    role: str | None = None, *, config: ModelConfig | None = None
) -> AgentGate:
    """Return the AgentGate backing the K2 tier that best fits the requested role."""

    resolved = config or load_model_config()
    alias = _resolve_alias_for_role(role, resolved)
    gate = gate_for_model(resolved, alias)
    if gate is None:
        raise ValueError(
            f"Configured model '{alias}' is not declared in configs/model.yaml"
        )
    _LOGGER.info(
        "Provisioned %s tier for role=%s (base_url=%s, tools=%s)",
        alias,
        (role or "short_loop").lower(),
        resolved.base_url,
        gate.allowed_tools,
    )
    return gate


def validate_tool_call(config: ModelConfig, model_alias: str, tool_name: str) -> None:
    gate = gate_for_model(config, model_alias)
    if gate is None:
        raise ValueError(f"Unknown agent model '{model_alias}'")
    if tool_name not in gate.allowed_tools:
        raise ValueError(
            f"Tool '{tool_name}' is not permitted for {model_alias}; allowed {gate.allowed_tools}"
        )
    if tool_name not in TOOL_SCHEMA_BY_NAME:
        raise ValueError(f"No schema registered for tool '{tool_name}'")


================================================================================
File: tools_api_smoke.py
================================================================================

"""Exercise the OpenAI tool schemas listed in ai_scientist/tools_api.py (Wave 8 checklist in docs/TASKS_CODEX_MINI.md:157-190)."""

from __future__ import annotations

import logging
from typing import Any, Mapping

from ai_scientist.tools_api import list_tool_schemas

_LOGGER = logging.getLogger(__name__)
SAMPLE_PARAMS: Mapping[str, Any] = {
    "r_cos": [[1.5, 0.0], [0.0, 0.05]],
    "z_sin": [[0.0, 0.0], [0.0, 0.05]],
    "n_field_periods": 1,
}


def _build_sample_payload(tool_name: str) -> Mapping[str, Any]:
    if tool_name == "make_boundary":
        return {"params": SAMPLE_PARAMS}
    if tool_name in {"evaluate_p1", "evaluate_p2", "evaluate_p3"}:
        return {
            "params": SAMPLE_PARAMS,
            "problem": tool_name.replace("evaluate_", ""),
            "stage": "screen",
        }
    if tool_name == "log_citation":
        return {
            "source_path": "docs/MASTER_PLAN_AI_SCIENTIST.md",
            "anchor": "Phase 1",
            "quote": "Tiered K2 gate ensures deterministic tooling.",
        }
    if tool_name == "write_report":
        return {
            "title": "Smoke Report",
            "sections": [
                {"heading": "Summary", "body": "This report confirms tool schemas."},
                {
                    "heading": "Next Steps",
                    "body": "Log this output and proceed with Phase 9.",
                },
            ],
            "references": [
                "docs/TASKS_CODEX_MINI.md:157-190",
                "docs/MASTER_PLAN_AI_SCIENTIST.md:247-368",
            ],
        }
    if tool_name == "retrieve_rag":
        return {"query": "Phase 3 planning guidance", "k": 2}
    raise ValueError(f"No smoke sample defined for tool '{tool_name}'")


def _validate_schema(schema: Mapping[str, Any], payload: Mapping[str, Any]) -> None:
    parameters = schema.get("parameters", {})
    required = parameters.get("required", [])
    for field in required:
        if field not in payload:
            raise AssertionError(
                f"Payload for {schema['name']} is missing required field '{field}'"
            )


def run_smoke() -> None:
    """Ensure every tool schema exposes a minimal payload (Phase 1 smoke test)."""

    for schema in list_tool_schemas():
        name = schema["name"]
        sample = _build_sample_payload(name)
        _validate_schema(schema, sample)
        _LOGGER.info("Tool schema '%s' accepts sample payload %s", name, sample)


def smoke_entrypoint() -> None:
    logging.basicConfig(level=logging.INFO)
    run_smoke()


if __name__ == "__main__":
    smoke_entrypoint()


================================================================================
File: test_helpers.py
================================================================================

"""Shared fixtures and helpers reused between ai_scientist tests."""

from constellaration.forward_model import ConstellarationMetrics


def base_params() -> dict[str, list[list[float]] | int | bool]:
    return {
        "r_cos": [[0.0, 0.0, 1.5, 0.0, 0.2], [0.0, 0.0, 0.05, 0.0, 0.1]],
        "z_sin": [[0.0, 0.0, 0.0, 0.05, 0.0], [0.0, 0.0, 0.02, 0.0, 0.0]],
        "n_field_periods": 1,
        "is_stellarator_symmetric": True,
    }


def dummy_metrics() -> ConstellarationMetrics:
    return ConstellarationMetrics(
        aspect_ratio=3.0,
        aspect_ratio_over_edge_rotational_transform=2.0,
        max_elongation=1.2,
        axis_rotational_transform_over_n_field_periods=0.35,
        edge_rotational_transform_over_n_field_periods=0.4,
        axis_magnetic_mirror_ratio=0.7,
        edge_magnetic_mirror_ratio=0.6,
        average_triangularity=-0.3,
        vacuum_well=0.1,
        minimum_normalized_magnetic_gradient_scale_length=0.2,
        qi=1e-5,
        flux_compression_in_regions_of_bad_curvature=0.1,
    )


def dummy_metrics_with(**overrides: float) -> ConstellarationMetrics:
    data = dummy_metrics().model_dump()
    data.update(overrides)
    return ConstellarationMetrics(**data)


================================================================================
File: reporting.py
================================================================================

"""Deterministic Markdown reporting with citations, Pareto figures, and statements (Phase 8/X guidance)."""

from __future__ import annotations

import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Any, Mapping, Sequence

from ai_scientist import rag
from ai_scientist.rag import DEFAULT_INDEX_SOURCES
from ai_scientist.memory import StageHistoryEntry

_LOGGER = logging.getLogger(__name__)
_ALLOWED_REFERENCE_PREFIXES = (
    "docs/",
    "constellaration/",
    "Jr.AI-Scientist/",
    "reports/",
    "tests/",
)
_POSITIONING_SOURCES = DEFAULT_INDEX_SOURCES + (
    "docs/MASTER_PLAN_AI_SCIENTIST.md",
    "docs/TASKS_CODEX_MINI.md",
)


def write_report(title: str, content: str, out_dir: str | Path = "reports") -> Path:
    ts = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    safe_name = title.replace(" ", "_")
    out_path = Path(out_dir) / f"{ts}_{safe_name}.md"
    out_path.parent.mkdir(parents=True, exist_ok=True)
    header = f"# {title}\n\nGenerated: {ts} UTC\n\n"
    out_path.write_text(header + content)
    return out_path


def validate_references(references: Sequence[str]) -> None:
    if not references:
        raise ValueError(
            "Deterministic reports must cite at least one anchor (docs/ or constellaration/)."
        )
    invalid = [
        ref for ref in references if not ref.startswith(_ALLOWED_REFERENCE_PREFIXES)
    ]
    if invalid:
        raise ValueError(
            "Reference anchors must point to repo docs or known guides, got: %s"
            % invalid
        )


def _statement_value(statement: Any, key: str) -> Any:
    if isinstance(statement, Mapping):
        return statement.get(key)
    return getattr(statement, key)


def _format_statements_table(statements: Sequence[Mapping[str, Any] | Any]) -> str:
    if not statements:
        return "No statements tracked for this cycle."
    lines = [
        "| Stage | Statement | Status | Tool | Seed | Created |",
        "| --- | --- | --- | --- | --- | --- |",
    ]
    for statement in statements:
        stage = _statement_value(statement, "stage") or "unknown"
        text = _statement_value(statement, "text") or ""
        status = _statement_value(statement, "status") or "pending"
        tool_name = _statement_value(statement, "tool_name") or "n/a"
        seed = _statement_value(statement, "seed")
        created_at = _statement_value(statement, "created_at") or "unknown"
        safe_text = str(text).replace("|", "\u007c").replace("\n", " ")
        lines.append(
            f"| {stage.upper()} | {safe_text} | {status} | {tool_name} | {seed or '-'} | {created_at} |"
        )
    return "\n".join(lines)


def _relative_path(path: Path, base_dir: Path) -> str:
    try:
        rel = path.relative_to(base_dir)
    except ValueError:
        rel = path
    return rel.as_posix()


def _format_stage_history_table(stage_history: Sequence[StageHistoryEntry]) -> str:
    if not stage_history:
        return "No governance stage history recorded yet."
    lines = [
        "| Cycle | Stage | Selected At |",
        "| --- | --- | --- |",
    ]
    for entry in stage_history:
        lines.append(f"| {entry.cycle} | {entry.stage.upper()} | {entry.selected_at} |")
    return "\n".join(lines)


def _format_reference_table(references: Sequence[str]) -> str:
    lines = [
        "| Reference |",
        "| --- |",
        *[f"| {reference} |" for reference in references],
    ]
    return "\n".join(lines)


def _format_artifact_table(
    artifact_entries: Sequence[tuple[str, Path]],
    out_dir: Path,
) -> str:
    if not artifact_entries:
        return "No artifacts logged this cycle."
    lines = [
        "| Kind | Path |",
        "| --- | --- |",
    ]
    for kind, path in artifact_entries:
        lines.append(f"| {kind} | {_relative_path(path, out_dir)} |")
    return "\n".join(lines)


def _format_adaptation_figures(
    figures: Sequence[Path],
    out_dir: Path,
) -> list[str]:
    if not figures:
        return ["- No adaptation figures captured for this cycle."]
    lines: list[str] = []
    for figure in figures:
        lines.append(f"- ![{figure.name}]({_relative_path(figure, out_dir)})")
    return lines


def _normalize_quote_text(text: str) -> str:
    return " ".join(text.strip().split())


def _truncate_quote_text(text: str, max_words: int = 25) -> str:
    words = text.split()
    if len(words) <= max_words:
        return text
    return " ".join(words[:max_words]) + " ..."


def _collect_positioning_quotes(
    *,
    min_quotes: int = 3,
    queries: Sequence[str] | None = None,
) -> list[dict[str, str]]:
    queries = queries or (
        "hypervolume baseline acceptance",
        "pareto archives vs baseline story",
        "Task X.4 related work rewrite positioning",
    )
    rag.ensure_index(sources=_POSITIONING_SOURCES)
    seen: set[tuple[str, str, str]] = set()
    quotes: list[dict[str, str]] = []
    for query in queries:
        for chunk in rag.retrieve(query, k=3):
            key = (chunk["source"], chunk["start_line"], chunk["end_line"])
            if key in seen:
                continue
            seen.add(key)
            text = _normalize_quote_text(chunk["chunk"])
            if not text:
                continue
            final_text = _truncate_quote_text(text)
            quotes.append(
                {
                    "text": final_text,
                    "source": chunk["source"],
                    "start_line": chunk["start_line"],
                    "end_line": chunk["end_line"],
                }
            )
            if len(quotes) >= min_quotes:
                return quotes[:min_quotes]
    return quotes


def _build_positioning_section(
    p3_summary: Mapping[str, Any],
    *,
    positioning_artifacts: Mapping[str, str] | None = None,
) -> list[str]:
    hv_score = p3_summary.get("hv_score")
    archive_size = p3_summary.get("archive_size")
    positioning_lines: list[str] = ["### Positioning vs baselines"]
    hv_line = (
        f"- Current HV: {hv_score:.6f}, tracking the positive-delta expectation from the master plan."
        if hv_score is not None
        else "- Current HV: n/a"
    )
    archive_line = (
        f"- Pareto archive size: {archive_size} entries, so we can document the front the master plan wants us to guard."
        if archive_size is not None
        else "- Pareto archive: unknown count"
    )
    positioning_lines.extend([hv_line, archive_line])
    artifact_lines: list[str] = []
    if positioning_artifacts:
        anchors = ", ".join(
            anchor
            for anchor in (
                positioning_artifacts.get("preference_pairs"),
                positioning_artifacts.get("p3_summary"),
                positioning_artifacts.get("trajectory"),
            )
            if anchor
        )
        if anchors:
            artifact_lines.append(
                f"- RLAIF evidence anchored at {anchors}; the master plan calls this linkage out in docs/MASTER_PLAN_AI_SCIENTIST.md:226-247 "
                "and docs/TASKS_CODEX_MINI.md:238 so reviewers can trace the HV claim."
            )
    if artifact_lines:
        positioning_lines.extend(artifact_lines)
    quotes = _collect_positioning_quotes()
    if not quotes:
        positioning_lines.append(
            "- Baseline quotes could not be retrieved; ensure the RAG index is built and rerun the report."
        )
        return positioning_lines
    for quote in quotes:
        citation = f"{quote['source']}:{quote['start_line']}-{quote['end_line']}"
        positioning_lines.append(f"> {quote['text']} [{citation}]")
    return positioning_lines


def save_pareto_figure(
    pareto_entries: Sequence[Any],
    out_dir: str | Path,
    *,
    title: str,
    cycle_index: int,
) -> Path | None:
    if not pareto_entries:
        return None
    try:
        import matplotlib.pyplot as plt
    except ImportError:  # pragma: no cover - optional dependency
        _LOGGER.warning(
            "matplotlib not available, skipping Pareto figure for cycle %s",
            cycle_index + 1,
        )
        return None
    gradients: list[float] = [entry.gradient for entry in pareto_entries]
    aspects: list[float] = [entry.aspect_ratio for entry in pareto_entries]
    fig, ax = plt.subplots(figsize=(5, 4))
    ax.scatter(gradients, aspects, c="tab:purple", edgecolor="black")
    ax.set_xlabel("Minimum normalized gradient")
    ax.set_ylabel("Aspect ratio")
    ax.set_title(f"Pareto cycle {cycle_index + 1}: {title}")
    ax.grid(True, linestyle="--", linewidth=0.5)
    figure_dir = Path(out_dir) / "figures"
    figure_dir.mkdir(parents=True, exist_ok=True)
    safe_title = "".join(c if c.isalnum() else "_" for c in title)[:32]
    file_name = f"pareto_cycle_{cycle_index + 1}_{safe_title}.png"
    out_path = figure_dir / file_name
    fig.tight_layout()
    fig.savefig(out_path, dpi=120)
    plt.close(fig)
    return out_path


def build_cycle_report(
    *,
    cycle_index: int,
    problem: str,
    screened: int,
    promoted: int,
    governance_stage: str,
    best_metrics: Mapping[str, Any],
    config_snapshot: Mapping[str, Any],
    reproduction_steps: Sequence[str],
    reproduction_snippet: str,
    environment_block: str,
    pareto_lines: str,
    p3_summary: Mapping[str, Any],
    positioning_artifacts: Mapping[str, str] | None = None,
    statements: Sequence[Mapping[str, Any] | Any],
    references: Sequence[str],
    figure_paths: Sequence[Path],
    stage_history: Sequence[StageHistoryEntry],
    artifact_entries: Sequence[tuple[str, Path]],
    adaptation_figures: Sequence[Path],
    out_dir: str | Path = "reports",
) -> str:
    validate_references(references)
    base_dir = Path(out_dir)
    summary_lines = [
        f"- Screened: {screened}",
        f"- Promoted: {promoted}",
        f"- Governance stage: {governance_stage.upper()}",
    ]
    reproduction_lines = "\n".join(
        f"{idx + 1}. {step}" for idx, step in enumerate(reproduction_steps)
    )
    config_block = json.dumps(config_snapshot, indent=2)
    best_metrics_block = json.dumps(best_metrics, indent=2)
    pareto_block = f"#### Non-dominated front\n{pareto_lines}"
    figure_section = []
    for figure in figure_paths:
        try:
            rel = figure.relative_to(Path(out_dir))
        except ValueError:
            rel = figure
        figure_section.append(f"- ![{figure.name}]({rel.as_posix()})")
    if not figure_section:
        figure_section.append("- No Pareto figures generated for this cycle.")
    statement_table = _format_statements_table(statements)
    stage_table = _format_stage_history_table(stage_history)
    artifact_table = _format_artifact_table(artifact_entries, base_dir)
    adaptation_lines = _format_adaptation_figures(adaptation_figures, base_dir)
    citation_table = _format_reference_table(references)
    citation_status = "Citation validation: PASS (all anchors resolve to repo docs)."
    hv_score = p3_summary.get("hv_score")
    reference_point = p3_summary.get("reference_point")
    hv_lines = [
        f"- Hypervolume: {hv_score:.6f}"
        if hv_score is not None
        else "- Hypervolume: n/a",
        f"- Reference point: {reference_point}"
        if reference_point
        else "- Reference point: unknown",
        f"- Feasible evaluations: {p3_summary.get('feasible_count', 0)}",
        f"- Pareto archive size: {p3_summary.get('archive_size', 0)}",
    ]
    positioning_section = _build_positioning_section(
        {
            "hv_score": hv_score,
            "archive_size": p3_summary.get("archive_size"),
        },
        positioning_artifacts=positioning_artifacts,
    )
    document = [
        f"## Cycle {cycle_index + 1}",
        f"- Problem: {problem}",
        *summary_lines,
        "",  # blank line
        "### Best candidate metrics",
        f"```json\n{best_metrics_block}\n```",
        "### Config snapshot",
        f"```json\n{config_block}\n```",
        "### Reproduction",
        reproduction_lines,
        reproduction_snippet,
        "### Environment",
        environment_block,
        "### Stage history",
        stage_table,
        "### Phase 6 / P3 summary",
        *hv_lines,
        *positioning_section,
        pareto_block,
        "### Pareto figures",
        *figure_section,
        "### Adaptation figures",
        *adaptation_lines,
        "### Artifacts",
        artifact_table,
        "### Statements",
        statement_table,
        "### Citations",
        citation_status,
        citation_table,
        "### References for governance",
        "- docs/TASKS_CODEX_MINI.md:200-248",
        "- docs/MASTER_PLAN_AI_SCIENTIST.md:247-368",
    ]
    return "\n".join(str(line) for line in document)


def collect_adaptation_figures(out_dir: str | Path = "reports") -> list[Path]:
    base_dir = Path(out_dir)
    figure_dir = base_dir / "adaptation" / "figures"
    if not figure_dir.exists():
        return []
    collected: list[Path] = []
    for pattern in ("*.png", "*.svg", "*.jpg", "*.jpeg"):
        collected.extend(sorted(figure_dir.glob(pattern)))
    return sorted(collected)


================================================================================
File: optim/__init__.py
================================================================================

"""Optimization helpers (skeleton)."""

from . import search as _search
from . import surrogate as _surrogate

search = _search
surrogate = _surrogate

__all__ = ["search", "surrogate"]


================================================================================
File: optim/surrogate.py
================================================================================

"""Lightweight surrogate ranking helpers for candidate screening.

Per docs/MASTER_PLAN_AI_SCIENTIST.md:332 and docs/TASKS_CODEX_MINI.md:151 the
ranker now trains on cached metrics (KRR/MLP-style features) so Phase 4/5
memory data can beat random ordering before promotion.
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Mapping, Sequence

import numpy as np

from ai_scientist import memory


def _flatten_boundary_params(params: Mapping[str, Any]) -> np.ndarray:
    values: list[np.ndarray] = []
    for key in ("r_cos", "z_sin"):
        payload = params.get(key)
        if payload is None:
            continue
        arr = np.asarray(payload, dtype=float).ravel()
        if arr.size:
            values.append(arr)
    if not values:
        return np.zeros((0,), dtype=float)
    return np.concatenate(values)


def _params_feature_vector(params: Mapping[str, Any]) -> np.ndarray:
    flattened = _flatten_boundary_params(params)
    if flattened.size == 0:
        return np.zeros((2,), dtype=float)
    return np.array([float(np.sum(flattened)), float(flattened.size)], dtype=float)


@dataclass(frozen=True)
class SurrogateRank:
    """Surrogate score + reference for a candidate."""

    score: float
    metrics: Mapping[str, Any]


class SimpleSurrogateRanker:
    """Surrogate ranker that uses a ridge-learned model instead of heuristics."""

    def __init__(self, *, alpha: float = 1e-2) -> None:
        self._alpha = float(alpha)
        self._feature_weights: np.ndarray | None = None
        self._bias: float = 0.0

    def _feature_vector(self, metrics: Mapping[str, Any]) -> np.ndarray:
        params = metrics.get("candidate_params")
        if isinstance(params, Mapping):
            return _params_feature_vector(params)
        gradient = float(
            metrics.get("minimum_normalized_magnetic_gradient_scale_length", 0.0)
        )
        aspect = float(metrics.get("aspect_ratio", 0.0))
        hv = float(metrics.get("hv", gradient - aspect))
        return np.array([gradient, aspect, hv, gradient - aspect], dtype=float)

    def fit(
        self,
        metrics_list: Sequence[Mapping[str, Any]],
        target_values: Sequence[float],
    ) -> None:
        if not metrics_list:
            raise ValueError("training data cannot be empty")
        if len(metrics_list) != len(target_values):
            raise ValueError("metrics and targets must be the same length")
        features = np.vstack([self._feature_vector(m) for m in metrics_list])
        targets = np.asarray(target_values, dtype=float)
        intercept = np.ones((features.shape[0], 1), dtype=float)
        design = np.hstack([features, intercept])
        reg = np.eye(design.shape[1], dtype=float)
        reg[-1, -1] = 0.0
        xtx = design.T @ design + self._alpha * reg
        xty = design.T @ targets
        weights = np.linalg.solve(xtx, xty)
        self._feature_weights = weights[:-1]
        self._bias = float(weights[-1])

    def fit_from_world_model(
        self,
        world_model: memory.WorldModel,
        *,
        target_column: str = "hv",
        problem: str | None = None,
    ) -> None:
        history = world_model.surrogate_training_data(
            target=target_column, problem=problem
        )
        if not history:
            raise ValueError("insufficient history for surrogate training")
        metrics_list, targets = zip(*history)
        self.fit(metrics_list, targets)

    def _ensure_trained(self) -> None:
        if self._feature_weights is None:
            raise RuntimeError("surrogate ranker has not been trained")

    def rank(self, metrics_list: Sequence[Mapping[str, Any]]) -> list[SurrogateRank]:
        self._ensure_trained()
        assert self._feature_weights is not None
        ranked: list[SurrogateRank] = []
        weights = self._feature_weights
        for metrics in metrics_list:
            features = self._feature_vector(metrics)
            score = float(np.dot(features, weights) + self._bias)
            ranked.append(SurrogateRank(score=score, metrics=metrics))
        return sorted(ranked, key=lambda item: item.score, reverse=True)


================================================================================
File: optim/search.py
================================================================================

"""Wave 7 search wrappers for P3 candidate generation and ranking.

These helpers follow the gating notes in docs/MASTER_PLAN_AI_SCIENTIST.md:205 and
docs/TASKS_CODEX_MINI.md:145, where the Wave 7 DoD requires a structured
``search wrapper`` kernel (Nelder–Mead or CMA-ES) that still calls
`tools.evaluate_p3_set` for HV-aware scoring.
"""

from __future__ import annotations

from dataclasses import dataclass
from itertools import cycle
from typing import Any, Literal, Mapping, Sequence

import numpy as np

from ai_scientist import tools

_Method = Literal["nelder_mead", "cma_es"]


@dataclass(frozen=True)
class BatchSummary:
    """Minimal record of a candidate batch tied to P3 evaluations."""

    stage: str
    hv_score: float
    objectives: Sequence[Mapping[str, Any]]


@dataclass(frozen=True)
class _ParamSlot:
    path: tuple[str, ...]
    shape: tuple[int, ...]
    length: int


class _ParameterVectorizer:
    """Simple flatten/unflatten helper for the active parameter dict."""

    def __init__(self, prototype: Mapping[str, Any]) -> None:
        self._slots: list[_ParamSlot] = []
        self._dim = 0
        self._collect_slots(prototype, ())
        self._reference = self.flatten(prototype)

    def _collect_slots(self, node: Any, path: tuple[str, ...]) -> None:
        if isinstance(node, Mapping):
            for key, value in node.items():
                self._collect_slots(value, path + (key,))
            return
        array = np.asarray(node, dtype=float)
        shape = () if array.ndim == 0 else array.shape
        length = int(array.size)
        self._slots.append(_ParamSlot(path, shape, length))
        self._dim += length

    def _get_by_path(self, params: Mapping[str, Any], path: tuple[str, ...]) -> Any:
        node: Any = params
        for key in path:
            node = node[key]
        return node

    def _set_by_path(
        self, target: dict[str, Any], path: tuple[str, ...], value: Any
    ) -> None:
        node = target
        for key in path[:-1]:
            if key not in node or not isinstance(node[key], dict):
                node[key] = {}
            node = node[key]
        node[path[-1]] = value

    @property
    def dim(self) -> int:
        return self._dim

    @property
    def reference(self) -> np.ndarray:
        return self._reference.copy()

    def flatten(self, params: Mapping[str, Any]) -> np.ndarray:
        if self._dim == 0:
            return np.zeros(0, dtype=float)
        vector = np.empty(self._dim, dtype=float)
        offset = 0
        for slot in self._slots:
            raw_value = self._get_by_path(params, slot.path)
            array = np.asarray(raw_value, dtype=float)
            if slot.shape:
                array = array.reshape(slot.shape)
            else:
                array = array.reshape(())
            length = slot.length
            vector[offset : offset + length] = array.reshape(-1)
            offset += length
        return vector

    def unflatten(self, vector: Sequence[float] | np.ndarray) -> dict[str, Any]:
        if self._dim == 0:
            return {}
        params: dict[str, Any] = {}
        offset = 0
        arr = np.asarray(vector, dtype=float)
        for slot in self._slots:
            segment = arr[offset : offset + slot.length]
            offset += slot.length
            value: Any
            if slot.shape:
                value = segment.reshape(slot.shape).tolist()
            else:
                value = float(segment[0])
            self._set_by_path(params, slot.path, value)
        return params


class P3SearchWrapper:
    """Generate and score P3 proposals using the HV-aware set evaluator.

    The wrapper now follows the Wave 7 optimizer kernel mandate in
    docs/MASTER_PLAN_AI_SCIENTIST.md:331 and docs/TASKS_CODEX_MINI.md:145 by
    flattening the parameter dictionary and emitting either a Nelder–Mead
    simplex or CMA-ES-style batch before the HV surrogate ranking step.
    """

    def __init__(
        self,
        base_params: Mapping[str, Any],
        *,
        perturbation_scale: float = 0.05,
        method: _Method = "nelder_mead",
    ) -> None:
        self._vectorizer = _ParameterVectorizer(base_params)
        self._method = method
        self._scale = float(perturbation_scale)
        self._dim = self._vectorizer.dim
        self._mean = self._vectorizer.flatten(base_params)
        self._simplex = self._build_simplex(self._mean)
        self._sigma = max(self._scale, 1e-4)
        self._best_score = float("-inf")

    def _build_simplex(self, center: np.ndarray) -> list[np.ndarray]:
        if self._dim == 0:
            return [center.copy()]
        simplex: list[np.ndarray] = [center.copy()]
        for axis in range(self._dim):
            direction = np.zeros(self._dim, dtype=float)
            direction[axis] = self._scale
            simplex.append(center + direction)
        return simplex

    def propose_candidates(
        self, batch_size: int, seed: int | None = None
    ) -> list[Mapping[str, Any]]:
        """Generate a structured batch of proposals anchored on the current mean."""

        if batch_size <= 0:
            return []
        rng = np.random.default_rng(seed)
        proposals: list[Mapping[str, Any]] = []
        if self._dim == 0:
            base = self._vectorizer.unflatten(self._mean)
            return [base for _ in range(batch_size)]

        if self._method == "cma_es":
            for _ in range(batch_size):
                delta = rng.normal(scale=self._sigma, size=self._dim)
                proposals.append(self._vectorizer.unflatten(self._mean + delta))
            return proposals

        simplex_iter = cycle(self._simplex)
        while len(proposals) < batch_size:
            vertex = next(simplex_iter)
            if len(proposals) < len(self._simplex):
                proposals.append(self._vectorizer.unflatten(vertex))
            else:
                jitter = rng.uniform(-self._scale, self._scale, size=self._dim)
                proposals.append(self._vectorizer.unflatten(self._mean + jitter))
        return proposals[:batch_size]

    def evaluate_batch(self, candidates: Sequence[Mapping[str, Any]]) -> BatchSummary:
        """Score the proposals using the P3 HV-aware set evaluator."""

        evaluation = tools.evaluate_p3_set(candidates)
        return BatchSummary(
            stage=evaluation["stage"],
            hv_score=float(evaluation["hv_score"]),
            objectives=tuple(evaluation["objectives"]),
        )

    def _score_metrics(self, metrics: Mapping[str, Any]) -> float:
        gradient = float(metrics["minimum_normalized_magnetic_gradient_scale_length"])
        aspect = float(metrics["aspect_ratio"])
        return gradient - aspect

    def _update_state(
        self, best_vector: np.ndarray, hv_score: float, improved: bool
    ) -> None:
        if self._dim == 0:
            return
        if self._method == "nelder_mead":
            self._mean = best_vector
            self._simplex = self._build_simplex(best_vector)
            return
        self._best_score = max(self._best_score, hv_score)
        if improved:
            self._mean = best_vector
            self._sigma = max(self._sigma * 0.9, 1e-6)
        else:
            self._sigma = min(self._sigma * 1.1, 1.0)

    def rank_candidates(
        self, candidates: Sequence[Mapping[str, Any]]
    ) -> list[tuple[Mapping[str, Any], float]]:
        """Return proposals ordered by the HV proxy (gradient minus aspect)."""

        evaluation = tools.evaluate_p3_set(candidates)
        metrics_seq = evaluation.get("metrics_list", [])
        if not metrics_seq:
            return []
        scored: list[tuple[Mapping[str, Any], float]] = []
        for candidate, metrics in zip(candidates, metrics_seq):
            proxy_score = self._score_metrics(metrics)
            scored.append((candidate, proxy_score))
        best_idx = max(range(len(scored)), key=lambda idx: scored[idx][1])
        best_vector = self._vectorizer.flatten(candidates[best_idx])
        improved = evaluation["hv_score"] >= self._best_score
        self._update_state(best_vector, float(evaluation["hv_score"]), improved)
        return sorted(scored, key=lambda item: item[1], reverse=True)

