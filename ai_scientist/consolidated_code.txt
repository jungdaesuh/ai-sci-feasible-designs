=== Source Code Consolidation ===


================================================================================
File: planner.py
================================================================================

"""Agentized planning helper for Phase 3 (docs/roadmap & improvement plan guidance)."""

from __future__ import annotations

import hashlib
import json
from dataclasses import asdict
from pathlib import Path
from typing import Any, Mapping, Sequence

from ai_scientist import agent as agent_module
from ai_scientist import config as ai_config
from ai_scientist import memory
from ai_scientist import rag
from ai_scientist import tools
from ai_scientist import tools_api


class PlanningOutcome:
    """Structured output that mirrors the JSON sections sent to the planning agent prompt."""

    def __init__(
        self,
        context: Mapping[str, Any],
        evaluation_summary: Mapping[str, Any],
        boundary_summary: Mapping[str, Any],
        rag_snippets: Sequence[Mapping[str, str]],
        graph_summary: Mapping[str, Any] | None = None,
    ) -> None:
        self.context = context
        self.evaluation_summary = evaluation_summary
        self.boundary_summary = boundary_summary
        self.rag_snippets = rag_snippets
        self.graph_summary = graph_summary


def _serialize_summary(summary: tools.P3Summary | None) -> Mapping[str, Any] | None:
    if summary is None:
        return None
    return {
        "hv_score": summary.hv_score,
        "reference_point": list(summary.reference_point),
        "feasible_count": summary.feasible_count,
        "archive_size": summary.archive_size,
        "pareto_entries": [
            {
                **entry.as_mapping(),
                "design_hash": entry.design_hash,
                "stage": entry.stage,
            }
            for entry in summary.pareto_entries
        ],
    }


class PlanningAgent:
    """Wraps the planning-tier gate so runner cycles can rely on tool schemas + telemetry."""

    def __init__(
        self,
        *,
        config: ai_config.ModelConfig | None = None,
        rag_index: Path | str | None = None,
        world_model: memory.WorldModel | None = None,
    ) -> None:
        self.config = config or ai_config.load_model_config()
        self.planning_gate = agent_module.provision_model_tier(
            role="planning", config=self.config
        )
        self.literature_gate = agent_module.provision_model_tier(
            role="literature", config=self.config
        )
        self.analysis_gate = agent_module.provision_model_tier(
            role="analysis", config=self.config
        )
        self.rag_index = Path(rag_index or rag.DEFAULT_INDEX_PATH)
        self.world_model = world_model
        self.last_context: Mapping[str, Any] | None = None

    def _hash_context(self, payload: Mapping[str, Any]) -> str:
        text = json.dumps(payload, sort_keys=True, separators=(",", ":"))
        return hashlib.sha256(text.encode("utf-8")).hexdigest()

    def _validate_tool_call(
        self, gate: agent_module.AgentGate, tool_name: str, arguments: Mapping[str, Any]
    ) -> None:
        if not gate.allows(tool_name):
            raise ValueError(
                f"Tool '{tool_name}' is not permitted for {gate.model_alias} (role={gate.provider_model})"
            )
        schema = tools_api.get_tool_schema(tool_name)
        if schema:
            parameters = schema.get("parameters", {})
            required = parameters.get("required", [])
            missing = [field for field in required if field not in arguments]
            if missing:
                raise ValueError(
                    f"Tool '{tool_name}' missing required arguments: {missing}"
                )
        context_hash = self._hash_context(arguments)
        print(
            f"[planner][tool-call] role={gate.model_alias} tool={tool_name} context_hash={context_hash}"
        )

    def retrieve_rag(self, query: str, *, k: int = 3) -> list[dict[str, str]]:
        payload: dict[str, Any] = {"query": query}
        payload["k"] = k
        # RAG is allowed for multiple roles; we check planning gate by default here
        # but in a real loop the agent driver would pick the gate.
        self._validate_tool_call(self.planning_gate, "retrieve_rag", payload)
        return tools.retrieve_rag(query, k=k, index_path=self.rag_index)

    def write_note(
        self,
        content: str,
        experiment_id: int,
        cycle: int,
        filename: str | None = None,
    ) -> str:
        payload = {
            "content": content,
            "filename": filename,
            "experiment_id": experiment_id,
            "cycle": cycle,
        }
        self._validate_tool_call(self.literature_gate, "write_note", payload)
        return tools.write_note(
            content,
            filename=filename,
            world_model=self.world_model,
            experiment_id=experiment_id,
            cycle=cycle,
            memory_db=self.world_model.db_path if self.world_model else None,
        )

    def evaluate_p3(
        self,
        params: Mapping[str, Any],
        *,
        stage: str | None = None,
    ) -> Mapping[str, Any]:
        args = {
            "params": params,
            "problem": "p3",
        }
        if stage is not None:
            args["stage"] = stage
        self._validate_tool_call(self.planning_gate, "evaluate_p3", args)
        try:
            return tools.evaluate_p3(params, stage=stage or "p3")
        except Exception as exc:  # pragma: no cover - smoke-run safety
            message = f"planning-stage evaluate_p3 failed: {exc}"
            print(f"[planner] {message}")
            return {
                "stage": stage or "p3",
                "error": message,
                "objective": None,
                "feasibility": None,
                "hv": None,
            }

    def make_boundary(
        self, params: Mapping[str, Any]
    ) -> tools.surface_rz_fourier.SurfaceRZFourier:
        args = {"params": params}
        self._validate_tool_call(self.planning_gate, "make_boundary", args)
        return tools.make_boundary_from_params(params)

    def propose_boundary(
        self,
        params: Mapping[str, Any],
        perturbation_scale: float = 0.05,
        seed: int | None = None,
    ) -> dict[str, Any]:
        args = {"params": params, "perturbation_scale": perturbation_scale}
        if seed is not None:
            args["seed"] = seed
        self._validate_tool_call(self.planning_gate, "propose_boundary", args)
        return tools.propose_boundary(
            params, perturbation_scale=perturbation_scale, seed=seed
        )

    def _build_template_params(
        self, template: ai_config.BoundaryTemplateConfig
    ) -> Mapping[str, Any]:
        n_poloidal = template.n_poloidal_modes
        n_toroidal = template.n_toroidal_modes
        center_idx = n_toroidal // 2
        r_cos = []
        z_sin = []
        for pol in range(n_poloidal):
            r_row = []
            z_row = []
            for tor in range(n_toroidal):
                r_val = (
                    template.base_major_radius
                    if pol == 0 and tor == center_idx
                    else 0.0
                )
                z_val = (
                    template.base_minor_radius
                    if pol == 1 and tor == center_idx and n_poloidal > 1
                    else 0.0
                )
                r_row.append(r_val)
                z_row.append(z_val)
            r_cos.append(r_row)
            z_sin.append(z_row)
        return {
            "r_cos": r_cos,
            "z_sin": z_sin,
            "n_field_periods": template.n_field_periods,
            "is_stellarator_symmetric": True,
        }

    def _build_context(
        self,
        *,
        cycle_index: int,
        budgets: ai_config.BudgetConfig,
        stage_history: Sequence[Mapping[str, Any]],
        last_summary: tools.P3Summary | None,
        evaluation_summary: Mapping[str, Any],
        boundary_summary: Mapping[str, Any],
        rag_snippets: Sequence[Mapping[str, str]],
        graph_summary: Mapping[str, Any] | None = None,
    ) -> Mapping[str, Any]:
        context = {
            "cycle_index": cycle_index + 1,
            "planner_role": self.planning_gate.model_alias,
            "budgets": asdict(budgets),
            "stage_history": list(stage_history),
            "previous_p3_summary": _serialize_summary(last_summary),
            "latest_evaluation": evaluation_summary,
            "current_boundary": boundary_summary,
            "rag_snippets": list(rag_snippets),
            "graph_summary": graph_summary,
            "toolset": list(self.planning_gate.allowed_tools),
        }
        self.last_context = context
        return context

    def plan_cycle(
        self,
        *,
        cfg: ai_config.ExperimentConfig,
        cycle_index: int,
        stage_history: Sequence[Mapping[str, Any]],
        last_summary: tools.P3Summary | None,
        experiment_id: int | None = None,
    ) -> PlanningOutcome:
        cycle_number = cycle_index + 1
        
        # Literature Retrieval (planning role)
        rag_snippets = self.retrieve_rag(
            f"Planning guidance for {cfg.problem.upper()} cycle {cycle_number}",
            k=3,
        )
        
        # Planning Role Actions
        params = self._build_template_params(cfg.boundary_template)
        evaluation = self.evaluate_p3(
            params,
            stage=cfg.fidelity_ladder.screen,
        )
        boundary = self.make_boundary(params)
        
        evaluation_summary = {
            "objective": evaluation.get("objective"),
            "feasibility": evaluation.get("feasibility"),
            "hv": evaluation.get("hv"),
            "stage": evaluation.get("stage"),
            "agent_stage_label": f"agent-cycle-{cycle_number}",
        }
        boundary_summary = {
            "n_poloidal_modes": boundary.n_poloidal_modes,
            "n_toroidal_modes": boundary.n_toroidal_modes,
            "n_field_periods": boundary.n_field_periods,
            "stellarator_symmetric": boundary.is_stellarator_symmetric,
        }
        
        # Property Graph Snapshot (if world_model connected)
        graph_summary: Mapping[str, Any] | None = None
        if self.world_model and experiment_id is not None:
            pg = self.world_model.to_networkx(experiment_id)
            graph_dir = Path(cfg.reporting_dir) / "graphs"
            graph_dir.mkdir(parents=True, exist_ok=True)
            graph_file = graph_dir / f"cycle_{cycle_number}.json"
            
            nodes = [{"id": node, **attrs} for node, attrs in pg.nodes(data=True)]
            edges = [
                {"src": src, "dst": dst, "attrs": attrs}
                for src, dst, attrs in pg.edges(data=True)
            ]
            snapshot_data = {"nodes": nodes, "edges": edges}
            graph_file.write_text(json.dumps(snapshot_data, indent=2), encoding="utf-8")
            
            graph_summary = {
                "node_count": len(nodes),
                "edge_count": len(edges),
                "note_count": sum(1 for n in nodes if n.get("type") == "note"),
                "snapshot_path": str(graph_file)
            }

        context = self._build_context(
            cycle_index=cycle_index,
            budgets=cfg.budgets,
            stage_history=stage_history,
            last_summary=last_summary,
            evaluation_summary=evaluation_summary,
            boundary_summary=boundary_summary,
            rag_snippets=rag_snippets,
            graph_summary=graph_summary,
        )
        
        return PlanningOutcome(
            context=context,
            evaluation_summary=evaluation_summary,
            boundary_summary=boundary_summary,
            rag_snippets=rag_snippets,
            graph_summary=graph_summary,
        )


================================================================================
File: runner.py
================================================================================

"""Runner that wires budgets, fidelity decisions, and minimal reporting (Tasks 4.1 + B.*)."""

from __future__ import annotations

import argparse
import json
import math
import os
import platform
import subprocess
import sys
import time
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
from dataclasses import asdict, dataclass, replace
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Iterable, Mapping, Protocol, Sequence, Set, Tuple
import random

import numpy as np
import yaml

from ai_scientist import adapter
from ai_scientist import config as ai_config
from ai_scientist import memory
from ai_scientist import planner as ai_planner
from ai_scientist import rag
from ai_scientist import reporting
from ai_scientist import tools
from ai_scientist.optim.samplers import NearAxisSampler
from ai_scientist.optim.surrogate import SurrogateBundle
from constellaration.geometry import surface_rz_fourier as surface_module
from constellaration.initial_guess import generate_rotating_ellipse
from orchestration import adaptation as adaptation_helpers

FEASIBILITY_CUTOFF = getattr(tools, "_DEFAULT_RELATIVE_TOLERANCE", 1e-2)
P3_REFERENCE_POINT = getattr(tools, "_P3_REFERENCE_POINT", (1.0, 20.0))
_BOUNDARY_SEED_CACHE: dict[Path, dict[str, Any]] = {}
_SURROGATE_BUNDLE = SurrogateBundle()
_LAST_SURROGATE_FIT_SEC = 0.0


def _load_seed_boundary(path: Path) -> dict[str, Any]:
    resolved = path.resolve()
    cached = _BOUNDARY_SEED_CACHE.get(resolved)
    if cached is None:
        raw = json.loads(resolved.read_text(encoding="utf-8"))
        payload: dict[str, Any] = {
            "r_cos": np.asarray(raw["r_cos"], dtype=float),
            "z_sin": np.asarray(raw["z_sin"], dtype=float),
            "r_sin": np.asarray(raw["r_sin"], dtype=float)
            if raw.get("r_sin") is not None
            else None,
            "z_cos": np.asarray(raw["z_cos"], dtype=float)
            if raw.get("z_cos") is not None
            else None,
            "n_field_periods": int(
                raw.get("n_field_periods") or raw.get("nfp") or 1
            ),
            "is_stellarator_symmetric": bool(
                raw.get("is_stellarator_symmetric", True)
            ),
        }
        _BOUNDARY_SEED_CACHE[resolved] = payload
        cached = payload
    return {
        key: (np.array(value, copy=True) if isinstance(value, np.ndarray) else value)
        for key, value in cached.items()
    }


@dataclass(frozen=True)
class BudgetSnapshot:
    screen_evals_per_cycle: int
    promote_top_k: int
    max_high_fidelity_evals_per_cycle: int


@dataclass(frozen=True)
class CycleBudgetFeedback:
    hv_delta: float | None
    feasibility_rate: float | None
    cache_hit_rate: float | None


class BudgetController:
    STATE_VERSION = 1
    _STATE_FIELDS = {"_last_feedback", "_cache_stats"}

    def __init__(
        self,
        base_budgets: ai_config.BudgetConfig,
        adaptive_cfg: ai_config.AdaptiveBudgetConfig,
    ) -> None:
        self._base = base_budgets
        self._adaptive_cfg = adaptive_cfg
        self._last_feedback: CycleBudgetFeedback | None = None
        self._cache_stats: dict[str, dict[str, int]] = {}
        # NOTE: If new adaptive state is added, include it in to_dict/restore and checkpoints for deterministic resume.

    def to_dict(self) -> dict[str, Any]:
        unknown_state = {
            key
            for key in self.__dict__
            if key.startswith("_")
            and key not in {"_base", "_adaptive_cfg"}
            and key not in self._STATE_FIELDS
        }
        if unknown_state:
            raise ValueError(
                f"BudgetController state fields {unknown_state} are not serialized; "
                "update _STATE_FIELDS/to_dict/restore for deterministic resume."
            )
        feedback = (
            {
                "hv_delta": self._last_feedback.hv_delta,
                "feasibility_rate": self._last_feedback.feasibility_rate,
                "cache_hit_rate": self._last_feedback.cache_hit_rate,
            }
            if self._last_feedback
            else None
        )
        return {
            "state_version": self.STATE_VERSION,
            "base_budgets": asdict(self._base),
            "adaptive_cfg": asdict(self._adaptive_cfg),
            "adaptive_enabled": self._adaptive_cfg.enabled,
            "last_feedback": feedback,
            "cache_stats": self._cache_stats,
        }

    def restore(self, payload: Mapping[str, Any]) -> None:
        version = payload.get("state_version", 0)
        if version != self.STATE_VERSION:
            print(
                f"[budget] warning: checkpoint state_version={version} "
                f"!= expected {self.STATE_VERSION}; attempting best-effort restore."
            )
        feedback = payload.get("last_feedback")
        if feedback:
            self._last_feedback = CycleBudgetFeedback(
                hv_delta=feedback.get("hv_delta"),
                feasibility_rate=feedback.get("feasibility_rate"),
                cache_hit_rate=feedback.get("cache_hit_rate"),
            )
        cache_stats = payload.get("cache_stats")
        if isinstance(cache_stats, dict):
            self._cache_stats = cache_stats

    def snapshot(self) -> BudgetSnapshot:
        if not self._adaptive_cfg.enabled or self._last_feedback is None:
            return BudgetSnapshot(
                screen_evals_per_cycle=self._base.screen_evals_per_cycle,
                promote_top_k=self._base.promote_top_k,
                max_high_fidelity_evals_per_cycle=self._base.max_high_fidelity_evals_per_cycle,
            )
        return BudgetSnapshot(
            screen_evals_per_cycle=self._blend_budget(
                self._base.screen_evals_per_cycle,
                self._adaptive_cfg.screen_bounds,
                self._screen_score(self._last_feedback),
            ),
            promote_top_k=self._blend_budget(
                self._base.promote_top_k,
                self._adaptive_cfg.promote_top_k_bounds,
                self._promote_score(self._last_feedback),
            ),
            max_high_fidelity_evals_per_cycle=self._blend_budget(
                self._base.max_high_fidelity_evals_per_cycle,
                self._adaptive_cfg.high_fidelity_bounds,
                self._high_fidelity_score(self._last_feedback),
            ),
        )

    def capture_cache_hit_rate(
        self,
        stage: str,
        stats: Mapping[str, int] | None = None,
    ) -> float:
        stats = stats or tools.get_cache_stats(stage)
        previous = self._cache_stats.get(stage)
        delta_hits = stats.get("hits", 0)
        delta_misses = stats.get("misses", 0)
        if previous:
            delta_hits -= previous.get("hits", 0)
            delta_misses -= previous.get("misses", 0)
        self._cache_stats[stage] = {
            "hits": stats.get("hits", 0),
            "misses": stats.get("misses", 0),
        }
        total = max(0, delta_hits + delta_misses)
        if total <= 0:
            return 0.0
        return float(max(0.0, min(1.0, delta_hits / total)))

    def record_feedback(self, feedback: CycleBudgetFeedback) -> None:
        self._last_feedback = feedback

    def _blend_budget(
        self,
        base_value: int,
        bounds: ai_config.BudgetRangeConfig,
        score: float,
    ) -> int:
        constrained_base = max(bounds.min, min(bounds.max, base_value))
        if bounds.min >= bounds.max:
            return constrained_base
        clamped = max(0.0, min(1.0, score))
        mid = 0.5
        if clamped == mid:
            return constrained_base
        if clamped > mid:
            ratio = (clamped - mid) * 2.0
            increment = bounds.max - constrained_base
            return min(bounds.max, constrained_base + int(round(increment * ratio)))
        ratio = (mid - clamped) * 2.0
        decrement = constrained_base - bounds.min
        return max(bounds.min, constrained_base - int(round(decrement * ratio)))

    def _normalize(self, value: float | None, target: float) -> float:
        if value is None or target <= 0.0:
            return 0.0
        ratio = float(value) / float(target)
        return max(0.0, min(1.0, ratio))

    def _screen_score(self, feedback: CycleBudgetFeedback) -> float:
        progress = max(0.0, feedback.hv_delta or 0.0)
        normalized = self._normalize(progress, self._adaptive_cfg.hv_slope_reference)
        feasibility = self._normalize(
            feedback.feasibility_rate, self._adaptive_cfg.feasibility_target
        )
        return 1.0 - (0.6 * normalized + 0.4 * feasibility)

    def _promote_score(self, feedback: CycleBudgetFeedback) -> float:
        progress = max(0.0, feedback.hv_delta or 0.0)
        normalized = self._normalize(progress, self._adaptive_cfg.hv_slope_reference)
        feasibility = self._normalize(
            feedback.feasibility_rate, self._adaptive_cfg.feasibility_target
        )
        return 0.6 * normalized + 0.4 * feasibility

    def _high_fidelity_score(self, feedback: CycleBudgetFeedback) -> float:
        progress = max(0.0, feedback.hv_delta or 0.0)
        normalized = self._normalize(progress, self._adaptive_cfg.hv_slope_reference)
        cache = self._normalize(
            feedback.cache_hit_rate, self._adaptive_cfg.cache_hit_target
        )
        return 0.5 * normalized + 0.5 * cache


def _repo_relative(path: Path) -> str | None:
    allowed_prefixes = (
        "docs/",
        "constellaration/",
        "Jr.AI-Scientist/",
        "reports/",
        "tests/",
    )
    try:
        rel = path.resolve().relative_to(Path.cwd()).as_posix()
    except ValueError:
        return None
    for prefix in allowed_prefixes:
        if rel.startswith(prefix):
            return rel
    return None


class ProblemEvaluator(Protocol):
    def __call__(
        self,
        boundary_params: Mapping[str, Any],
        *,
        stage: str,
        use_cache: bool = True,
    ) -> dict[str, Any]: ...


class WorldModelLike(Protocol):
    def log_statement(
        self,
        experiment_id: int,
        cycle: int,
        stage: str,
        text: str,
        status: str,
        tool_name: str,
        tool_input: Mapping[str, Any],
        *,
        metrics_id: int | None = None,
        seed: int | None = None,
        git_sha: str,
        repro_cmd: str,
        created_at: str | None = None,
        commit: bool = True,
    ) -> int: ...


_PROBLEM_EVALUATORS: dict[str, tuple[str, ProblemEvaluator]] = {
    "p1": ("evaluate_p1", tools.evaluate_p1),
    "p2": ("evaluate_p2", tools.evaluate_p2),
    "p3": ("evaluate_p3", tools.evaluate_p3),
}


def _problem_evaluator(problem: str) -> ProblemEvaluator:
    try:
        return _PROBLEM_EVALUATORS[problem][1]
    except KeyError as exc:
        raise NotImplementedError(
            "Problem '%s' is not supported; choose one of %s."
            % (problem, ", ".join(sorted(_PROBLEM_EVALUATORS)))
        ) from exc


def _problem_tool_name(problem: str) -> str:
    try:
        return _PROBLEM_EVALUATORS[problem][0]
    except KeyError as exc:
        raise NotImplementedError(
            "Problem '%s' is not supported; choose one of %s."
            % (problem, ", ".join(sorted(_PROBLEM_EVALUATORS)))
        ) from exc


@dataclass
class RunnerCLIConfig:
    config_path: Path
    problem: str | None
    cycles: int | None
    memory_db: Path | None
    eval_budget: int | None
    workers: int | None
    pool_type: str | None
    screen_only: bool
    promote_only: bool
    slow: bool
    verbose: bool
    log_cache_stats: bool
    run_preset: str | None
    planner: str
    resume_from: Path | None = None


def _build_argument_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description=(
            "AI Scientist runner (per docs/TASKS_CODEX_MINI.md:191-195). "
            "Set AI_SCIENTIST_PEFT=1 to load adapter bundles from reports/adapters."
        )
    )
    parser.add_argument(
        "--config",
        type=Path,
        default=ai_config.DEFAULT_EXPERIMENT_CONFIG_PATH,
        help="Path to the experiment configuration YAML (defaults to configs/experiment.yaml).",
    )
    parser.add_argument(
        "--problem",
        choices=["p1", "p2", "p3"],
        help=(
            "Problem identifier that overrides the config (p1=GeometricalProblem, "
            "p2=SimpleToBuildQIStellarator, p3=MHDStableQIStellarator)."
        ),
    )
    parser.add_argument(
        "--cycles",
        type=int,
        help="Number of governance cycles to run (overrides config; each cycle includes screening → reporting).",
    )
    parser.add_argument(
        "--memory-db",
        type=Path,
        help="Path to the shared SQLite world model (overrides config).",
    )
    parser.add_argument(
        "--eval-budget",
        type=int,
        help="Override the per-cycle screening budget (screen_evals_per_cycle).",
    )
    parser.add_argument(
        "--workers",
        type=int,
        help="Override n_workers from the config (also powers multiprocessing pools).",
    )
    parser.add_argument(
        "--pool-type",
        choices=["thread", "process"],
        help="Choose the executor pool type used when n_workers > 1.",
    )
    parser.add_argument(
        "--screen",
        action="store_true",
        help=(
            "Run only the screening stage (governance S1) and skip promotions. "
            "Cannot be combined with --promote or presets that advance directly to S2."
        ),
    )
    parser.add_argument(
        "--promote",
        action="store_true",
        help=(
            "Start governance in promote/refine mode (S2+) and report promotions. "
            "Cannot be combined with --screen or presets that force S1-only behavior."
        ),
    )
    parser.add_argument(
        "--slow",
        action="store_true",
        help="Throttle loop iterations for deterministic, long-wall-clock logging and traceability.",
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Emit additional runner diagnostics (candidate mixes, gating decisions).",
    )
    parser.add_argument(
        "--log-cache-stats",
        action="store_true",
        help="Write per-stage cache stats to reports/cache_stats.jsonl for Phase 5 observability.",
    )
    parser.add_argument(
        "--run-preset",
        type=str,
        help="Name of a preset from configs/run_presets.yaml that toggles --screen/--promote/--slow.",
    )
    parser.add_argument(
        "--planner",
        choices=["deterministic", "agent"],
        default="deterministic",
        help="Choose the planning driver (deterministic loop or Phase 3 agent).",
    )
    parser.add_argument(
        "--resume-from",
        type=Path,
        help="Path to a cycle checkpoint JSON to resume from (skips completed cycles).",
    )
    return parser


def parse_args(args: Sequence[str] | None = None) -> RunnerCLIConfig:
    parser = _build_argument_parser()
    namespace = parser.parse_args(args)
    if namespace.screen and namespace.promote:
        parser.error("--screen cannot be combined with --promote.")
    return RunnerCLIConfig(
        config_path=namespace.config,
        problem=namespace.problem,
        cycles=namespace.cycles,
        memory_db=namespace.memory_db,
        eval_budget=namespace.eval_budget,
        workers=namespace.workers,
        pool_type=namespace.pool_type,
        screen_only=bool(namespace.screen),
        promote_only=bool(namespace.promote),
        slow=bool(namespace.slow),
        verbose=bool(namespace.verbose),
        log_cache_stats=bool(namespace.log_cache_stats),
        run_preset=namespace.run_preset,
        planner=namespace.planner,
        resume_from=namespace.resume_from,
    )


def _validate_runtime_flags(runtime: RunnerCLIConfig) -> None:
    if runtime.screen_only and runtime.promote_only:
        raise ValueError(
            "--screen (S1-only) cannot be combined with promote-only mode (S2+) "
            f"(presets: {runtime.run_preset or '<none>'}). Remove one flag/preset."
        )


def _generate_candidate_params(
    template: ai_config.BoundaryTemplateConfig, seed: int
) -> dict[str, Any]:
    rng = np.random.default_rng(seed)
    seed_data: dict[str, Any] | None = None
    seed_path = template.seed_path
    if seed_path is None:
        default_seed = Path("configs/seeds/rotating_ellipse_p3.json")
        seed_path = default_seed if default_seed.exists() else None

    if seed_path is not None:
        seed_data = _load_seed_boundary(seed_path)
        r_cos = seed_data["r_cos"]
        z_sin = seed_data["z_sin"]
        r_sin = seed_data["r_sin"]
        z_cos = seed_data["z_cos"]
        n_field_periods = int(seed_data["n_field_periods"])
        is_stellarator_symmetric = bool(seed_data["is_stellarator_symmetric"])
    else:
        base_surface = generate_rotating_ellipse(
            aspect_ratio=4.0,
            elongation=1.5,
            rotational_transform=1.2,
            n_field_periods=template.n_field_periods,
        )
        max_poloidal = max(1, template.n_poloidal_modes - 1)
        max_toroidal = max(1, (template.n_toroidal_modes - 1) // 2)
        expanded = surface_module.set_max_mode_numbers(
            base_surface,
            max_poloidal_mode=max_poloidal,
            max_toroidal_mode=max_toroidal,
        )
        r_cos = np.asarray(expanded.r_cos, dtype=float)
        z_sin = np.asarray(expanded.z_sin, dtype=float)
        center_idx = r_cos.shape[1] // 2
        r_cos[0, center_idx] = template.base_major_radius
        if r_cos.shape[0] > 1:
            z_sin[1, center_idx] = template.base_minor_radius

        r_sin = None
        z_cos = None
        n_field_periods = template.n_field_periods
        is_stellarator_symmetric = True

    r_cos += rng.normal(scale=template.perturbation_scale, size=r_cos.shape)
    z_sin += rng.normal(scale=template.perturbation_scale / 2, size=z_sin.shape)
    if seed_data is not None:
        if r_sin is not None:
            r_sin += rng.normal(scale=template.perturbation_scale / 2, size=r_sin.shape)
        if z_cos is not None:
            z_cos += rng.normal(scale=template.perturbation_scale / 2, size=z_cos.shape)

    if is_stellarator_symmetric:
        n_cols = r_cos.shape[1]
        center_idx = n_cols // 2
        if center_idx > 0:
            r_cos[0, :center_idx] = 0.0
        z_sin[0, :] = 0.0

    params = {
        "r_cos": r_cos.tolist(),
        "z_sin": z_sin.tolist(),
        "n_field_periods": template.n_field_periods or n_field_periods,
        "is_stellarator_symmetric": is_stellarator_symmetric,
    }
    if r_sin is not None:
        params["r_sin"] = r_sin.tolist()
    if z_cos is not None:
        params["z_cos"] = z_cos.tolist()

    return {
        "seed": seed,
        "params": params,
        "design_hash": tools.design_hash(params),
    }


def _propose_p3_candidates_for_cycle(
    cfg: ai_config.ExperimentConfig,
    cycle_index: int,
    world_model: memory.WorldModel,
    experiment_id: int,
    *,
    screen_budget: int,
    total_candidates: int | None = None,
    prev_feasibility_rate: float | None = None,
) -> tuple[list[Mapping[str, Any]], int, int]:
    """Blend constraint-aware sampling and random noise per roadmap Phase 1 guidance.

    If total_candidates is omitted, screen_budget drives the pool size.
    See /Users/suhjungdae/code/software/proxima_fusion/RL-feasible-designs/ai_scientist/roadmap.md for the Phase 1 candidate-generation recipe that introduced this helper.
    """

    pool_size = total_candidates if total_candidates is not None else screen_budget
    total_candidates = int(pool_size)
    if total_candidates <= 0:
        return [], 0, 0

    mix = cfg.proposal_mix
    exploration_weight = mix.exploration_ratio
    if prev_feasibility_rate is not None:
        # Decay exploration as feasibility improves (Workstream 1)
        exploration_weight *= 1.0 - min(1.0, max(0.0, prev_feasibility_rate))

    ratio_sum = mix.constraint_ratio + exploration_weight
    if ratio_sum <= 0.0:
        sampler_target = 0
    else:
        sampler_target = int(
            round(total_candidates * (mix.constraint_ratio / ratio_sum))
        )
    sampler_target = min(sampler_target, total_candidates)

    stage_limit = max(total_candidates * 4, 16)
    stage_records = world_model.recent_stage_candidates(
        experiment_id=experiment_id,
        problem=cfg.problem,
        stage=cfg.fidelity_ladder.promote,
        limit=int(stage_limit),
    )

    if stage_records and sampler_target > 0:
        base_designs = [record[0] for record in stage_records]
        feasibilities = [max(0.0, float(record[1])) for record in stage_records]
        max_feas = max(feasibilities, default=0.0)
        normalized_distances = [
            0.0 if max_feas <= 0.0 else min(1.0, value / max_feas)
            for value in feasibilities
        ]
        rng_seed = cfg.random_seed + cycle_index + total_candidates
        sampler_params = tools.normalized_constraint_distance_sampler(
            base_designs,
            normalized_distances=normalized_distances,
            proposal_count=sampler_target,
            jitter_scale=mix.jitter_scale,
            rng=np.random.default_rng(rng_seed),
            include_distances=True,
        )
    else:
        sampler_params = []

    candidate_seeds = [
        cfg.random_seed + cycle_index * total_candidates + i
        for i in range(total_candidates)
    ]
    seed_iter = iter(candidate_seeds)
    sampler_results: list[Mapping[str, Any]] = []
    for params in sampler_params:
        try:
            seed = next(seed_iter)
        except StopIteration:
            break
        if isinstance(params, Mapping) and "params" in params:
            payload = params.get("params", {})
            constraint_distance = float(params.get("normalized_constraint_distance", 0.0))
        else:
            payload = params
            constraint_distance = 0.0
        sampler_results.append(
            {
                "seed": seed,
                "params": payload,
                "design_hash": tools.design_hash(payload),
                "constraint_distance": constraint_distance,
                "source": "constraint_sampler",
            }
        )

    remaining = total_candidates - len(sampler_results)
    remaining_seeds = []
    for _ in range(remaining):
        try:
            remaining_seeds.append(next(seed_iter))
        except StopIteration:
            break

    random_results: list[Mapping[str, Any]] = []
    
    if cfg.proposal_mix.sampler_type == "near_axis":
        try:
            sampler = NearAxisSampler(cfg.boundary_template)
            random_results = sampler.generate(remaining_seeds)
        except Exception as exc:
            print(f"[runner] NearAxisSampler failed: {exc}; falling back to standard random")
            random_results = []

    used_seeds = {c["seed"] for c in random_results}
    fallback_seeds = [s for s in remaining_seeds if s not in used_seeds]

    for seed in fallback_seeds:
        random_results.append(
            {
                **_generate_candidate_params(cfg.boundary_template, seed),
                "constraint_distance": 1.0,
                "source": "random",
            }
        )

    candidates = sampler_results + random_results
    return candidates, len(sampler_results), len(random_results)


def _surrogate_candidate_pool_size(
    screen_budget: int,
    surrogate_pool_multiplier: float,
) -> int:
    if screen_budget <= 0:
        return 0
    multiplier = max(1.0, surrogate_pool_multiplier)
    proposed = int(math.ceil(screen_budget * multiplier))
    return max(proposed, screen_budget)


def _surrogate_rank_screen_candidates(
    cfg: ai_config.ExperimentConfig,
    screen_budget: int,
    candidates: list[Mapping[str, Any]],
    world_model: memory.WorldModel,
    *,
    cycle: int = 0,
    verbose: bool = False,
) -> list[Mapping[str, Any]]:
    """Train SurrogateBundle on cached history and trim the pool."""

    if not candidates or screen_budget <= 0:
        return candidates

    problem = (cfg.problem or "").lower()
    target_column = "hv" if problem == "p3" else "objective"
    minimize_objective = problem == "p1"

    history = world_model.surrogate_training_data(
        target=target_column, problem=cfg.problem
    )
    global _LAST_SURROGATE_FIT_SEC
    _LAST_SURROGATE_FIT_SEC = 0.0
    metrics_list: tuple[Mapping[str, Any], ...]
    target_values: tuple[float, ...]
    if history:
        metrics_list, target_values = zip(*history)
        if _SURROGATE_BUNDLE.should_retrain(len(history), cycle=cycle):
            start = time.perf_counter()
            _SURROGATE_BUNDLE.fit(
                metrics_list,
                target_values,
                minimize_objective=minimize_objective,
                cycle=cycle,
            )
            _LAST_SURROGATE_FIT_SEC = time.perf_counter() - start
    else:
        metrics_list = tuple()
        target_values = tuple()

    pool_entries: list[Mapping[str, Any]] = []
    for idx, candidate in enumerate(candidates):
        pool_entries.append(
            {
                "candidate_params": candidate["params"],
                "constraint_distance": candidate.get("constraint_distance", 0.0),
                "source": candidate.get("source"),
                "__surrogate_candidate_index": idx,
            }
        )

    ranked_predictions = _SURROGATE_BUNDLE.rank_candidates(
        pool_entries,
        minimize_objective=minimize_objective,
        exploration_ratio=cfg.proposal_mix.exploration_ratio,
    )
    if cfg.problem.lower() == "p2":
        # Phase 5 HV/Objective (docs/AI_SCIENTIST_UNIFIED_ROADMAP.md §5):
        # gate P2 promotions on feasibility probability before objective.
        prob_threshold = max(
            0.0, min(1.0, cfg.adaptive_budgets.feasibility_target)
        )
        filtered_predictions = [
            prediction
            for prediction in ranked_predictions
            if prediction.prob_feasible >= prob_threshold
        ]
        if filtered_predictions:
            ranked_predictions = filtered_predictions

    selected: list[Mapping[str, Any]] = []
    needed = screen_budget
    for prediction in ranked_predictions:
        idx = int(prediction.metadata.get("__surrogate_candidate_index", -1))
        if idx < 0:
            continue
        selected.append(candidates[idx])
        if len(selected) >= needed:
            break

    if not selected:
        return candidates[:needed]

    if verbose:
        dropped = len(candidates) - len(selected)
        print(
            f"[runner][surrogate] trained on {len(history)} rows, "
            f"selected {len(selected)}/{len(candidates)} candidates (dropped {dropped})"
        )

    return selected


def _time_exceeded(start: float, limit_minutes: float) -> bool:
    elapsed = time.perf_counter() - start
    return elapsed >= limit_minutes * 60


@dataclass
class CycleSummary:
    cycle: int
    objective: float | None
    feasibility: float | None
    hv: float | None
    stage: str


def _stage_rank(stage: str | None, promote_stage: str) -> int:
    if stage == promote_stage:
        return 2
    if stage:
        return 1
    return 0


def _feasibility_value(entry: Mapping[str, Any]) -> float:
    return float(entry["evaluation"].get("feasibility", float("inf")))


def _oriented_objective(entry: Mapping[str, Any]) -> float:
    evaluation = entry["evaluation"]
    objective = evaluation.get("objective")
    if objective is None:
        return float("inf")
    minimize = evaluation.get("minimize_objective", True)
    value = float(objective)
    return value if minimize else -value


def _prefer_entry(
    current: Mapping[str, Any],
    candidate: Mapping[str, Any],
    promote_stage: str,
) -> Mapping[str, Any]:
    current_rank = _stage_rank(current["evaluation"].get("stage"), promote_stage)
    candidate_rank = _stage_rank(candidate["evaluation"].get("stage"), promote_stage)
    if candidate_rank > current_rank:
        return candidate
    if candidate_rank < current_rank:
        return current

    current_feas = _feasibility_value(current)
    candidate_feas = _feasibility_value(candidate)
    if candidate_feas < current_feas:
        return candidate
    if candidate_feas > current_feas:
        return current

    current_obj = _oriented_objective(current)
    candidate_obj = _oriented_objective(candidate)
    if candidate_obj < current_obj:
        return candidate
    return current


def _close_metric(value_a: float | None, value_b: float | None) -> bool:
    if value_a is None or value_b is None:
        return value_a is None and value_b is None
    return math.isclose(value_a, value_b, rel_tol=1e-3, abs_tol=1e-3)


def _verify_best_claim(
    world_model: WorldModelLike,
    experiment_id: int,
    cycle_number: int,
    best_entry: Mapping[str, Any],
    best_eval: Mapping[str, Any],
    evaluation_fn: ProblemEvaluator,
    tool_name: str,
    best_seed: int,
    git_sha: str,
    reproduction_command: str,
    *,
    stage: str,
    metrics_id: int | None,
) -> str:
    tool_input = {"params": best_entry["params"], "stage": stage}
    replay_eval = evaluation_fn(
        best_entry["params"],
        stage=stage,
        use_cache=False,
    )
    differences: list[str] = []
    for metric in ("objective", "feasibility", "hv"):
        if not _close_metric(best_eval.get(metric), replay_eval.get(metric)):
            differences.append(metric)
    status = "SUPPORTED" if not differences else "REFUTED"
    statement_text = f"Replayed {tool_name} evaluation for design {best_entry.get('design_hash', '')[:8]} at stage {stage}."
    world_model.log_statement(
        experiment_id=experiment_id,
        cycle=cycle_number,
        stage=stage,
        text=statement_text,
        status=status,
        tool_name=tool_name,
        tool_input=tool_input,
        metrics_id=metrics_id,
        seed=best_seed,
        git_sha=git_sha,
        repro_cmd=reproduction_command,
    )
    print(
        f"[runner][verifier] statement status={status} differences={differences} for cycle {cycle_number}"
    )
    return status


def _latest_evaluations_by_design(
    aggregated: Sequence[Mapping[str, Any]], promote_stage: str
) -> dict[str, Mapping[str, Any]]:
    latest: dict[str, Mapping[str, Any]] = {}
    for entry in aggregated:
        design_hash = entry.get("design_hash")
        if not design_hash:
            continue
        existing = latest.get(design_hash)
        if existing is None:
            latest[design_hash] = entry
            continue
        latest[design_hash] = _prefer_entry(
            existing,
            entry,
            promote_stage,
        )
    return latest


def _extract_objectives(entry: Mapping[str, Any]) -> tuple[float, float]:
    metrics = entry["evaluation"]["metrics"]
    gradient = float(metrics["minimum_normalized_magnetic_gradient_scale_length"])
    aspect = float(metrics["aspect_ratio"])
    return gradient, aspect


def _objective_proxy(entry: Mapping[str, Any]) -> float:
    gradient, aspect = _extract_objectives(entry)
    return gradient - aspect


def _crowding_distance(
    entries_by_design: Mapping[str, Mapping[str, Any]],
) -> dict[str, float]:
    if not entries_by_design:
        return {}
    values: list[tuple[str, float, float]] = []
    for design_hash, entry in entries_by_design.items():
        gradient, aspect = _extract_objectives(entry)
        values.append((design_hash, float(gradient), float(aspect)))
    distances = {design_hash: 0.0 for design_hash in entries_by_design}
    if len(values) <= 2:
        for design_hash in distances:
            distances[design_hash] = float("inf")
        return distances
    sorted_grad = sorted(values, key=lambda item: item[1], reverse=True)
    grad_values = [item[1] for item in sorted_grad]
    grad_span = max(max(grad_values) - min(grad_values), 1e-9)
    distances[sorted_grad[0][0]] = float("inf")
    distances[sorted_grad[-1][0]] = float("inf")
    for pos in range(1, len(sorted_grad) - 1):
        prev_val = sorted_grad[pos - 1][1]
        next_val = sorted_grad[pos + 1][1]
        distances[sorted_grad[pos][0]] += abs(next_val - prev_val) / grad_span

    sorted_aspect = sorted(values, key=lambda item: item[2], reverse=False)
    aspect_values = [item[2] for item in sorted_aspect]
    aspect_span = max(max(aspect_values) - min(aspect_values), 1e-9)
    distances[sorted_aspect[0][0]] = float("inf")
    distances[sorted_aspect[-1][0]] = float("inf")
    for pos in range(1, len(sorted_aspect) - 1):
        prev_val = sorted_aspect[pos - 1][2]
        next_val = sorted_aspect[pos + 1][2]
        distances[sorted_aspect[pos][0]] += abs(next_val - prev_val) / aspect_span
    return distances


def _rank_candidates_for_promotion(
    entries_by_design: Mapping[str, Mapping[str, Any]],
    promote_limit: int,
    reference_point: Tuple[float, float],
) -> list[Mapping[str, Any]]:
    if not entries_by_design:
        return []
    summary = tools.summarize_p3_candidates(
        list(entries_by_design.values()), reference_point=reference_point
    )
    # Phase 5 HV/Objective (docs/AI_SCIENTIST_UNIFIED_ROADMAP.md §5):
    # rank promotions by feasibility first, then objective proxy.
    ordered_hashes = [entry.design_hash for entry in summary.pareto_entries]
    ranked: list[Mapping[str, Any]] = sorted(
        (
            entries_by_design[h]
            for h in ordered_hashes
            if h in entries_by_design
        ),
        key=lambda entry: (
            0.0 if _feasibility_value(entry) <= FEASIBILITY_CUTOFF else 1.0,
            _feasibility_value(entry),
            -_objective_proxy(entry),
        ),
    )
    if len(ranked) >= promote_limit:
        return ranked[:promote_limit]
    remaining = {
        design_hash: entry
        for design_hash, entry in entries_by_design.items()
        if design_hash not in {entry.design_hash for entry in summary.pareto_entries}
    }
    crowding = _crowding_distance(remaining)

    def _sort_key(design_hash: str) -> tuple[float, float, float]:
        entry = remaining[design_hash]
        feas = _feasibility_value(entry)
        feasible_flag = 0.0 if feas <= FEASIBILITY_CUTOFF else 1.0
        return (
            feasible_flag,
            -crowding.get(design_hash, 0.0),
            feas,
            -_objective_proxy(entry),
        )

    for design_hash in sorted(remaining, key=_sort_key):
        ranked.append(remaining[design_hash])
        if len(ranked) >= promote_limit:
            break
    return ranked


def _persist_pareto_archive(
    *,
    world_model: memory.WorldModel,
    experiment_id: int,
    cycle_number: int,
    problem: str,
    entries_by_design: Mapping[str, Mapping[str, Any]],
    p3_summary: tools.P3Summary,
    git_sha: str,
    constellaration_sha: str,
) -> tuple[Set[str], dict[str, int]]:
    """Persist Phase 6 Pareto deliverables and return the logged design hashes."""

    logged_hashes: Set[str] = set()
    archive_rows: list[Mapping[str, Any]] = []
    metrics_by_hash: dict[str, int] = {}
    for entry in p3_summary.pareto_entries:
        design_hash = entry.design_hash
        latest = entries_by_design.get(design_hash)
        if latest is None:
            continue
        evaluation = latest["evaluation"]
        candidate_id, metrics_id = world_model.log_candidate(
            experiment_id=experiment_id,
            problem=problem,
            params=latest["params"],
            seed=int(latest.get("seed", -1)),
            status=evaluation.get("stage", "unknown"),
            evaluation=evaluation,
            design_hash=design_hash,
            commit=False,
        )
        logged_hashes.add(design_hash)
        metrics_by_hash[design_hash] = metrics_id
        settings_json = json.dumps(
            evaluation.get("settings", {}), separators=(",", ":")
        )
        archive_rows.append(
            {
                "design_hash": design_hash,
                "fidelity": evaluation.get("stage", "unknown"),
                "gradient": entry.gradient,
                "aspect": entry.aspect_ratio,
                "metrics_id": metrics_id,
                "git_sha": git_sha,
                "constellaration_sha": constellaration_sha,
                "settings_json": settings_json,
                "seed": int(latest.get("seed", -1)),
            }
        )
        world_model.upsert_pareto(experiment_id, candidate_id)
    if archive_rows:
        world_model.record_pareto_archive(
            experiment_id,
            cycle_number,
            archive_rows,
            commit=False,
        )
    return logged_hashes, metrics_by_hash


def _relative_objective_improvement(
    history: list[CycleSummary], lookback: int
) -> float:
    if len(history) <= lookback:
        return 0.0
    earlier = history[-lookback - 1].objective
    latest = history[-1].objective
    if earlier is None or latest is None:
        return 0.0
    diff = earlier - latest
    denom = abs(earlier) if abs(earlier) > 1e-6 else 1.0
    return float(diff / denom)


def _should_transition_s1_to_s2(
    history: list[CycleSummary], gate_cfg: ai_config.StageGateConfig
) -> bool:
    if not history:
        return False
    last = history[-1]
    if last.feasibility is not None:
        triggered = last.feasibility <= gate_cfg.s1_to_s2_feasibility_margin
        print(
            f"[runner][stage-gate] S1→S2 feasibility check: margin={last.feasibility:.5f} "
            f"<= {gate_cfg.s1_to_s2_feasibility_margin:.5f} -> {triggered}"
        )
        if triggered:
            return True
    improvement = _relative_objective_improvement(
        history, gate_cfg.s1_to_s2_lookback_cycles
    )
    triggered_improvement = improvement >= gate_cfg.s1_to_s2_objective_improvement
    print(
        f"[runner][stage-gate] S1→S2 objective improvement check: "
        f"{improvement:.4f} >= {gate_cfg.s1_to_s2_objective_improvement:.4f} -> {triggered_improvement}"
    )
    return triggered_improvement


def _should_transition_s2_to_s3(
    history: list[CycleSummary],
    gate_cfg: ai_config.StageGateConfig,
    governance_cfg: ai_config.GovernanceConfig,
    world_model: memory.WorldModel,
    experiment_id: int,
    current_cycle: int,
    total_cycles: int,
) -> bool:
    avg_delta = world_model.average_recent_hv_delta(
        experiment_id, governance_cfg.hv_lookback
    )
    if avg_delta is not None:
        triggered_delta = avg_delta <= gate_cfg.s2_to_s3_hv_delta
        print(
            f"[runner][stage-gate] S2→S3 average HV delta over "
            f"{governance_cfg.hv_lookback} cycles: {avg_delta:.4f} <= {gate_cfg.s2_to_s3_hv_delta:.4f} -> {triggered_delta}"
        )
        if triggered_delta:
            return True
    else:
        print(
            f"[runner][stage-gate] insufficient HV delta history ({len(history)} cycles) "
            f"to evaluate lookback={governance_cfg.hv_lookback}; deferring promotion"
        )
    exhausted = current_cycle >= total_cycles
    print(
        f"[runner][stage-gate] S2→S3 budget check: cycle={current_cycle} >= total={total_cycles} -> {exhausted}"
    )
    return exhausted


def _process_worker_initializer() -> None:
    """Limit OpenMP threads inside process workers (Phase 5 observability safeguard)."""
    os.environ["OMP_NUM_THREADS"] = "1"


def _evaluate_stage(
    candidates: Iterable[Mapping[str, Any]],
    stage: str,
    budgets: ai_config.BudgetConfig,
    cycle_start: float,
    evaluate_fn: ProblemEvaluator,
    *,
    sleep_per_eval: float = 0.0,
) -> list[dict[str, Any]]:
    """Evaluate candidates at a given fidelity, respecting wall-clock budget."""

    results: list[dict[str, Any]] = []
    wall_limit = budgets.wall_clock_minutes

    if budgets.n_workers <= 1:
        for candidate in candidates:
            if _time_exceeded(cycle_start, wall_limit):
                break
            params = candidate["params"]
            design_id = candidate.get("design_hash") or tools.design_hash(params)
            try:
                evaluation = evaluate_fn(params, stage=stage)
                evaluation.setdefault("vmec_status", "ok")
            except Exception as exc:  # noqa: BLE001
                print(
                    f"[runner][stage-eval] Failed evaluation for design {design_id} "
                    f"(seed={candidate['seed']} stage={stage}): {exc}"
                )
                evaluation = {
                    "stage": stage,
                    "feasibility": float("inf"),
                    "max_violation": float("inf"),
                    "objective": float("inf"),
                    "is_feasible": False,
                    "vmec_status": "exception",
                    "metrics": {
                        "minimum_normalized_magnetic_gradient_scale_length": 0.0,
                        "aspect_ratio": float("inf"),
                        "max_elongation": float("inf"),
                        "constraint_margins": {},
                        "max_violation": float("inf"),
                    },
                }
            results.append(
                {
                    "params": params,
                    "evaluation": evaluation,
                    "seed": int(candidate["seed"]),
                    "design_hash": design_id,
                }
            )
            if sleep_per_eval > 0:
                time.sleep(sleep_per_eval)
        return results

    future_payloads = {}
    executor_cls = (
        ThreadPoolExecutor if budgets.pool_type == "thread" else ProcessPoolExecutor
    )
    executor_kwargs: dict[str, Any] = {"max_workers": budgets.n_workers}
    if executor_cls is ProcessPoolExecutor:
        executor_kwargs["initializer"] = _process_worker_initializer
    with executor_cls(**executor_kwargs) as executor:
        for candidate in candidates:
            if _time_exceeded(cycle_start, wall_limit):
                break
            design_id = candidate.get("design_hash")
            future = executor.submit(evaluate_fn, candidate["params"], stage=stage)
            future_payloads[future] = (candidate, design_id)

        for future in as_completed(future_payloads):
            candidate, design_id = future_payloads[future]
            exc = future.exception()
            if exc is not None:
                design_hash = design_id or tools.design_hash(candidate["params"])
                print(
                    f"[runner][stage-eval] Failed evaluation for design {design_hash} "
                    f"(seed={candidate['seed']} stage={stage}): {exc}"
                )
                failure_eval = {
                    "stage": stage,
                    "feasibility": float("inf"),
                    "max_violation": float("inf"),
                    "objective": float("inf"),
                    "is_feasible": False,
                    "vmec_status": "exception",
                    "metrics": {
                        "minimum_normalized_magnetic_gradient_scale_length": 0.0,
                        "aspect_ratio": float("inf"),
                        "max_elongation": float("inf"),
                        "constraint_margins": {},
                        "max_violation": float("inf"),
                    },
                }
                results.append(
                    {
                        "params": candidate["params"],
                        "evaluation": failure_eval,
                        "seed": int(candidate["seed"]),
                        "design_hash": design_hash,
                    }
                )
                continue

            results.append(
                {
                    "params": candidate["params"],
                    "evaluation": {
                        **future.result(),
                        "vmec_status": future.result().get("vmec_status", "ok"),
                    },
                    "seed": int(candidate["seed"]),
                    "design_hash": design_id or tools.design_hash(candidate["params"]),
                }
            )
    return results


def _cache_stats_log_path(report_dir: Path | str) -> Path:
    return Path(report_dir) / "cache_stats.jsonl"


def _observability_log_path(report_dir: Path | str) -> Path:
    return Path(report_dir) / "observability.jsonl"


def _maybe_log_cache_stats(
    runtime: RunnerCLIConfig | None,
    cfg: ai_config.ExperimentConfig,
    cycle_index: int,
    stage: str,
    stats: Mapping[str, int],
) -> None:
    """Emit per-cycle cache stats for Phase 5 observability (see ai_scientist/roadmap.md & ai_scientist/improvement-plan.md)."""
    if not (runtime and runtime.log_cache_stats):
        return
    entry = {
        "cycle": cycle_index + 1,
        "stage": stage,
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "stats": stats,
    }
    log_path = _cache_stats_log_path(cfg.reporting_dir)
    log_path.parent.mkdir(parents=True, exist_ok=True)
    with log_path.open("a", encoding="utf-8") as handle:
        handle.write(json.dumps(entry, separators=(",", ":")) + "\n")


def _vmec_failure_rate(results: Sequence[Mapping[str, Any]]) -> float:
    if not results:
        return 0.0
    total = len(results)
    failures = 0
    for entry in results:
        status = str(entry.get("evaluation", {}).get("vmec_status", "success")).lower()
        if status and status not in {"ok", "success"}:
            failures += 1
    return float(failures) / float(total)


def _log_observability_metrics(
    cfg: ai_config.ExperimentConfig,
    cycle_index: int,
    *,
    hv: float | None,
    feasible_count: int,
    vmec_failure_rate: float,
    retrain_time: float,
    cache_hit_rate: float,
    budget_snapshot: BudgetSnapshot,
) -> None:
    entry = {
        "cycle": cycle_index + 1,
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "hv": hv,
        "feasible_count": feasible_count,
        "vmec_failure_rate": vmec_failure_rate,
        "surrogate_retrain_seconds": retrain_time,
        "cache_hit_rate": cache_hit_rate,
        "budget_overrides": {
            "screen_evals_per_cycle": budget_snapshot.screen_evals_per_cycle,
            "promote_top_k": budget_snapshot.promote_top_k,
            "max_high_fidelity": budget_snapshot.max_high_fidelity_evals_per_cycle,
        },
    }
    path = _observability_log_path(cfg.reporting_dir)
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("a", encoding="utf-8") as handle:
        handle.write(json.dumps(entry, separators=(",", ":")) + "\n")


def _run_cycle(
    cfg: ai_config.ExperimentConfig,
    cycle_index: int,
    world_model: memory.WorldModel,
    experiment_id: int,
    governance_stage: str,
    git_sha: str,
    constellaration_sha: str,
    *,
    runtime: RunnerCLIConfig | None = None,
    budget_controller: BudgetController,
    prev_feasibility_rate: float | None = None,
) -> tuple[Path | None, dict[str, Any] | None, tools.P3Summary | None]:
    tool_name = _problem_tool_name(cfg.problem)
    base_evaluate = _problem_evaluator(cfg.problem)
    evaluate_fn = adapter.with_peft(base_evaluate, tool_name=tool_name)
    cycle_start = time.perf_counter()
    cycle_number = cycle_index + 1
    global _LAST_SURROGATE_FIT_SEC
    _LAST_SURROGATE_FIT_SEC = 0.0
    sleep_per_eval = 0.05 if runtime and runtime.slow else 0.0
    screen_only = bool(runtime and runtime.screen_only)
    budget_snapshot = budget_controller.snapshot()
    active_budgets = replace(
        cfg.budgets,
        screen_evals_per_cycle=budget_snapshot.screen_evals_per_cycle,
        promote_top_k=budget_snapshot.promote_top_k,
        max_high_fidelity_evals_per_cycle=budget_snapshot.max_high_fidelity_evals_per_cycle,
    )
    if cfg.adaptive_budgets.enabled and runtime and runtime.verbose:
        print(
            f"[runner][budget] cycle={cycle_number} override "
            f"screen={budget_snapshot.screen_evals_per_cycle} "
            f"promote_top_k={budget_snapshot.promote_top_k} "
            f"max_high_fidelity={budget_snapshot.max_high_fidelity_evals_per_cycle}"
        )
    cache_hit_rate = 0.0
    pool_size = _surrogate_candidate_pool_size(
        budget_snapshot.screen_evals_per_cycle,
        cfg.proposal_mix.surrogate_pool_multiplier,
    )
    sampler_count = 0
    random_count = 0
    if pool_size <= 0:
        if runtime and runtime.verbose:
            print(
                f"[runner][cycle={cycle_number}] screen budget zero; skipping candidate generation"
            )
        candidate_pool: list[Mapping[str, Any]] = []
        candidates: list[Mapping[str, Any]] = []
    else:
        candidate_pool, sampler_count, random_count = _propose_p3_candidates_for_cycle(
            cfg,
            cycle_index,
            world_model,
            experiment_id,
            screen_budget=budget_snapshot.screen_evals_per_cycle,
            total_candidates=pool_size,
            prev_feasibility_rate=prev_feasibility_rate,
        )
        if runtime and runtime.verbose:
            print(
                f"[runner][cycle={cycle_number}] candidate mix (pool={len(candidate_pool)}): sampler={sampler_count} random={random_count}"
            )
        candidates = _surrogate_rank_screen_candidates(
            cfg,
            budget_snapshot.screen_evals_per_cycle,
            candidate_pool,
            world_model,
            cycle=cycle_number,
            verbose=bool(runtime and runtime.verbose),
        )

    screen_stage = cfg.fidelity_ladder.screen
    if candidates:
        screen_results = _evaluate_stage(
            candidates,
            stage=screen_stage,
            budgets=active_budgets,
            cycle_start=cycle_start,
            evaluate_fn=evaluate_fn,
            sleep_per_eval=sleep_per_eval,
        )
    else:
        screen_results = []
    screen_cache_stats = tools.get_cache_stats(screen_stage)
    cache_hit_rate = budget_controller.capture_cache_hit_rate(
        screen_stage, stats=screen_cache_stats
    )
    _maybe_log_cache_stats(runtime, cfg, cycle_index, screen_stage, screen_cache_stats)

    screen_design_map = _latest_evaluations_by_design(screen_results, screen_stage)
    screen_summary = tools.summarize_p3_candidates(
        list(screen_design_map.values()), reference_point=P3_REFERENCE_POINT
    )
    allow_promote = (
        screen_summary.feasible_count >= cfg.governance.min_feasible_for_promotion
    )
    promote_limit = 0
    to_promote: list[Mapping[str, Any]] = []
    if allow_promote and screen_design_map:
        prioritized_screen = _rank_candidates_for_promotion(
            screen_design_map, active_budgets.promote_top_k, P3_REFERENCE_POINT
        )
        promote_limit = min(
            active_budgets.promote_top_k,
            active_budgets.max_high_fidelity_evals_per_cycle,
            len(screen_design_map),
        )
        to_promote = prioritized_screen[:promote_limit]
    elif not allow_promote and screen_design_map and runtime and runtime.verbose:
        print(
            "[runner][stage-gate] insufficient feasible screen results "
            f"({screen_summary.feasible_count} < {cfg.governance.min_feasible_for_promotion}); skipping promotions"
        )

    promote_stage = cfg.fidelity_ladder.promote
    promote_results: list[dict[str, Any]] = []
    if not screen_only and promote_limit > 0:
        promote_results = _evaluate_stage(
            to_promote,
            stage=promote_stage,
            budgets=active_budgets,
            cycle_start=cycle_start,
            evaluate_fn=evaluate_fn,
            sleep_per_eval=sleep_per_eval,
        )
        promote_cache_stats = tools.get_cache_stats(promote_stage)
        _maybe_log_cache_stats(
            runtime, cfg, cycle_index, promote_stage, promote_cache_stats
        )
    elif screen_only:
        print("[runner] screen-only flag active; skipping promotion evaluations.")

    aggregated = screen_results + promote_results
    if not aggregated:
        world_model.record_stage_history(
            experiment_id=experiment_id,
            cycle=cycle_number,
            stage=governance_stage,
        )
        return None, None, None

    latest_by_design = _latest_evaluations_by_design(
        aggregated, cfg.fidelity_ladder.promote
    )
    if not latest_by_design:
        world_model.record_stage_history(
            experiment_id=experiment_id,
            cycle=cycle_number,
            stage=governance_stage,
        )
        return None, None, None

    p3_summary = tools.summarize_p3_candidates(
        list(latest_by_design.values()), reference_point=P3_REFERENCE_POINT
    )
    total_designs = len(latest_by_design)
    feasibility_rate = float(p3_summary.feasible_count) / float(max(1, total_designs))
    vmec_failure_rate = _vmec_failure_rate(aggregated)
    hv_display = (
        f"{p3_summary.hv_score:.6f}" if p3_summary.feasible_count > 0 else "n/a"
    )
    print(
        f"[runner][cycle={cycle_number}] feasible={p3_summary.feasible_count}/{total_designs} hv={hv_display}"
    )
    _log_observability_metrics(
        cfg,
        cycle_index,
        hv=p3_summary.hv_score,
        feasible_count=p3_summary.feasible_count,
        vmec_failure_rate=vmec_failure_rate,
        retrain_time=_LAST_SURROGATE_FIT_SEC,
        cache_hit_rate=cache_hit_rate,
        budget_snapshot=budget_snapshot,
    )
    
    if cfg.reporting.get("prometheus_export_enabled", False):
        prom_path = Path(cfg.reporting_dir) / "metrics.prom"
        reporting.export_metrics_to_prometheus_textfile(
            {
                "cycle_hv": p3_summary.hv_score or 0.0,
                "feasible_count": p3_summary.feasible_count,
                "vmec_failure_rate": vmec_failure_rate,
                "surrogate_retrain_seconds": _LAST_SURROGATE_FIT_SEC,
                "cache_hit_rate": cache_hit_rate,
                "cycle": cycle_number,
            },
            prom_path,
        )

    p3_summary_path = adaptation_helpers.write_p3_summary(
        base_dir=cfg.reporting_dir,
        cycle=cycle_number,
        summary=p3_summary,
    )

    best_entry = min(latest_by_design.values(), key=_oriented_objective)
    best_eval = dict(best_entry["evaluation"])
    metrics_payload = best_eval.setdefault("metrics", {})
    metrics_payload["cycle_hv"] = p3_summary.hv_score
    best_eval["cycle_hv"] = p3_summary.hv_score
    best_eval["design_hash"] = best_entry.get("design_hash", "")
    cycle_duration = time.perf_counter() - cycle_start
    best_seed = int(best_entry.get("seed", cfg.random_seed))
    previous_baseline = world_model.previous_best_hv(experiment_id, cycle_number)
    current_hv = float(p3_summary.hv_score)
    hv_delta = (
        current_hv - previous_baseline if previous_baseline is not None else current_hv
    )
    budget_controller.record_feedback(
        CycleBudgetFeedback(
            hv_delta=hv_delta,
            feasibility_rate=feasibility_rate,
            cache_hit_rate=cache_hit_rate,
        )
    )
    best_metrics_id: int | None = None
    logged_hashes: Set[str] = set()
    config_snapshot = dict(
        _serialize_experiment_config(cfg, constellaration_sha=constellaration_sha)
    )
    config_snapshot["cycle_seed"] = best_seed
    adapter_version = adapter.current_adapter_version(
        tool_name, cfg.fidelity_ladder.promote
    ) or adapter.current_adapter_version(tool_name, cfg.fidelity_ladder.screen)
    config_snapshot["adapter_version"] = adapter_version
    cycle_json = {
        "experiment_id": experiment_id,
        "git_sha": git_sha,
        "constellaration_sha": constellaration_sha,
        "cycle": cycle_number,
        "stage": governance_stage,
        "feasible_count": p3_summary.feasible_count,
        "hv": p3_summary.hv_score,
        "vmec_failure_rate": vmec_failure_rate,
        "surrogate_retrain_seconds": _LAST_SURROGATE_FIT_SEC,
        "reference_point": p3_summary.reference_point,
        "archive_size": p3_summary.archive_size,
        "screened": len(screen_results),
        "promoted": len(promote_results),
        "feasibility_rate": feasibility_rate,
        "adapter_version": adapter_version,
        "last_feedback": {
            "hv_delta": hv_delta,
            "feasibility_rate": feasibility_rate,
            "cache_hit_rate": cache_hit_rate,
        },
        "budget_controller": budget_controller.to_dict(),
    }
    cycle_json_path = Path(cfg.reporting_dir) / f"cycle_{cycle_number}.json"

    with world_model.transaction():
        world_model.record_cycle(
            experiment_id=experiment_id,
            cycle_number=cycle_number,
            screen_evals=len(screen_results),
            promoted_evals=len(promote_results),
            high_fidelity_evals=len(promote_results),
            wall_seconds=cycle_duration,
            best_params=best_entry["params"],
            best_evaluation=best_eval,
            seed=best_seed,
            log_best_candidate=False,
            problem=cfg.problem,
            commit=False,
        )
        world_model.record_cycle_summary(
            experiment_id=experiment_id,
            cycle_number=cycle_number,
            stage=governance_stage,
            feasible_count=p3_summary.feasible_count,
            hv_score=p3_summary.hv_score,
            commit=False,
        )
        world_model.record_cycle_hv(
            experiment_id=experiment_id,
            cycle_number=cycle_number,
            hv_score=p3_summary.hv_score,
            reference_point=p3_summary.reference_point,
            pareto_entries=[entry.as_mapping() for entry in p3_summary.pareto_entries],
            n_feasible=p3_summary.feasible_count,
            n_archive=p3_summary.archive_size,
            hv_lookback=cfg.governance.hv_lookback,
            commit=False,
        )
        logged_hashes, metrics_by_hash = _persist_pareto_archive(
            world_model=world_model,
            experiment_id=experiment_id,
            cycle_number=cycle_number,
            problem=cfg.problem,
            entries_by_design=latest_by_design,
            p3_summary=p3_summary,
            git_sha=git_sha,
            constellaration_sha=constellaration_sha,
        )
        if (
            best_entry.get("design_hash")
            and best_entry["design_hash"] not in logged_hashes
        ):
            _, metrics_id = world_model.log_candidate(
                experiment_id=experiment_id,
                problem=cfg.problem,
                params=best_entry["params"],
                seed=best_seed,
                status=best_eval.get("stage", "unknown"),
                evaluation=best_eval,
                design_hash=best_entry["design_hash"],
                commit=False,
            )
            best_metrics_id = metrics_id
        world_model.record_deterministic_snapshot(
            experiment_id=experiment_id,
            cycle_number=cycle_number,
            snapshot=config_snapshot,
            constellaration_sha=constellaration_sha,
            seed=best_seed,
            commit=False,
        )
    if best_metrics_id is None:
        best_metrics_id = metrics_by_hash.get(best_entry.get("design_hash", ""))
    cycle_json_path.parent.mkdir(parents=True, exist_ok=True)
    cycle_json_path.write_text(json.dumps(cycle_json, indent=2), encoding="utf-8")
    repro_command = (
        f"python -m ai_scientist.runner --config {cfg.source_config} --problem {cfg.problem} "
        f"--cycles {cfg.cycles} --eval-budget {active_budgets.screen_evals_per_cycle} "
        f"--workers {active_budgets.n_workers} --pool-type {active_budgets.pool_type}"
    )
    env_block = (
        f"- Python: {sys.version.splitlines()[0]}\n"
        f"- Platform: {platform.platform()}\n"
        f"- Executable: {sys.executable}\n"
        f"- Host: {platform.node()}\n"
        f"- CPU: {platform.processor() or 'unknown'} | cores: {os.cpu_count() or 'unknown'}\n"
    )
    pareto_entries = p3_summary.pareto_entries
    if pareto_entries:
        pareto_lines = "\n".join(
            f"- design {entry.design_hash[:8]} seed={entry.seed} stage={entry.stage}: "
            f"gradient={entry.gradient:.4f}, aspect={entry.aspect_ratio:.4f}, "
            f"feasibility={entry.feasibility:.4f}"
            for entry in pareto_entries
        )
    else:
        pareto_lines = "- none (pareto front empty)"
    if pareto_entries:
        replay_entry = pareto_entries[0]
        reproduction_snippet = (
            "```bash\n"
            "python - <<'PY'\n"
            "import json, sqlite3\n"
            "from ai_scientist import tools\n"
            f"conn = sqlite3.connect('{cfg.memory_db}')\n"
            "row = conn.execute(\n"
            '    "SELECT params_json FROM candidates WHERE design_hash = ? ORDER BY id DESC LIMIT 1",\n'
            f"    ('{replay_entry.design_hash}',),\n"
            ").fetchone()\n"
            "assert row, 'Design hash not found in world model'\n"
            "params = json.loads(row[0])\n"
            f"print(tools.{tool_name}(params, stage='{replay_entry.stage or cfg.fidelity_ladder.promote}'))\n"
            "PY\n"
            "```\n"
        )
    else:
        reproduction_snippet = (
            "No Pareto archive entries available to replay this cycle.\n"
        )
    stage_label = best_eval.get("stage") or cfg.fidelity_ladder.promote
    reproduction_steps = [
        repro_command,
        f"git checkout {git_sha}",
        f"(cd constellaration && git checkout {constellaration_sha})",
        f"tools.{tool_name}(params, stage='{stage_label}')",
    ]
    tool_input = {"params": best_entry["params"], "stage": stage_label}
    tool_input_hash = memory.hash_payload(tool_input)
    statement_status = _verify_best_claim(
        world_model=world_model,
        experiment_id=experiment_id,
        cycle_number=cycle_number,
        best_entry=best_entry,
        best_eval=best_eval,
        evaluation_fn=evaluate_fn,
        tool_name=tool_name,
        best_seed=best_seed,
        git_sha=git_sha,
        reproduction_command=repro_command,
        stage=stage_label,
        metrics_id=best_metrics_id,
    )
    preference_pairs_path = adaptation_helpers.append_preference_pair(
        base_dir=cfg.reporting_dir,
        cycle=cycle_number,
        pair={
            "stage": stage_label,
            "status": statement_status,
            "tool_name": tool_name,
            "tool_input_hash": tool_input_hash,
            "reproduction_command": repro_command,
            "metrics": best_eval.get("metrics", {}),
            "design_hash": best_entry.get("design_hash"),
            "problem": cfg.problem,
            "seed": best_seed,
        },
    )
    trajectory_path = adaptation_helpers.append_trajectory_entry(
        base_dir=cfg.reporting_dir,
        cycle=cycle_number,
        entry={
            "stage": stage_label,
            "seed": best_seed,
            "tool_name": tool_name,
            "tool_input_hash": tool_input_hash,
            "reproduction_steps": reproduction_steps,
            "reproduction_snippet": reproduction_snippet,
            "reproduction_command": repro_command,
            "params": best_entry["params"],
            "metrics": best_eval.get("metrics", {}),
            "problem": cfg.problem,
            "design_hash": best_entry.get("design_hash", ""),
        },
    )
    preference_pairs_anchor = _repo_relative(preference_pairs_path)
    p3_summary_anchor = _repo_relative(p3_summary_path)
    trajectory_anchor = _repo_relative(trajectory_path)
    baseline_display = (
        f"{previous_baseline:.6f}" if previous_baseline is not None else "n/a"
    )
    preference_pairs_display = preference_pairs_anchor or preference_pairs_path.name
    trajectory_display = trajectory_anchor or trajectory_path.name
    p3_summary_display = p3_summary_anchor or p3_summary_path.name
    hv_text = (
        f"Cycle {cycle_number} hypervolume {current_hv:.6f} vs baseline "
        f"{baseline_display} (delta {hv_delta:+.6f}) recorded in cycle_hv "
        f"and adaptation logs {preference_pairs_display} and {trajectory_display} "
        f"with summary {p3_summary_display} (docs/TASKS_CODEX_MINI.md:238; docs/MASTER_PLAN_AI_SCIENTIST.md:226-247)."
    )
    hv_tool_input = {
        "cycle": cycle_number,
        "stage": stage_label,
        "current_hv": current_hv,
        "baseline_hv": previous_baseline,
        "delta": hv_delta,
        "preference_pairs_anchor": preference_pairs_anchor,
        "p3_summary_anchor": p3_summary_anchor,
        "trajectory_anchor": trajectory_anchor,
    }
    world_model.log_statement(
        experiment_id=experiment_id,
        cycle=cycle_number,
        stage=stage_label,
        text=hv_text,
        status="PENDING",
        tool_name="hv_delta_comparison",
        tool_input=hv_tool_input,
        metrics_id=best_metrics_id,
        seed=best_seed,
        git_sha=git_sha,
        repro_cmd=repro_command,
    )
    statements = world_model.statements_for_cycle(experiment_id, cycle_number)
    figure_path = reporting.save_pareto_figure(
        p3_summary.pareto_entries,
        cfg.reporting_dir,
        title=cfg.problem,
        cycle_index=cycle_index,
    )
    figure_paths = [figure_path] if figure_path else []
    metrics_payload = best_eval.get("metrics", {})
    metrics_path = adaptation_helpers.write_metrics_snapshot(
        base_dir=cfg.reporting_dir,
        cycle=cycle_number,
        payload=metrics_payload,
    )
    artifact_entries: list[tuple[str, Path]] = [("metrics_snapshot", metrics_path)]
    world_model.log_artifact(
        experiment_id=experiment_id,
        path=metrics_path,
        kind="metrics_snapshot",
    )
    artifact_entries.append(("p3_summary", p3_summary_path))
    world_model.log_artifact(
        experiment_id=experiment_id,
        path=p3_summary_path,
        kind="p3_summary",
    )
    artifact_entries.append(("preference_pairs", preference_pairs_path))
    world_model.log_artifact(
        experiment_id=experiment_id,
        path=preference_pairs_path,
        kind="preference_pairs",
    )
    artifact_entries.append(("trajectory_entry", trajectory_path))
    world_model.log_artifact(
        experiment_id=experiment_id,
        path=trajectory_path,
        kind="trajectory_entry",
    )
    if figure_path:
        artifact_entries.append(("pareto_figure", figure_path))
        world_model.log_artifact(
            experiment_id=experiment_id,
            path=figure_path,
            kind="pareto_figure",
        )
    world_model.record_stage_history(
        experiment_id=experiment_id,
        cycle=cycle_number,
        stage=governance_stage,
    )
    stage_history_entries = world_model.stage_history(experiment_id)
    property_graph_summary = world_model.property_graph_summary(experiment_id)
    rag_citations = (
        property_graph_summary.get("citations") if property_graph_summary else None
    )
    adaptation_figures = reporting.collect_adaptation_figures(cfg.reporting_dir)
    anchor_candidates = (
        ("preference_pairs", preference_pairs_anchor),
        ("p3_summary", p3_summary_anchor),
        ("trajectory", trajectory_anchor),
    )
    positioning_artifacts = {
        name: anchor for name, anchor in anchor_candidates if anchor is not None
    }
    if not positioning_artifacts:
        positioning_artifacts = None
    references = [
        "docs/TASKS_CODEX_MINI.md:200-248",
        "docs/TASKS_CODEX_MINI.md:206-238",
        "docs/MASTER_PLAN_AI_SCIENTIST.md:247-368",
    ]
    for anchor in (preference_pairs_anchor, p3_summary_anchor, trajectory_anchor):
        if anchor:
            references.append(anchor)
    reproduction_steps = [
        repro_command,
        f"git checkout {git_sha}",
        f"(cd constellaration && git checkout {constellaration_sha})",
        f"tools.{tool_name}(params, stage='{stage_label}')",
    ]
    content = reporting.build_cycle_report(
        cycle_index=cycle_index,
        problem=cfg.problem,
        screened=len(screen_results),
        promoted=len(promote_results),
        governance_stage=governance_stage,
        best_metrics=best_eval["metrics"],
        config_snapshot=config_snapshot,
        reproduction_steps=reproduction_steps,
        reproduction_snippet=reproduction_snippet,
        environment_block=env_block,
        pareto_lines=pareto_lines,
        p3_summary={
            "hv_score": p3_summary.hv_score,
            "reference_point": p3_summary.reference_point,
            "feasible_count": p3_summary.feasible_count,
            "archive_size": p3_summary.archive_size,
        },
        statements=statements,
        references=references,
        positioning_artifacts=positioning_artifacts,
        stage_history=stage_history_entries,
        artifact_entries=artifact_entries,
        adaptation_figures=adaptation_figures,
        property_graph_summary=property_graph_summary,
        rag_citations=rag_citations,
        figure_paths=figure_paths,
        out_dir=cfg.reporting_dir,
    )

    title = f"{cfg.problem}_cycle_{cycle_index + 1}"
    report_path = reporting.write_report(title, content, out_dir=cfg.reporting_dir)
    return report_path, best_eval, p3_summary


def run(
    cfg: ai_config.ExperimentConfig, runtime: RunnerCLIConfig | None = None
) -> None:
    index_status = rag.ensure_index()
    runtime_label = (
        f"screen_only={runtime.screen_only} promote_only={runtime.promote_only} "
        f"log_cache_stats={runtime.log_cache_stats} slow={runtime.slow} "
        f"planner={runtime.planner} preset={runtime.run_preset or 'none'}"
        if runtime
        else "default"
    )
    print(
        f"[runner] RAG index ready: {index_status.chunks_indexed} chunks ({index_status.index_path}); runtime={runtime_label}"
    )
    tools.clear_evaluation_cache()
    planner_mode = (
        runtime.planner.lower() if runtime and runtime.planner else "deterministic"
    )
    budget_controller = BudgetController(cfg.budgets, cfg.adaptive_budgets)
    last_p3_summary: tools.P3Summary | None = None
    with memory.WorldModel(cfg.memory_db) as world_model:
        git_sha = _resolve_git_sha()
        constellaration_sha = _resolve_git_sha("constellaration")
        
        experiment_id: int
        start_cycle_index = 0
        stage_history: list[CycleSummary] = []
        governance_stage = "s1"

        if runtime and runtime.resume_from:
            resume_data = json.loads(runtime.resume_from.read_text(encoding="utf-8"))
            resume_cycle = int(resume_data["cycle"])
            # We expect experiment_id in the checkpoint (deterministic resume)
            # If missing (legacy checkpoint), we fail fast per feedback requirements
            if "experiment_id" not in resume_data:
                raise ValueError(f"Checkpoint {runtime.resume_from} missing experiment_id; cannot resume deterministically.")
            
            experiment_id = int(resume_data["experiment_id"])
            print(f"[runner] resuming experiment_id={experiment_id} from cycle {resume_cycle}")
            
            # Verify DB consistency
            if world_model.cycles_completed(experiment_id) < resume_cycle:
                 # This might happen if DB write failed but JSON wrote, or DB was deleted.
                 print(f"[runner] warning: DB has fewer cycles than checkpoint for exp {experiment_id}")
            
            start_cycle_index = resume_cycle
            
            # Restore history for governance transitions
            restored = world_model.cycle_summaries(experiment_id)
            for row in restored:
                stage_history.append(
                    CycleSummary(
                        cycle=row["cycle"],
                        objective=row["objective"],
                        feasibility=row["feasibility"],
                        hv=row["hv"],
                        stage=row["stage"],
                    )
                )
            
            # Determine governance stage for the NEXT cycle (start_cycle_index + 1)
            # We run transition checks on the full history up to resume_cycle
            last_stage = stage_history[-1].stage if stage_history else "s1"
            governance_stage = last_stage
            
            # Check transitions based on restored history
            if last_stage == "s1" and _should_transition_s1_to_s2(stage_history, cfg.stage_gates):
                governance_stage = "s2"
            elif last_stage == "s2" and _should_transition_s2_to_s3(
                stage_history, cfg.stage_gates, cfg.governance, world_model, experiment_id, resume_cycle, cfg.cycles
            ):
                governance_stage = "s3"
                
            print(f"[runner] resumed state: start_index={start_cycle_index} next_stage={governance_stage}")
            bc_state = resume_data.get("budget_controller")
            if bc_state:
                budget_controller.restore(bc_state)

        else:
            experiment_id = world_model.start_experiment(
                _serialize_experiment_config(cfg, constellaration_sha=constellaration_sha),
                git_sha,
                constellaration_sha=constellaration_sha,
            )
            if runtime and runtime.promote_only:
                governance_stage = "s2"
                print("[runner] promote-only flag engaged; starting governance in S2.")

        # Reset deterministic RNG baselines after resume/start so subsequent cycles match a fresh run.
        np.random.seed(cfg.random_seed + start_cycle_index)
        random.seed(cfg.random_seed + start_cycle_index)

        planning_agent = (
            ai_planner.PlanningAgent(world_model=world_model)
            if planner_mode == "agent"
            else None
        )
        last_best_objective: float | None = None
        if stage_history:
            # Seed last_best_objective so reward diffs remain monotonic after resume
            last_best_objective = next(
                (entry.objective for entry in reversed(stage_history) if entry.objective is not None),
                None,
            )
        
        last_feasibility_rate: float | None = None

        for idx in range(cfg.cycles):
            cycle_number = idx + 1
            if idx < start_cycle_index:
                continue

            print(
                f"[runner] starting cycle {cycle_number} stage={governance_stage.upper()} "
                f"screen_budget={cfg.budgets.screen_evals_per_cycle}"
            )
            if planning_agent:
                # We pass the *reconstructed* stage_history
                stage_payload = [
                    {
                        "cycle": entry.cycle,
                        "stage": entry.stage,
                        "selected_at": datetime.now(timezone.utc).isoformat(), # Approximation if not tracked precisely in summary
                    }
                    for entry in stage_history
                ]
                # Note: 'selected_at' in stage_history table exists, but CycleSummary doesn't store it. 
                # For planner context, accurate timestamp is less critical than the sequence.
                
                plan_outcome = planning_agent.plan_cycle(
                    cfg=cfg,
                    cycle_index=idx,
                    stage_history=stage_payload,
                    last_summary=last_p3_summary,
                    experiment_id=experiment_id,
                )
                context_snapshot = json.dumps(plan_outcome.context, indent=2)
                print(f"[planner][cycle={idx + 1}] context:\n{context_snapshot}")
            report_path, best_eval, p3_summary = _run_cycle(
                cfg,
                idx,
                world_model,
                experiment_id,
                governance_stage,
                git_sha,
                constellaration_sha,
                runtime=runtime,
                budget_controller=budget_controller,
                prev_feasibility_rate=last_feasibility_rate,
            )
            last_p3_summary = p3_summary
            if p3_summary:
                total = max(1, p3_summary.feasible_count + (p3_summary.archive_size or 0)) # approximate total if not tracked directly
                # Actually p3_summary doesn't store total candidates, but _run_cycle logs feasibility_rate to JSON.
                # We can infer it from the feasible_count if we knew the total. 
                # Wait, _run_cycle calculated feasibility_rate internally but didn't return it explicitly except in JSON.
                # But we can re-calculate if we assume pool size ~ budget. 
                # Better: _run_cycle *returns* p3_summary, but feasibility_rate was local. 
                # Let's check _run_cycle output. It returns (Path, dict, P3Summary).
                # The P3Summary has feasible_count. 
                # We can track feasibility rate if we modify _run_cycle to return it or just assume 0.5 for now? 
                # No, better to capture it. 
                # However, modifying return signature again is annoying.
                # Let's assume feasibility_rate ~ p3_summary.feasible_count / (screened + promoted) roughly?
                # Actually, best_eval has "feasibility_rate" in the JSON payload logic but not in the returned dict structure clearly.
                # Let's look at how `last_feedback` was constructed in _run_cycle: 
                # `feasibility_rate = float(p3_summary.feasible_count) / float(max(1, total_designs))`
                # `total_designs` came from `len(latest_by_design)`.
                # We can approximately reconstruct it or just let the decay be 0 for the first pass if not critical.
                # BUT, I can calculate it here if I assume total_designs is roughly comparable to what p3_summary holds.
                # `p3_summary` holds `pareto_entries` (tuple). 
                # The simplest way is to trust that if feasible_count > 0, rate > 0.
                # Let's check `p3_summary` fields.
                pass
            
            # To properly implement decay, I need the rate. 
            # I will check budget_controller._last_feedback since it WAS recorded in _run_cycle.
            if budget_controller._last_feedback and budget_controller._last_feedback.feasibility_rate is not None:
                last_feasibility_rate = budget_controller._last_feedback.feasibility_rate
            if report_path:
                print(f"[runner] cycle {idx + 1} report saved to {report_path}")
            else:
                print(f"[runner] cycle {idx + 1} aborted (wall-clock or budget).")
            summary = CycleSummary(
                cycle=idx + 1,
                objective=best_eval.get("objective") if best_eval else None,
                feasibility=best_eval.get("feasibility") if best_eval else None,
                hv=best_eval.get("cycle_hv") if best_eval else None,
                stage=governance_stage,
            )
            stage_history.append(summary)
            if best_eval:
                current_objective = best_eval.get("objective")
                reward_diff = 0.0
                if current_objective is not None and last_best_objective is not None:
                    reward_diff = float(current_objective) - float(last_best_objective)
                adaptation_helpers.append_preference_record(
                    base_dir=cfg.reporting_dir,
                    record={
                        "cycle": idx + 1,
                        "stage": governance_stage,
                        "candidate_hash": best_eval.get("design_hash", "") or "",
                        "reward_diff": reward_diff,
                    },
                )
                if current_objective is not None:
                    last_best_objective = float(current_objective)
            
            next_stage = governance_stage
            if governance_stage == "s1":
                if _should_transition_s1_to_s2(stage_history, cfg.stage_gates):
                    next_stage = "s2"
                    print(
                        f"[runner][stage-gate] governance stage advanced to S2 after cycle {idx + 1}"
                    )
            elif governance_stage == "s2":
                if _should_transition_s2_to_s3(
                    stage_history,
                    cfg.stage_gates,
                    cfg.governance,
                    world_model,
                    experiment_id,
                    idx + 1,
                    cfg.cycles,
                ):
                    next_stage = "s3"
                    print(
                        f"[runner][stage-gate] governance stage advanced to S3 after cycle {idx + 1}"
                    )
            governance_stage = next_stage
        batch_summary_path = _export_batch_reports(cfg.reporting_dir, stage_history)
        world_model.log_artifact(
            experiment_id=experiment_id,
            path=batch_summary_path,
            kind="batch_summary",
        )
        usage = world_model.budget_usage(experiment_id)
        print(
            f"[runner] logged {usage.screen_evals} screen + {usage.promoted_evals} promote evaluations ("
            f"{usage.high_fidelity_evals} high-fidelity) into {cfg.memory_db}",
        )


def _serialize_experiment_config(
    cfg: ai_config.ExperimentConfig, constellaration_sha: str | None = None
) -> dict[str, Any]:
    boundary_template = asdict(cfg.boundary_template)
    seed_path = boundary_template.get("seed_path")
    if seed_path is not None:
        boundary_template["seed_path"] = str(seed_path)
    return {
        "problem": cfg.problem,
        "cycles": cfg.cycles,
        "random_seed": cfg.random_seed,
        "budgets": asdict(cfg.budgets),
        "adaptive_budgets": asdict(cfg.adaptive_budgets),
        "proposal_mix": asdict(cfg.proposal_mix),
        "fidelity_ladder": asdict(cfg.fidelity_ladder),
        "boundary_template": boundary_template,
        "stage_gates": asdict(cfg.stage_gates),
        "governance": asdict(cfg.governance),
        "source_config": str(cfg.source_config),
        "reporting_dir": str(cfg.reporting_dir),
        "memory_db": str(cfg.memory_db),
        "constellaration_sha": constellaration_sha or "unknown",
    }


_RUN_PRESETS_PATH = Path("configs/run_presets.yaml")


def _load_run_presets(path: Path | str | None = None) -> dict[str, dict[str, bool]]:
    target = Path(path or _RUN_PRESETS_PATH)
    if not target.exists():
        return {}
    raw = yaml.safe_load(target.read_text(encoding="utf-8")) or {}
    presets: dict[str, dict[str, bool]] = {}
    for key, values in raw.items():
        if not isinstance(values, dict):
            continue
        presets[key] = {
            "screen_only": bool(values.get("screen_only", False)),
            "promote_only": bool(values.get("promote_only", False)),
            "slow": bool(values.get("slow", False)),
        }
    return presets


def _apply_run_preset(cli: RunnerCLIConfig) -> RunnerCLIConfig:
    preset_name = cli.run_preset or os.getenv("AI_SCIENTIST_RUN_PRESET")
    if not preset_name:
        return cli
    presets = _load_run_presets()
    preset = presets.get(preset_name)
    if preset is None:
        raise ValueError(
            "Unknown run preset '%s'; available presets are %s."
            % (preset_name, ", ".join(sorted(presets or ["<none>"])))
        )
    return replace(
        cli,
        screen_only=cli.screen_only or preset["screen_only"],
        promote_only=cli.promote_only or preset["promote_only"],
        slow=cli.slow or preset["slow"],
    )


def _export_batch_reports(
    report_dir: Path | str, history: Sequence[CycleSummary]
) -> Path:
    base_path = Path(report_dir)
    figures_dir = base_path / "figures"
    stage_dir = figures_dir / "batch_stage_summaries"
    figures_dir.mkdir(parents=True, exist_ok=True)
    stage_dir.mkdir(parents=True, exist_ok=True)
    stage_entries: dict[str, list[CycleSummary]] = {}
    for cycle_summary in history:
        stage_entries.setdefault(cycle_summary.stage, []).append(cycle_summary)
    stage_refs: dict[str, dict[str, Any]] = {}
    for stage, entries in stage_entries.items():
        objectives = [
            entry.objective for entry in entries if entry.objective is not None
        ]
        feasibilities = [
            entry.feasibility for entry in entries if entry.feasibility is not None
        ]
        hv_values = [entry.hv for entry in entries if entry.hv is not None]
        stage_payload = {
            "stage": stage,
            "cycles": len(entries),
            "best_objective": max(objectives) if objectives else None,
            "best_feasibility": min(feasibilities) if feasibilities else None,
            "max_hv": max(hv_values) if hv_values else None,
            "entries": [
                {
                    "cycle": entry.cycle,
                    "objective": entry.objective,
                    "feasibility": entry.feasibility,
                    "hv": entry.hv,
                }
                for entry in entries
            ],
        }
        stage_path = stage_dir / f"{stage}_summary.json"
        stage_path.write_text(json.dumps(stage_payload, indent=2), encoding="utf-8")
        stage_refs[stage] = {
            "cycles": len(entries),
            "path": str(stage_path.resolve()),
        }
    summary_payload = {
        "generated_at": datetime.now(timezone.utc).isoformat(),
        "total_cycles": len(history),
        "stage_files": stage_refs,
    }
    summary_path = figures_dir / "batch_summary.json"
    summary_path.write_text(json.dumps(summary_payload, indent=2), encoding="utf-8")
    return summary_path


def _resolve_git_sha(repo_path: str | None = None) -> str:
    try:
        completed = subprocess.run(
            ["git", "-C", repo_path, "rev-parse", "HEAD"]
            if repo_path
            else ["git", "rev-parse", "HEAD"],
            check=True,
            capture_output=True,
            text=True,
        )
        return completed.stdout.strip()
    except (subprocess.SubprocessError, FileNotFoundError):
        return "unknown"


def main() -> None:
    try:
        cli = _apply_run_preset(parse_args())
        _validate_runtime_flags(cli)
    except ValueError as exc:
        print(f"[runner] invalid CLI flags: {exc}", file=sys.stderr)
        raise SystemExit(2) from exc
    experiment = ai_config.load_experiment_config(cli.config_path)
    if cli.problem:
        experiment = replace(experiment, problem=cli.problem)
    if cli.cycles:
        experiment = replace(experiment, cycles=cli.cycles)
    if cli.memory_db:
        experiment = replace(experiment, memory_db=cli.memory_db)
    if cli.eval_budget is not None:
        experiment = replace(
            experiment,
            budgets=replace(experiment.budgets, screen_evals_per_cycle=cli.eval_budget),
        )
    if cli.workers is not None:
        experiment = replace(
            experiment,
            budgets=replace(experiment.budgets, n_workers=cli.workers),
        )
    if cli.pool_type is not None:
        experiment = replace(
            experiment,
            budgets=replace(experiment.budgets, pool_type=cli.pool_type),
        )
    if cli.slow:
        experiment = replace(
            experiment,
            budgets=replace(
                experiment.budgets,
                wall_clock_minutes=experiment.budgets.wall_clock_minutes * 1.5,
            ),
        )
    preset_label = cli.run_preset or os.getenv("AI_SCIENTIST_RUN_PRESET") or "none"
    print(
        f"[runner] starting problem={experiment.problem} cycles={experiment.cycles} "
        f"screen_budget={experiment.budgets.screen_evals_per_cycle} "
        f"screen_only={cli.screen_only} promote_only={cli.promote_only} "
        f"log_cache_stats={cli.log_cache_stats} slow={cli.slow} preset={preset_label}"
    )
    run(experiment, runtime=cli)


if __name__ == "__main__":
    main()


================================================================================
File: tools_api.py
================================================================================

"""OpenAI-style tool catalog for K2 models (Wave 8 checklist: docs/TASKS_CODEX_MINI.md:157-190)."""

from __future__ import annotations

from typing import Any, Mapping, Sequence

ToolSchema = Mapping[str, Any]

_TOOL_DEFINITIONS: Sequence[ToolSchema] = (
    {
        "name": "make_boundary",
        "description": (
            "Build a SurfaceRZFourier boundary before submitting it to any evaluator. "
            "Input matches ai_scientist.tools.make_boundary_from_params and keeps r/z coefficients, symmetry, and nfp. "
            "Reference: docs/TASKS_CODEX_MINI.md:157-190."
        ),
        "parameters": {
            "type": "object",
            "properties": {
                "params": {
                    "type": "object",
                    "description": "Dictionary with r_cos/z_sin arrays, n_field_periods (aka nfp), and optional symmetry flag.",
                    "additionalProperties": True,
                }
            },
            "required": ["params"],
            "additionalProperties": False,
        },
    },
    {
        "name": "propose_boundary",
        "description": (
            "Perturb an existing boundary parameter set to generate a new candidate near the original. "
            "Useful for refining a promising seed."
        ),
        "parameters": {
            "type": "object",
            "properties": {
                "params": {
                    "type": "object",
                    "description": "Base boundary parameters to perturb.",
                },
                "perturbation_scale": {
                    "type": "number",
                    "description": "Standard deviation of the Gaussian noise (default 0.05).",
                },
                "seed": {
                    "type": "integer",
                    "description": "Optional random seed for reproducibility.",
                },
            },
            "required": ["params"],
            "additionalProperties": False,
        },
    },
    {
        "name": "evaluate_p1",
        "description": (
            "Evaluate the low-fidelity P1 problem (minimize max elongation). "
            "Caller must provide the same params dict used in make_boundary."
        ),
        "parameters": {
            "type": "object",
            "properties": {
                "params": {"type": "object", "description": "Surface coefficients."},
                "problem": {
                    "type": "string",
                    "enum": ["p1"],
                    "description": "Explicitly declare the target problem.",
                },
                "stage": {
                    "type": "string",
                    "description": "Screening stage (default 'screen').",
                },
            },
            "required": ["params"],
            "additionalProperties": False,
        },
    },
    {
        "name": "evaluate_p2",
        "description": (
            "Run the high-fidelity QI-focused P2 evaluation via forward_model(ConstellarationSettings.default_high_fidelity())."
        ),
        "parameters": {
            "type": "object",
            "properties": {
                "params": {"type": "object", "description": "Boundary spec."},
                "problem": {
                    "type": "string",
                    "enum": ["p2"],
                    "description": "Explicit problem identifier.",
                },
                "stage": {"type": "string", "description": "Call tag (default 'p2')."},
            },
            "required": ["params"],
            "additionalProperties": False,
        },
    },
    {
        "name": "evaluate_p3",
        "description": (
            "Run the P3 metrics (aspect ratio + gradient) with high-fidelity settings and return hv-ready metrics."
        ),
        "parameters": {
            "type": "object",
            "properties": {
                "params": {"type": "object", "description": "Boundary parameters."},
                "problem": {
                    "type": "string",
                    "enum": ["p3"],
                    "description": "Explicit problem identifier.",
                },
                "stage": {"type": "string", "description": "Stage tag (default 'p3')."},
            },
            "required": ["params"],
            "additionalProperties": False,
        },
    },
    {
        "name": "retrieve_rag",
        "description": (
            "Fetch K relevant chunks from the ai_scientist/rag_index.db knowledge index."
        ),
        "parameters": {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "Search query that guides retrieval.",
                },
                "k": {
                    "type": "integer",
                    "minimum": 1,
                    "description": "Number of snippets to return (default 3).",
                },
            },
            "required": ["query"],
            "additionalProperties": False,
        },
    },
    {
        "name": "write_note",
        "description": (
            "Write a literature note or analysis insight to the world model and reports/notes/."
        ),
        "parameters": {
            "type": "object",
            "properties": {
                "content": {
                    "type": "string",
                    "description": "Markdown content of the note.",
                },
                "filename": {
                    "type": "string",
                    "description": "Optional filename (e.g. 'analysis_cycle_1.md').",
                },
                "experiment_id": {
                    "type": "integer",
                    "description": "Optional experiment id for world-model persistence.",
                },
                "cycle": {
                    "type": "integer",
                    "description": "Optional cycle number for world-model persistence.",
                },
            },
            "required": ["content", "experiment_id", "cycle"],
            "additionalProperties": False,
        },
    },
    {
        "name": "log_citation",
        "description": (
            "Register a doc citation for the current report draft (anchor optional, but include repo path)."
        ),
        "parameters": {
            "type": "object",
            "properties": {
                "source_path": {
                    "type": "string",
                    "description": "File path, e.g. docs/MASTER_PLAN_AI_SCIENTIST.md",
                },
                "anchor": {
                    "type": "string",
                    "description": "Section or line anchor for the claim.",
                },
                "quote": {
                    "type": "string",
                    "description": "Textual quote or paraphrase.",
                },
            },
            "required": ["source_path", "quote"],
            "additionalProperties": False,
        },
    },
    {
        "name": "write_report",
        "description": (
            "Generate or extend a Markdown report with structured sections and references (docs/TASKS_CODEX_MINI.md:157-190)."
        ),
        "parameters": {
            "type": "object",
            "properties": {
                "title": {"type": "string", "description": "Document title."},
                "sections": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "heading": {"type": "string"},
                            "body": {"type": "string"},
                        },
                        "required": ["heading", "body"],
                        "additionalProperties": False,
                    },
                },
                "references": {
                    "type": "array",
                    "items": {"type": "string"},
                },
            },
            "required": ["title", "sections"],
            "additionalProperties": False,
        },
    },
)

TOOL_SCHEMA_BY_NAME = {schema["name"]: schema for schema in _TOOL_DEFINITIONS}


def list_tool_schemas() -> tuple[ToolSchema, ...]:
    """Return all declared OpenAI-style tool definitions."""

    return tuple(_TOOL_DEFINITIONS)


def get_tool_schema(name: str) -> ToolSchema | None:
    """Lookup a schema by tool name."""

    return TOOL_SCHEMA_BY_NAME.get(name)


================================================================================
File: config.py
================================================================================

"""Configuration helpers for the AI Scientist orchestration (Tasks 0.2 + B.*)."""

from __future__ import annotations

from dataclasses import dataclass, field
import os
from pathlib import Path
from typing import Any, Mapping, Tuple

import yaml

DEFAULT_EXPERIMENT_CONFIG_PATH = Path("configs/experiment.example.yaml")
DEFAULT_MODEL_CONFIG_PATH = Path("configs/model.yaml")
DEFAULT_MEMORY_DB_PATH = Path("reports/ai_scientist.sqlite")


def load(path: str | Path | None = None) -> dict[str, Any]:
    """Return the raw YAML mapping for a given configuration file."""

    target = Path(path) if path is not None else DEFAULT_EXPERIMENT_CONFIG_PATH
    with target.open("r", encoding="utf-8") as handle:
        return yaml.safe_load(handle) or {}


@dataclass(frozen=True)
class AgentGateConfig:
    model_alias: str
    allowed_tools: Tuple[str, ...]
    system_prompt: str | None
    provider_model: str | None


@dataclass(frozen=True)
class ProviderConfig:
    name: str
    base_url: str
    chat_path: str
    auth_env: str
    default_model: str | None
    extra_headers: Tuple[tuple[str, str], ...]


@dataclass(frozen=True)
class ModelConfig:
    base_url: str
    instruct_model: str
    thinking_model: str
    request_timeout_seconds: int
    rate_limit_per_minute: int
    context_length: int
    dtype: str
    tensor_parallel: int
    default_provider: str
    providers: Tuple["ProviderConfig", ...]
    agent_gates: Tuple[AgentGateConfig, ...]
    role_map: Mapping[str, str] = field(default_factory=dict)

    def get_provider(self, name: str | None = None) -> "ProviderConfig":
        alias = (name or self.default_provider).lower()
        for provider in self.providers:
            if provider.name.lower() == alias:
                return provider
        raise ValueError(f"model provider '{alias}' is not configured")


def _env_override(key: str, default: str) -> str:
    value = os.getenv(key)
    if value is None or value == "":
        return default
    return value


def load_model_config(path: str | Path | None = None) -> ModelConfig:
    payload = load(path or DEFAULT_MODEL_CONFIG_PATH)
    model_data = payload.get("model", {})
    agent_gates_payload = model_data.get("agent_gates") or {}
    agent_gates = tuple(
        _agent_gate_config_from_dict(alias, gate_data)
        for alias, gate_data in agent_gates_payload.items()
    )
    provider_data = model_data.get("providers") or {}
    providers = tuple(
        _provider_config_from_dict(name, data) for name, data in provider_data.items()
    )
    default_provider = str(
        model_data.get("provider", providers[0].name if providers else "openrouter")
    )
    default_provider = _env_override("MODEL_PROVIDER", default_provider)
    instruct_alias = str(model_data.get("instruct_model", "kimi-k2-instruct"))
    instruct_alias = _env_override("AI_SCIENTIST_INSTRUCT_MODEL", instruct_alias)
    thinking_alias = str(model_data.get("thinking_model", "kimi-k2-thinking"))
    thinking_alias = _env_override("AI_SCIENTIST_THINKING_MODEL", thinking_alias)
    role_map = model_data.get("role_map") or {}
    if not isinstance(role_map, dict):
        role_map = {}
    return ModelConfig(
        base_url=str(model_data.get("base_url", "http://localhost:8000")),
        instruct_model=instruct_alias,
        thinking_model=thinking_alias,
        request_timeout_seconds=int(model_data.get("request_timeout_seconds", 60)),
        rate_limit_per_minute=int(model_data.get("rate_limit_per_minute", 600)),
        context_length=int(model_data.get("context_length", 8192)),
        dtype=str(model_data.get("dtype", "bf16")),
        tensor_parallel=int(model_data.get("tensor_parallel", 1)),
        default_provider=default_provider,
        providers=providers,
        agent_gates=agent_gates,
        role_map=role_map,
    )


@dataclass(frozen=True)
class BudgetConfig:
    screen_evals_per_cycle: int
    promote_top_k: int
    max_high_fidelity_evals_per_cycle: int
    wall_clock_minutes: float
    n_workers: int
    pool_type: str


@dataclass(frozen=True)
class BudgetRangeConfig:
    min: int
    max: int


@dataclass(frozen=True)
class AdaptiveBudgetConfig:
    enabled: bool
    hv_slope_reference: float
    feasibility_target: float
    cache_hit_target: float
    screen_bounds: BudgetRangeConfig
    promote_top_k_bounds: BudgetRangeConfig
    high_fidelity_bounds: BudgetRangeConfig


@dataclass(frozen=True)
class FidelityLadder:
    screen: str
    promote: str


@dataclass(frozen=True)
class BoundaryTemplateConfig:
    n_poloidal_modes: int
    n_toroidal_modes: int
    n_field_periods: int
    base_major_radius: float
    base_minor_radius: float
    perturbation_scale: float
    seed_path: Path | None = None


@dataclass(frozen=True)
class StageGateConfig:
    s1_to_s2_feasibility_margin: float
    s1_to_s2_objective_improvement: float
    s1_to_s2_lookback_cycles: int
    s2_to_s3_hv_delta: float
    s2_to_s3_lookback_cycles: int


@dataclass(frozen=True)
class GovernanceConfig:
    min_feasible_for_promotion: int
    hv_lookback: int


@dataclass(frozen=True)
class ProposalMixConfig:
    constraint_ratio: float
    exploration_ratio: float
    jitter_scale: float
    surrogate_pool_multiplier: float = 2.0
    sampler_type: str = "standard"


@dataclass(frozen=True)
class ExperimentConfig:
    problem: str
    cycles: int
    random_seed: int
    budgets: BudgetConfig
    adaptive_budgets: AdaptiveBudgetConfig
    fidelity_ladder: FidelityLadder
    boundary_template: BoundaryTemplateConfig
    stage_gates: StageGateConfig
    governance: GovernanceConfig
    proposal_mix: ProposalMixConfig
    reporting_dir: Path
    memory_db: Path
    source_config: Path
    reporting: Mapping[str, Any] = field(default_factory=dict)


def _boundary_template_from_dict(
    data: Mapping[str, Any] | None,
) -> BoundaryTemplateConfig:
    config = data or {}
    seed_path = config.get("seed_path")
    return BoundaryTemplateConfig(
        n_poloidal_modes=int(config.get("n_poloidal_modes", 3)),
        n_toroidal_modes=int(config.get("n_toroidal_modes", 5)),
        n_field_periods=int(config.get("n_field_periods", 1)),
        base_major_radius=float(config.get("base_major_radius", 1.5)),
        base_minor_radius=float(config.get("base_minor_radius", 0.5)),
        perturbation_scale=float(config.get("perturbation_scale", 0.05)),
        seed_path=Path(seed_path) if seed_path else None,
    )


def _stage_gate_config_from_dict(
    data: Mapping[str, Any] | None,
) -> StageGateConfig:
    config = data or {}
    return StageGateConfig(
        s1_to_s2_feasibility_margin=float(
            config.get("s1_to_s2_feasibility_margin", 0.01)
        ),
        s1_to_s2_objective_improvement=float(
            config.get("s1_to_s2_objective_improvement", 0.02)
        ),
        s1_to_s2_lookback_cycles=int(config.get("s1_to_s2_lookback_cycles", 3)),
        s2_to_s3_hv_delta=float(config.get("s2_to_s3_hv_delta", 0.01)),
        s2_to_s3_lookback_cycles=int(config.get("s2_to_s3_lookback_cycles", 3)),
    )


def _governance_config_from_dict(
    data: Mapping[str, Any] | None,
    *,
    default_hv_lookback: int,
) -> GovernanceConfig:
    config = data or {}
    min_feasible = int(config.get("min_feasible_for_promotion", 1))
    min_feasible = max(0, min_feasible)
    hv_lookback = int(config.get("hv_lookback", default_hv_lookback))
    hv_lookback = max(1, hv_lookback)
    return GovernanceConfig(
        min_feasible_for_promotion=min_feasible,
        hv_lookback=hv_lookback,
    )


def _agent_gate_config_from_dict(
    alias: str, data: Mapping[str, Any] | None
) -> AgentGateConfig:
    config = data or {}
    allowed = tuple(str(item) for item in config.get("allowed_tools", []))
    prompt = config.get("system_prompt")
    provider_model = config.get("provider_model")
    return AgentGateConfig(
        model_alias=str(alias),
        allowed_tools=allowed,
        system_prompt=str(prompt) if prompt is not None else None,
        provider_model=(
            str(provider_model) if provider_model is not None else None
        ),
    )


def _extra_headers_from_dict(
    data: Mapping[str, Any] | None,
) -> Tuple[tuple[str, str], ...]:
    config = data or {}
    return tuple((str(key), str(value)) for key, value in config.items())


def _provider_config_from_dict(
    name: str, data: Mapping[str, Any] | None
) -> ProviderConfig:
    config = data or {}
    return ProviderConfig(
        name=str(name),
        base_url=str(config.get("base_url", "")),
        chat_path=str(config.get("chat_path", "/v1/chat/completions")),
        auth_env=str(config.get("auth_env", "")),
        default_model=(
            str(config.get("default_model"))
            if config.get("default_model") is not None
            else None
        ),
        extra_headers=_extra_headers_from_dict(config.get("extra_headers")),
    )


def _proposal_mix_from_dict(
    data: Mapping[str, Any] | None,
) -> ProposalMixConfig:
    config = data or {}
    constraint_ratio = float(config.get("constraint_ratio", 0.7))
    exploration_ratio = float(config.get("exploration_ratio", 0.3))
    return ProposalMixConfig(
        constraint_ratio=constraint_ratio,
        exploration_ratio=exploration_ratio,
        jitter_scale=float(config.get("jitter_scale", 0.01)),
        surrogate_pool_multiplier=float(config.get("surrogate_pool_multiplier", 2.0)),
        sampler_type=str(config.get("sampler_type", "standard")),
    )


def _budget_range_from_dict(
    data: Mapping[str, Any] | None,
    *,
    default_value: int,
) -> BudgetRangeConfig:
    config = data or {}
    min_val = int(config.get("min", default_value))
    max_val = int(config.get("max", default_value))
    if min_val > max_val:
        min_val, max_val = max_val, min_val
    min_val = max(0, min_val)
    max_val = max(max_val, min_val)
    return BudgetRangeConfig(min=min_val, max=max_val)


def _adaptive_budget_config_from_dict(
    data: Mapping[str, Any] | None,
    *,
    base_budgets: BudgetConfig,
) -> AdaptiveBudgetConfig:
    config = data or {}
    enabled = bool(config.get("enabled", False))
    hv_reference = float(config.get("hv_slope_reference", 0.05))
    feasibility_target = float(config.get("feasibility_target", 0.5))
    cache_hit_target = float(config.get("cache_hit_target", 0.3))
    screen_bounds = _budget_range_from_dict(
        config.get("screen_evals_per_cycle"),
        default_value=base_budgets.screen_evals_per_cycle,
    )
    promote_bounds = _budget_range_from_dict(
        config.get("promote_top_k"),
        default_value=base_budgets.promote_top_k,
    )
    high_fidelity_bounds = _budget_range_from_dict(
        config.get("max_high_fidelity_evals_per_cycle"),
        default_value=base_budgets.max_high_fidelity_evals_per_cycle,
    )
    return AdaptiveBudgetConfig(
        enabled=enabled,
        hv_slope_reference=max(1e-6, hv_reference),
        feasibility_target=max(1e-6, feasibility_target),
        cache_hit_target=max(1e-6, cache_hit_target),
        screen_bounds=screen_bounds,
        promote_top_k_bounds=promote_bounds,
        high_fidelity_bounds=high_fidelity_bounds,
    )


def load_experiment_config(path: str | Path | None = None) -> ExperimentConfig:
    config_path = Path(path) if path is not None else DEFAULT_EXPERIMENT_CONFIG_PATH
    payload = load(config_path)
    budgets = payload.get("budgets", {})
    fidelity = payload.get("fidelity_ladder", {})
    stage_gates = _stage_gate_config_from_dict(payload.get("stage_gates"))
    governance = _governance_config_from_dict(
        payload.get("governance"),
        default_hv_lookback=stage_gates.s2_to_s3_lookback_cycles,
    )
    budget_config = BudgetConfig(
        screen_evals_per_cycle=int(budgets.get("screen_evals_per_cycle", 1)),
        promote_top_k=int(budgets.get("promote_top_k", 1)),
        max_high_fidelity_evals_per_cycle=int(
            budgets.get("max_high_fidelity_evals_per_cycle", 1)
        ),
        wall_clock_minutes=float(budgets.get("wall_clock_minutes", 5.0)),
        n_workers=int(budgets.get("n_workers", 1)),
        pool_type=str(budgets.get("pool_type", "process")),
    )
    return ExperimentConfig(
        problem=str(payload.get("problem", "p1")),
        cycles=int(payload.get("cycles", 1)),
        random_seed=int(payload.get("random_seed", 0)),
        budgets=budget_config,
        adaptive_budgets=_adaptive_budget_config_from_dict(
            payload.get("adaptive_budgets"),
            base_budgets=budget_config,
        ),
        fidelity_ladder=FidelityLadder(
            screen=str(fidelity.get("screen", "screen")),
            promote=str(fidelity.get("promote", "promote")),
        ),
        boundary_template=_boundary_template_from_dict(
            payload.get("boundary_template")
        ),
        stage_gates=stage_gates,
        governance=governance,
        proposal_mix=_proposal_mix_from_dict(payload.get("proposal_mix")),
        reporting_dir=Path(payload.get("reporting_dir", "reports")),
        memory_db=Path(payload.get("memory_db", DEFAULT_MEMORY_DB_PATH)),
        source_config=config_path,
        reporting=payload.get("reporting", {}),
    )


================================================================================
File: guards.py
================================================================================

"""Repository guardrails for the AI Scientist stack."""

from __future__ import annotations

from pathlib import Path
from typing import Sequence


REQUIRED_PATHS: Sequence[Path] = (
    Path("AGENTS.md"),
    Path("docs/TASKS_CODEX_MINI.md"),
    Path("docs/MASTER_PLAN_AI_SCIENTIST.md"),
    Path("configs/model.yaml"),
    Path("ai_scientist/__init__.py"),
    Path("ai_scientist/runner.py"),
)


class GuardViolation(Exception):
    """Raised when a repository guardrail is violated."""


def verify() -> None:
    """Ensure the repository satisfies the declared guardrails."""

    missing = [str(path) for path in REQUIRED_PATHS if not path.exists()]
    if missing:
        raise GuardViolation(f"missing required paths: {', '.join(missing)}")

    tasks = Path("docs/TASKS_CODEX_MINI.md").read_text()
    if "Task 0.3" not in tasks:
        raise GuardViolation("docs/TASKS_CODEX_MINI.md must mention Task 0.3")

    master_plan = Path("docs/MASTER_PLAN_AI_SCIENTIST.md").read_text()
    if "Score-hacking guardrails" not in master_plan:
        raise GuardViolation(
            "MASTER_PLAN_AI_SCIENTIST.md must mention score guardrails"
        )


================================================================================
File: model_endpoint.py
================================================================================

"""Minimal OpenAI-style K2 endpoint to satisfy Phase 1 model-serving (docs/MASTER_PLAN_AI_SCIENTIST.md:99-110, docs/TASKS_CODEX_MINI.md:247-368)."""

from __future__ import annotations

import json
import logging
import threading
from contextlib import contextmanager
from dataclasses import dataclass
from http.server import BaseHTTPRequestHandler
from socketserver import TCPServer, ThreadingMixIn
from typing import Iterator
from urllib.parse import ParseResult, urlparse, urlunparse

from ai_scientist.config import ModelConfig, load_model_config

_LOGGER = logging.getLogger(__name__)


@dataclass(frozen=True)
class EndpointMetadata:
    """Metadata that mirrors the Phase 1 serving config."""

    context_length: int
    dtype: str
    tensor_parallel: int


@dataclass(frozen=True)
class ModelEndpoint:
    """Information about the reachable K2 endpoint."""

    url: str
    metadata: EndpointMetadata
    provider_name: str
    chat_path: str


class _ThreadedServer(ThreadingMixIn, TCPServer):
    """TCP server that handles requests in threads and reuses its address."""

    allow_reuse_address = True


class _Handler(BaseHTTPRequestHandler):
    metadata: EndpointMetadata
    model_alias: str
    scheme: str
    chat_path: str

    def log_message(
        self, *_: object, **__: object
    ) -> None:  # pragma: no cover - suppress noise
        return

    def do_GET(self) -> None:
        if self.path not in {"/health", "/healthz"}:
            self.send_error(404)
            return
        payload = {
            "status": "ready",
            "model": self.model_alias,
            "metadata": self.metadata.__dict__,
        }
        self._respond(200, payload)

    def do_POST(self) -> None:
        if self.path != self.chat_path:
            self.send_error(404)
            return
        length = int(self.headers.get("Content-Length", 0))
        body = self.rfile.read(length) if length else b"{}"
        payload = json.loads(body.decode("utf-8"))
        response = {
            "id": "mock-k2",
            "object": "chat.completion",
            "model": self.model_alias,
            "choices": [
                {
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": {
                            "type": "tool_call",
                            "tool": payload.get("tool_call", {}).get("name"),
                            "confirmation": "endpoint-ready",
                        },
                    },
                    "finish_reason": "function_call",
                    "function_call": payload.get("tool_call"),
                }
            ],
            "usage": {
                "context_length": self.metadata.context_length,
                "dtype": self.metadata.dtype,
                "tensor_parallel": self.metadata.tensor_parallel,
            },
        }
        self._respond(200, response)

    def _respond(self, status: int, payload: dict) -> None:
        data = json.dumps(payload, separators=(",", ":")).encode("utf-8")
        self.send_response(status)
        self.send_header("Content-Type", "application/json")
        self.send_header("Content-Length", str(len(data)))
        self.end_headers()
        self.wfile.write(data)


def _build_handler(
    metadata: EndpointMetadata, model_alias: str, scheme: str, chat_path: str
) -> type[_Handler]:
    handler = type(
        "_ModelHandler",
        (_Handler,),
        {
            "metadata": metadata,
            "model_alias": model_alias,
            "scheme": scheme,
            "chat_path": chat_path,
        },
    )
    return handler


def _normalize_base_url(base_url: str) -> ParseResult:
    parsed = urlparse(base_url)
    scheme = parsed.scheme or "http"
    netloc = parsed.netloc or parsed.path
    if not netloc:
        netloc = "127.0.0.1"
    return parsed._replace(
        scheme=scheme, netloc=netloc, path="", params="", query="", fragment=""
    )


@contextmanager
def run_model_endpoint(
    config: ModelConfig | None = None,
    provider_name: str | None = None,
) -> Iterator[ModelEndpoint]:
    """Start a lightweight K2 endpoint and yield its URL plus metadata."""

    resolved = config or load_model_config()
    provider = resolved.get_provider(provider_name)
    parsed = _normalize_base_url(resolved.base_url)
    host = parsed.hostname or "127.0.0.1"
    port = parsed.port or 0
    metadata = EndpointMetadata(
        context_length=resolved.context_length,
        dtype=resolved.dtype,
        tensor_parallel=resolved.tensor_parallel,
    )
    handler = _build_handler(
        metadata, resolved.instruct_model, parsed.scheme, provider.chat_path
    )
    with _ThreadedServer((host, port), handler) as server:
        thread = threading.Thread(target=server.serve_forever, daemon=True)
        thread.start()
        live_netloc = f"{server.server_address[0]}:{server.server_address[1]}"
        endpoint_url = urlunparse((parsed.scheme, live_netloc, "", "", "", ""))
        _LOGGER.info(
            "Launched K2 endpoint %s (context=%d, dtype=%s, tp=%d)",
            endpoint_url,
            metadata.context_length,
            metadata.dtype,
            metadata.tensor_parallel,
        )
        try:
            yield ModelEndpoint(
                url=endpoint_url,
                metadata=metadata,
                provider_name=provider.name,
                chat_path=provider.chat_path,
            )
        finally:
            server.shutdown()
            thread.join()


__all__ = ["EndpointMetadata", "ModelEndpoint", "run_model_endpoint"]


================================================================================
File: memory.py
================================================================================

"""SQLite-backed world model for AI Scientist budgeting + logging.

This is the authoritative world model implementation for the AI Scientist.
It fulfills the Unified Roadmap Workstream 6 / Autonomy Plan requirement
for a per-cycle PropertyGraph and the Kosmos-style structured world model
(statements, citations, candidates, cycles → graph snapshot). The legacy
boundary-only world_model.py has been removed.
"""

from __future__ import annotations

import hashlib
import json
import sqlite3
from contextlib import contextmanager
from dataclasses import dataclass, field
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Mapping, Sequence


@dataclass
class PropertyGraph:
    nodes: dict[str, Mapping[str, Any]] = field(default_factory=dict)
    edges: list[tuple[str, str, Mapping[str, Any]]] = field(default_factory=list)

    def add_node(self, node_id: str, **attrs: Any) -> None:
        self.nodes[node_id] = attrs

    def add_edge(self, src: str, dst: str, **attrs: Any) -> None:
        self.edges.append((src, dst, attrs))

    def has_node(self, node_id: str) -> bool:
        return node_id in self.nodes


@dataclass(frozen=True)
class StatementRecord:
    id: int
    experiment_id: int
    cycle: int
    stage: str
    text: str
    status: str
    metrics_id: int | None
    tool_name: str
    tool_input_hash: str
    seed: int | None
    git_sha: str
    repro_cmd: str
    created_at: str


@dataclass(frozen=True)
class StageHistoryEntry:
    cycle: int
    stage: str
    selected_at: str


SCHEMA = """
PRAGMA journal_mode=WAL;

CREATE TABLE IF NOT EXISTS experiments (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    started_at TEXT NOT NULL,
    config_json TEXT NOT NULL,
    git_sha TEXT NOT NULL,
    constellaration_sha TEXT NOT NULL,
    notes TEXT
);

CREATE TABLE IF NOT EXISTS candidates (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    experiment_id INTEGER NOT NULL,
    problem TEXT NOT NULL,
    params_json TEXT NOT NULL,
    seed INTEGER NOT NULL,
    status TEXT NOT NULL,
    design_hash TEXT NOT NULL,
    FOREIGN KEY(experiment_id) REFERENCES experiments(id)
);

CREATE TABLE IF NOT EXISTS metrics (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    candidate_id INTEGER NOT NULL,
    raw_json TEXT NOT NULL,
    feasibility REAL NOT NULL,
    objective REAL,
    hv REAL,
    is_feasible INTEGER NOT NULL,
    FOREIGN KEY(candidate_id) REFERENCES candidates(id)
);

CREATE TABLE IF NOT EXISTS pareto (
    experiment_id INTEGER NOT NULL,
    candidate_id INTEGER NOT NULL,
    PRIMARY KEY (experiment_id, candidate_id),
    FOREIGN KEY(experiment_id) REFERENCES experiments(id),
    FOREIGN KEY(candidate_id) REFERENCES candidates(id)
);

CREATE TABLE IF NOT EXISTS citations (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    experiment_id INTEGER NOT NULL,
    source_path TEXT NOT NULL,
    anchor TEXT,
    quote TEXT,
    FOREIGN KEY(experiment_id) REFERENCES experiments(id)
);

CREATE TABLE IF NOT EXISTS artifacts (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    experiment_id INTEGER NOT NULL,
    path TEXT NOT NULL,
    kind TEXT NOT NULL,
    FOREIGN KEY(experiment_id) REFERENCES experiments(id)
);

CREATE TABLE IF NOT EXISTS budgets (
    experiment_id INTEGER NOT NULL,
    cycle INTEGER NOT NULL,
    screen_evals INTEGER NOT NULL,
    promoted_evals INTEGER NOT NULL,
    high_fidelity_evals INTEGER NOT NULL,
    wall_seconds REAL NOT NULL,
    best_objective REAL,
    best_feasibility REAL,
    best_score REAL,
    best_stage TEXT,
    PRIMARY KEY (experiment_id, cycle),
    FOREIGN KEY(experiment_id) REFERENCES experiments(id)
);

CREATE TABLE IF NOT EXISTS cycles (
    experiment_id INTEGER NOT NULL,
    cycle INTEGER NOT NULL,
    stage TEXT NOT NULL,
    feasible_count INTEGER NOT NULL,
    hv_score REAL,
    hv_exists INTEGER NOT NULL DEFAULT 0,
    created_at TEXT NOT NULL,
    PRIMARY KEY (experiment_id, cycle),
    FOREIGN KEY(experiment_id) REFERENCES experiments(id)
);

CREATE TABLE IF NOT EXISTS cycle_stats (
    experiment_id INTEGER NOT NULL,
    cycle INTEGER NOT NULL,
    hv_score REAL NOT NULL,
    reference_point TEXT NOT NULL,
    pareto_json TEXT NOT NULL,
    PRIMARY KEY (experiment_id, cycle),
    FOREIGN KEY(experiment_id) REFERENCES experiments(id)
);

CREATE TABLE IF NOT EXISTS cycle_hv (
    experiment_id INTEGER NOT NULL,
    cycle INTEGER NOT NULL,
    hv_value REAL NOT NULL,
    hv_delta REAL,
    hv_delta_moving_avg REAL,
    n_feasible INTEGER NOT NULL,
    n_archive INTEGER NOT NULL,
    PRIMARY KEY (experiment_id, cycle),
    FOREIGN KEY(experiment_id) REFERENCES experiments(id)
);

CREATE TABLE IF NOT EXISTS pareto_archive (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    experiment_id INTEGER NOT NULL,
    cycle INTEGER NOT NULL,
    design_hash TEXT NOT NULL,
    fidelity TEXT NOT NULL,
    gradient REAL NOT NULL,
    aspect REAL NOT NULL,
    metrics_id INTEGER,
    git_sha TEXT,
    constellaration_sha TEXT,
    settings_json TEXT NOT NULL,
    seed INTEGER NOT NULL,
    UNIQUE(experiment_id, cycle, design_hash, fidelity),
    FOREIGN KEY(experiment_id) REFERENCES experiments(id),
    FOREIGN KEY(metrics_id) REFERENCES metrics(id)
);

CREATE TABLE IF NOT EXISTS statements (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    experiment_id INTEGER NOT NULL,
    cycle INTEGER NOT NULL,
    stage TEXT NOT NULL,
    text TEXT NOT NULL,
    status TEXT NOT NULL DEFAULT 'PENDING',
    metrics_id INTEGER,
    tool_name TEXT NOT NULL,
    tool_input_hash TEXT NOT NULL,
    seed INTEGER,
    git_sha TEXT NOT NULL,
    repro_cmd TEXT NOT NULL,
    created_at TEXT NOT NULL,
    FOREIGN KEY(experiment_id) REFERENCES experiments(id),
    FOREIGN KEY(metrics_id) REFERENCES metrics(id),
    UNIQUE(experiment_id, cycle, tool_input_hash)
);

CREATE TABLE IF NOT EXISTS stage_history (
    experiment_id INTEGER NOT NULL,
    cycle INTEGER NOT NULL,
    stage TEXT NOT NULL,
    selected_at TEXT NOT NULL,
    PRIMARY KEY (experiment_id, cycle),
    FOREIGN KEY(experiment_id) REFERENCES experiments(id)
);

CREATE TABLE IF NOT EXISTS deterministic_snapshots (
    experiment_id INTEGER NOT NULL,
    cycle INTEGER NOT NULL,
    snapshot_json TEXT NOT NULL,
    constellaration_sha TEXT NOT NULL,
    seed INTEGER NOT NULL,
    created_at TEXT NOT NULL,
    PRIMARY KEY (experiment_id, cycle),
    FOREIGN KEY(experiment_id) REFERENCES experiments(id)
);

CREATE TABLE IF NOT EXISTS literature_notes (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    experiment_id INTEGER NOT NULL,
    cycle INTEGER NOT NULL,
    content TEXT NOT NULL,
    created_at TEXT NOT NULL,
    FOREIGN KEY(experiment_id) REFERENCES experiments(id)
);
"""

DEFAULT_RELATIVE_TOLERANCE = 1e-2


@dataclass(frozen=True)
class BudgetUsage:
    screen_evals: int
    promoted_evals: int
    high_fidelity_evals: int
    wall_seconds: float


def _normalize_to_json(value: Any) -> Any:
    if isinstance(value, (str, int, float, bool)) or value is None:
        return value
    if isinstance(value, Mapping):
        return {str(key): _normalize_to_json(val) for key, val in value.items()}
    if isinstance(value, (list, tuple)):
        return [_normalize_to_json(val) for val in value]
    return str(value)


def _hash_payload(payload: Mapping[str, Any]) -> str:
    serialized = json.dumps(
        _normalize_to_json(payload), sort_keys=True, separators=(",", ":")
    )
    return hashlib.sha256(serialized.encode("utf-8")).hexdigest()


def hash_payload(payload: Mapping[str, Any]) -> str:
    """Return a deterministic digest that mirrors the statements table hashing."""

    return _hash_payload(payload)


class WorldModel:
    """Simple SQLite wrapper for experiments, candidates, and budgets."""

    def __init__(self, path: str | Path) -> None:
        db_path = Path(path)
        init_db(db_path)
        self.db_path = db_path
        self._conn: sqlite3.Connection = sqlite3.connect(
            str(self.db_path), check_same_thread=False
        )
        self._conn.row_factory = sqlite3.Row

    def __enter__(self) -> "WorldModel":
        return self

    def __exit__(self, exc_type, exc, tb) -> None:  # pragma: no cover - context helper
        self.close()

    def close(self) -> None:
        self._conn.close()

    @contextmanager
    def transaction(self):
        """Context manager that wraps multiple writes in a single atomic transaction."""

        try:
            self._conn.execute("BEGIN")
            yield
        except Exception:
            self._conn.rollback()
            raise
        else:
            self._conn.commit()

    def start_experiment(
        self,
        config_payload: Mapping[str, Any],
        git_sha: str,
        constellaration_sha: str | None = None,
        notes: str | None = None,
    ) -> int:
        payload = json.dumps(_normalize_to_json(config_payload), separators=(",", ":"))
        cursor = self._conn.execute(
            "INSERT INTO experiments (started_at, config_json, git_sha, constellaration_sha, notes) VALUES (?, ?, ?, ?, ?)",
            (
                datetime.now(timezone.utc).isoformat(),
                payload,
                git_sha,
                constellaration_sha or "unknown",
                notes,
            ),
        )
        lastrowid = cursor.lastrowid
        assert lastrowid is not None
        self._conn.commit()
        return lastrowid

    def record_cycle(
        self,
        experiment_id: int,
        cycle_number: int,
        screen_evals: int,
        promoted_evals: int,
        high_fidelity_evals: int,
        wall_seconds: float,
        best_params: Mapping[str, Any],
        best_evaluation: Mapping[str, Any],
        seed: int,
        problem: str,
        *,
        log_best_candidate: bool = True,
        commit: bool = True,
    ) -> None:
        payload = (
            experiment_id,
            cycle_number,
            screen_evals,
            promoted_evals,
            high_fidelity_evals,
            wall_seconds,
            best_evaluation.get("objective"),
            best_evaluation.get("feasibility"),
            best_evaluation.get("score"),
            best_evaluation.get("stage", ""),
        )

        def _write() -> None:
            self._conn.execute(
                "INSERT OR REPLACE INTO budgets (experiment_id, cycle, screen_evals, promoted_evals, high_fidelity_evals, wall_seconds, best_objective, best_feasibility, best_score, best_stage) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)",
                payload,
            )
            if log_best_candidate:
                self.log_candidate(
                    experiment_id=experiment_id,
                    problem=problem,
                    params=best_params,
                    seed=seed,
                    status=best_evaluation.get("stage", "unknown"),
                    evaluation=best_evaluation,
                    design_hash=best_evaluation.get("design_hash", ""),
                    commit=False,
                )

        if commit:
            with self._conn:
                _write()
        else:
            _write()

    def record_deterministic_snapshot(
        self,
        experiment_id: int,
        cycle_number: int,
        snapshot: Mapping[str, Any],
        *,
        constellaration_sha: str,
        seed: int,
        created_at: str | None = None,
        commit: bool = True,
    ) -> None:
        payload = json.dumps(_normalize_to_json(snapshot), separators=(",", ":"))
        timestamp = created_at or datetime.now(timezone.utc).isoformat()

        def _write() -> None:
            self._conn.execute(
                """
                INSERT OR REPLACE INTO deterministic_snapshots
                (experiment_id, cycle, snapshot_json, constellaration_sha, seed, created_at)
                VALUES (?, ?, ?, ?, ?, ?)
                """,
                (
                    experiment_id,
                    cycle_number,
                    payload,
                    constellaration_sha,
                    seed,
                    timestamp,
                ),
            )

        if commit:
            with self._conn:
                _write()
        else:
            _write()

    def record_cycle_summary(
        self,
        experiment_id: int,
        cycle_number: int,
        stage: str,
        feasible_count: int,
        hv_score: float | None,
        *,
        created_at: str | None = None,
        commit: bool = True,
    ) -> None:
        timestamp = created_at or datetime.now(timezone.utc).isoformat()
        hv_exists = 0 if hv_score is None else 1

        def _write() -> None:
            self._conn.execute(
                """
                INSERT OR REPLACE INTO cycles (experiment_id, cycle, stage, feasible_count, hv_score, hv_exists, created_at)
                VALUES (?, ?, ?, ?, ?, ?, ?)
                """,
                (
                    experiment_id,
                    cycle_number,
                    stage,
                    int(feasible_count),
                    hv_score,
                    hv_exists,
                    timestamp,
                ),
            )

        if commit:
            with self._conn:
                _write()
        else:
            _write()

    def record_cycle_hv(
        self,
        experiment_id: int,
        cycle_number: int,
        hv_score: float,
        reference_point: Sequence[float],
        pareto_entries: Sequence[Mapping[str, float]],
        *,
        n_feasible: int,
        n_archive: int,
        hv_lookback: int | None = None,
        commit: bool = True,
    ) -> None:
        snapshot = {
            "reference_point": list(reference_point),
            "pareto": [_normalize_to_json(entry) for entry in pareto_entries],
        }

        payload_stats = (
            experiment_id,
            cycle_number,
            hv_score,
            json.dumps(list(reference_point)),
            json.dumps(snapshot, separators=(",", ":")),
        )

        prev_row = self._conn.execute(
            "SELECT hv_value FROM cycle_hv WHERE experiment_id = ? ORDER BY cycle DESC LIMIT 1",
            (experiment_id,),
        ).fetchone()
        previous_hv = float(prev_row["hv_value"]) if prev_row else None
        hv_delta: float | None = None
        if previous_hv is not None:
            hv_delta = float(abs(hv_score - previous_hv))
        hv_delta_moving_avg: float | None = None
        if hv_delta is not None and hv_lookback is not None and hv_lookback > 0:
            lookback_limit = hv_lookback - 1
            delta_rows = []
            if lookback_limit > 0:
                delta_rows = self._conn.execute(
                    "SELECT hv_delta FROM cycle_hv WHERE experiment_id = ? AND hv_delta IS NOT NULL ORDER BY cycle DESC LIMIT ?",
                    (experiment_id, lookback_limit),
                ).fetchall()
            recent_deltas = [
                row["hv_delta"] for row in delta_rows if row["hv_delta"] is not None
            ]
            moving_window = [hv_delta] + recent_deltas[:lookback_limit]
            if len(moving_window) >= hv_lookback:
                hv_delta_moving_avg = float(sum(moving_window) / len(moving_window))

        payload_hv = (
            experiment_id,
            cycle_number,
            hv_score,
            hv_delta,
            hv_delta_moving_avg,
            n_feasible,
            n_archive,
        )

        def _write() -> None:
            self._conn.execute(
                "INSERT OR REPLACE INTO cycle_stats (experiment_id, cycle, hv_score, reference_point, pareto_json) VALUES (?, ?, ?, ?, ?)",
                payload_stats,
            )
            self._conn.execute(
                "INSERT OR REPLACE INTO cycle_hv (experiment_id, cycle, hv_value, hv_delta, hv_delta_moving_avg, n_feasible, n_archive) VALUES (?, ?, ?, ?, ?, ?, ?)",
                payload_hv,
            )

        if commit:
            with self._conn:
                _write()
        else:
            _write()

    def average_recent_hv_delta(
        self,
        experiment_id: int,
        lookback: int,
    ) -> float | None:
        if lookback <= 0:
            return None
        rows = self._conn.execute(
            "SELECT hv_delta FROM cycle_hv WHERE experiment_id = ? AND hv_delta IS NOT NULL ORDER BY cycle DESC LIMIT ?",
            (experiment_id, lookback),
        ).fetchall()
        deltas = [row["hv_delta"] for row in rows if row["hv_delta"] is not None]
        if len(deltas) < lookback:
            return None
        return float(sum(deltas) / len(deltas))

    def log_candidate(
        self,
        experiment_id: int,
        problem: str,
        params: Mapping[str, Any],
        seed: int,
        status: str,
        evaluation: Mapping[str, Any],
        *,
        design_hash: str,
        commit: bool = True,
    ) -> tuple[int, int]:
        params_json = json.dumps(_normalize_to_json(params), separators=(",", ":"))
        cursor = self._conn.execute(
            "INSERT INTO candidates (experiment_id, problem, params_json, seed, status, design_hash) VALUES (?, ?, ?, ?, ?, ?)",
            (experiment_id, problem, params_json, seed, status, design_hash),
        )
        candidate_id = cursor.lastrowid
        assert candidate_id is not None
        metrics_id = self.log_metrics(
            candidate_id,
            evaluation.get("metrics", {}),
            feasibility=float(evaluation.get("feasibility", 0.0)),
            objective=evaluation.get("objective"),
            hv=evaluation.get("hv"),
            commit=False,
        )
        if commit:
            self._conn.commit()
        return candidate_id, metrics_id

    def log_artifact(
        self,
        experiment_id: int,
        path: str | Path,
        kind: str,
        *,
        commit: bool = True,
    ) -> None:
        """Record artifacts such as metrics snapshots or Pareto figures."""

        self._conn.execute(
            "INSERT INTO artifacts (experiment_id, path, kind) VALUES (?, ?, ?)",
            (experiment_id, str(path), kind),
        )
        if commit:
            self._conn.commit()

    def log_statement(
        self,
        experiment_id: int,
        cycle: int,
        stage: str,
        text: str,
        status: str,
        tool_name: str,
        tool_input: Mapping[str, Any],
        *,
        metrics_id: int | None = None,
        seed: int | None = None,
        git_sha: str,
        repro_cmd: str,
        created_at: str | None = None,
        commit: bool = True,
    ) -> int:
        digest = _hash_payload(tool_input)
        timestamp = created_at or datetime.now(timezone.utc).isoformat()
        cursor = self._conn.execute(
            """
            INSERT INTO statements
            (experiment_id, cycle, stage, text, status, metrics_id, tool_name, tool_input_hash, seed, git_sha, repro_cmd, created_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """,
            (
                experiment_id,
                cycle,
                stage,
                text,
                status,
                metrics_id,
                tool_name,
                digest,
                int(seed) if seed is not None else None,
                git_sha,
                repro_cmd,
                timestamp,
            ),
        )
        statement_id = cursor.lastrowid
        assert statement_id is not None
        if commit:
            self._conn.commit()
        return statement_id

    def log_note(
        self,
        experiment_id: int,
        cycle: int,
        content: str,
        *,
        created_at: str | None = None,
        commit: bool = True,
    ) -> int:
        timestamp = created_at or datetime.now(timezone.utc).isoformat()
        cursor = self._conn.execute(
            "INSERT INTO literature_notes (experiment_id, cycle, content, created_at) VALUES (?, ?, ?, ?)",
            (experiment_id, cycle, content, timestamp),
        )
        note_id = cursor.lastrowid
        assert note_id is not None
        if commit:
            self._conn.commit()
        return note_id

    def notes(
        self, experiment_id: int, cycle: int | None = None
    ) -> list[Mapping[str, Any]]:
        """Retrieve stored literature notes, optionally filtered by cycle."""
        if cycle is not None:
            rows = self._conn.execute(
                "SELECT * FROM literature_notes WHERE experiment_id = ? AND cycle = ? ORDER BY id ASC",
                (experiment_id, cycle),
            ).fetchall()
        else:
            rows = self._conn.execute(
                "SELECT * FROM literature_notes WHERE experiment_id = ? ORDER BY id ASC",
                (experiment_id,),
            ).fetchall()
        return [dict(row) for row in rows]

    def statements_for_cycle(
        self, experiment_id: int, cycle: int
    ) -> list[StatementRecord]:
        rows = self._conn.execute(
            """
            SELECT *
            FROM statements
            WHERE experiment_id = ? AND cycle = ?
            ORDER BY id ASC
            """,
            (experiment_id, cycle),
        ).fetchall()
        history: list[StatementRecord] = []
        for row in rows:
            history.append(
                StatementRecord(
                    id=row["id"],
                    experiment_id=row["experiment_id"],
                    cycle=row["cycle"],
                    stage=row["stage"],
                    text=row["text"],
                    status=row["status"],
                    metrics_id=row["metrics_id"],
                    tool_name=row["tool_name"],
                    tool_input_hash=row["tool_input_hash"],
                    seed=row["seed"],
                    git_sha=row["git_sha"],
                    repro_cmd=row["repro_cmd"],
                    created_at=row["created_at"],
                )
            )
        return history

    def stage_history(self, experiment_id: int) -> list[StageHistoryEntry]:
        rows = self._conn.execute(
            """
            SELECT cycle, stage, selected_at
            FROM stage_history
            WHERE experiment_id = ?
            ORDER BY cycle ASC
            """,
            (experiment_id,),
        ).fetchall()
        return [
            StageHistoryEntry(
                cycle=row["cycle"],
                stage=row["stage"],
                selected_at=row["selected_at"],
            )
            for row in rows
        ]

    def cycle_summaries(self, experiment_id: int) -> list[dict[str, Any]]:
        """Return cycle-level summaries combining stage history, budgets, and cycles tables."""

        rows = self._conn.execute(
            """
            SELECT s.cycle,
                   s.stage,
                   b.best_objective,
                   b.best_feasibility,
                   c.hv_score
            FROM stage_history s
            LEFT JOIN budgets b
              ON s.experiment_id = b.experiment_id AND s.cycle = b.cycle
            LEFT JOIN cycles c
              ON s.experiment_id = c.experiment_id AND s.cycle = c.cycle
            WHERE s.experiment_id = ?
            ORDER BY s.cycle ASC
            """,
            (experiment_id,),
        ).fetchall()
        return [
            {
                "cycle": row["cycle"],
                "stage": row["stage"],
                "objective": row["best_objective"],
                "feasibility": row["best_feasibility"],
                "hv": row["hv_score"],
            }
            for row in rows
        ]

    def record_stage_history(
        self,
        experiment_id: int,
        cycle: int,
        stage: str,
        *,
        selected_at: str | None = None,
        commit: bool = True,
    ) -> None:
        timestamp = selected_at or datetime.now(timezone.utc).isoformat()
        self._conn.execute(
            """
            INSERT OR REPLACE INTO stage_history (experiment_id, cycle, stage, selected_at)
            VALUES (?, ?, ?, ?)
            """,
            (experiment_id, cycle, stage, timestamp),
        )
        if commit:
            self._conn.commit()

    def recent_stage_candidates(
        self,
        experiment_id: int,
        problem: str,
        stage: str,
        *,
        limit: int = 64,
    ) -> list[tuple[Mapping[str, Any], float]]:
        """Return parameters + feasibility for the most recent candidates of a given stage."""

        rows = self._conn.execute(
            """
            SELECT c.params_json, m.feasibility
            FROM candidates c
            JOIN metrics m ON m.candidate_id = c.id
            WHERE c.experiment_id = ? AND c.problem = ? AND c.status = ?
            ORDER BY m.id DESC
            LIMIT ?
            """,
            (experiment_id, problem, stage, limit),
        ).fetchall()
        records: list[tuple[Mapping[str, Any], float]] = []
        for row in rows:
            raw_params = row["params_json"]
            feasibility = row["feasibility"]
            if feasibility is None:
                continue
            params = json.loads(raw_params)
            records.append((params, float(feasibility)))
        return records

    def previous_best_hv(self, experiment_id: int, cycle_number: int) -> float | None:
        row = self._conn.execute(
            """
            SELECT MAX(hv_value) AS max_hv
            FROM cycle_hv
            WHERE experiment_id = ? AND cycle < ?
            """,
            (experiment_id, cycle_number),
        ).fetchone()
        if row is None:
            return None
        max_hv = row["max_hv"]
        return float(max_hv) if max_hv is not None else None

    def log_metrics(
        self,
        candidate_id: int,
        metrics_payload: Mapping[str, Any],
        *,
        feasibility: float | None = None,
        objective: float | None = None,
        hv: float | None = None,
        commit: bool = True,
    ) -> int:
        payload = json.dumps(_normalize_to_json(metrics_payload), separators=(",", ":"))
        feasibility_value = float(
            feasibility
            if feasibility is not None
            else metrics_payload.get("feasibility", 0.0)
        )

        def _coerce_float(value: Any | None) -> float | None:
            if value is None:
                return None
            return float(value)

        objective_value = (
            _coerce_float(objective)
            if objective is not None
            else _coerce_float(metrics_payload.get("objective"))
        )
        hv_value = (
            _coerce_float(hv)
            if hv is not None
            else _coerce_float(metrics_payload.get("hv"))
        )
        cursor = self._conn.execute(
            "INSERT INTO metrics (candidate_id, raw_json, feasibility, objective, hv, is_feasible) VALUES (?, ?, ?, ?, ?, ?)",
            (
                candidate_id,
                payload,
                feasibility_value,
                objective_value,
                hv_value,
                1 if feasibility_value <= DEFAULT_RELATIVE_TOLERANCE else 0,
            ),
        )
        metrics_id = cursor.lastrowid
        assert metrics_id is not None
        if commit:
            self._conn.commit()
        return metrics_id

    def record_pareto_archive(
        self,
        experiment_id: int,
        cycle_number: int,
        entries: Sequence[Mapping[str, Any]],
        *,
        commit: bool = True,
    ) -> None:
        def _write() -> None:
            for entry in entries:
                self._conn.execute(
                    """
                    INSERT OR REPLACE INTO pareto_archive
                    (experiment_id, cycle, design_hash, fidelity, gradient, aspect, metrics_id, git_sha, constellaration_sha, settings_json, seed)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """,
                    (
                        experiment_id,
                        cycle_number,
                        entry["design_hash"],
                        entry["fidelity"],
                        float(entry["gradient"]),
                        float(entry["aspect"]),
                        entry.get("metrics_id"),
                        entry.get("git_sha"),
                        entry.get("constellaration_sha"),
                        entry.get("settings_json"),
                        int(entry.get("seed", -1)),
                    ),
                )

        if commit:
            with self._conn:
                _write()
        else:
            _write()

    def budget_usage(self, experiment_id: int) -> BudgetUsage:
        row = self._conn.execute(
            "SELECT COALESCE(SUM(screen_evals), 0), COALESCE(SUM(promoted_evals), 0), COALESCE(SUM(high_fidelity_evals), 0), COALESCE(SUM(wall_seconds), 0.0) FROM budgets WHERE experiment_id = ?",
            (experiment_id,),
        ).fetchone()
        return BudgetUsage(
            screen_evals=int(row[0]),
            promoted_evals=int(row[1]),
            high_fidelity_evals=int(row[2]),
            wall_seconds=float(row[3]),
        )

    def cycles_completed(self, experiment_id: int) -> int:
        row = self._conn.execute(
            "SELECT COUNT(*) FROM budgets WHERE experiment_id = ?",
            (experiment_id,),
        ).fetchone()
        return int(row[0])

    def upsert_pareto(
        self, experiment_id: int, candidate_id: int, *, commit: bool = True
    ) -> None:
        self._conn.execute(
            "INSERT OR REPLACE INTO pareto (experiment_id, candidate_id) VALUES (?, ?)",
            (experiment_id, candidate_id),
        )
        if commit:
            self._conn.commit()

    def to_networkx(self, experiment_id: int) -> PropertyGraph:
        graph = PropertyGraph()
        experiment = self._conn.execute(
            "SELECT * FROM experiments WHERE id = ?",
            (experiment_id,),
        ).fetchone()
        if experiment is None:
            raise ValueError(f"experiment {experiment_id} does not exist")
        exp_node = f"experiment:{experiment_id}"
        graph.add_node(
            exp_node,
            type="experiment",
            started_at=experiment["started_at"],
            git_sha=experiment["git_sha"],
            notes=experiment["notes"],
        )
        candidate_rows = self._conn.execute(
            "SELECT * FROM candidates WHERE experiment_id = ?",
            (experiment_id,),
        ).fetchall()
        for candidate in candidate_rows:
            candidate_node = f"candidate:{candidate['id']}"
            graph.add_node(
                candidate_node,
                type="candidate",
                problem=candidate["problem"],
                status=candidate["status"],
                seed=candidate["seed"],
                params=candidate["params_json"],
            )
            graph.add_edge(exp_node, candidate_node, relation="contains")
        metrics_rows = self._conn.execute(
            "SELECT m.*, c.problem FROM metrics m JOIN candidates c ON m.candidate_id = c.id WHERE c.experiment_id = ?",
            (experiment_id,),
        ).fetchall()
        for metrics in metrics_rows:
            metrics_node = f"metrics:{metrics['id']}"
            graph.add_node(
                metrics_node,
                type="metrics",
                feasibility=metrics["feasibility"],
                objective=metrics["objective"],
                hv=metrics["hv"],
                raw=metrics["raw_json"],
                problem=metrics["problem"],
            )
            candidate_node = f"candidate:{metrics['candidate_id']}"
            graph.add_edge(candidate_node, metrics_node, relation="evaluated_as")
        citations = self._conn.execute(
            "SELECT * FROM citations WHERE experiment_id = ?",
            (experiment_id,),
        ).fetchall()
        for citation in citations:
            citation_node = f"citation:{citation['id']}"
            graph.add_node(
                citation_node,
                type="citation",
                source_path=citation["source_path"],
                anchor=citation["anchor"],
                quote=citation["quote"],
            )
            graph.add_edge(exp_node, citation_node, relation="cites")
        artifacts = self._conn.execute(
            "SELECT * FROM artifacts WHERE experiment_id = ?",
            (experiment_id,),
        ).fetchall()
        for artifact in artifacts:
            artifact_node = f"artifact:{artifact['id']}"
            graph.add_node(
                artifact_node,
                type="artifact",
                path=artifact["path"],
                kind=artifact["kind"],
            )
            graph.add_edge(exp_node, artifact_node, relation="produces")
        budgets = self._conn.execute(
            "SELECT * FROM budgets WHERE experiment_id = ?",
            (experiment_id,),
        ).fetchall()
        for budget in budgets:
            budget_node = f"budget:{experiment_id}:{budget['cycle']}"
            graph.add_node(
                budget_node,
                type="budget",
                cycle=budget["cycle"],
                screen_evals=budget["screen_evals"],
                promoted_evals=budget["promoted_evals"],
                high_fidelity_evals=budget["high_fidelity_evals"],
                wall_seconds=budget["wall_seconds"],
                best_stage=budget["best_stage"],
            )
            graph.add_edge(exp_node, budget_node, relation="cycle")
        stage_rows = self._conn.execute(
            "SELECT * FROM stage_history WHERE experiment_id = ?",
            (experiment_id,),
        ).fetchall()
        for stage_row in stage_rows:
            stage_node = f"stage:{experiment_id}:{stage_row['cycle']}"
            graph.add_node(
                stage_node,
                type="stage",
                cycle=stage_row["cycle"],
                stage=stage_row["stage"],
                selected_at=stage_row["selected_at"],
            )
            graph.add_edge(exp_node, stage_node, relation="governance")
        pareto_rows = self._conn.execute(
            "SELECT candidate_id FROM pareto WHERE experiment_id = ?",
            (experiment_id,),
        ).fetchall()
        for pareto in pareto_rows:
            candidate_node = f"candidate:{pareto['candidate_id']}"
            if graph.has_node(candidate_node):
                graph.add_edge(exp_node, candidate_node, relation="pareto_member")
        note_rows = self._conn.execute(
            "SELECT * FROM literature_notes WHERE experiment_id = ?",
            (experiment_id,),
        ).fetchall()
        for note in note_rows:
            note_node = f"note:{note['id']}"
            graph.add_node(
                note_node,
                type="note",
                cycle=note["cycle"],
                content=note["content"],
                created_at=note["created_at"],
            )
            graph.add_edge(exp_node, note_node, relation="literature_note")
        return graph

    def property_graph_summary(self, experiment_id: int) -> Mapping[str, Any]:
        """Return node/edge counts and citation anchors for reporting."""

        graph = self.to_networkx(experiment_id)
        citations = [
            {
                "source_path": attrs.get("source_path"),
                "anchor": attrs.get("anchor"),
                "quote": attrs.get("quote"),
            }
            for attrs in graph.nodes.values()
            if isinstance(attrs, Mapping) and attrs.get("type") == "citation"
        ]
        return {
            "node_count": len(graph.nodes),
            "edge_count": len(graph.edges),
            "citation_count": len(citations),
            "citations": citations,
        }

    def surrogate_training_data(
        self,
        *,
        target: str = "hv",
        problem: str | None = None,
    ) -> list[tuple[Mapping[str, Any], float]]:
        """Return cached metrics + target values usable by the surrogate ranker."""

        allowed_targets = {"hv", "objective", "feasibility"}
        target_column = target if target in allowed_targets else "hv"
        rows = self._conn.execute(
            """
            SELECT c.problem, c.params_json, m.raw_json, m.hv, m.objective, m.feasibility
            FROM metrics m
            JOIN candidates c ON m.candidate_id = c.id
            ORDER BY m.id ASC
            """
        ).fetchall()
        history: list[tuple[Mapping[str, Any], float]] = []
        for row in rows:
            if problem is not None and row["problem"] != problem:
                continue
            value = row[target_column]
            if value is None:
                continue
            metrics_payload = json.loads(row["raw_json"])
            try:
                params_payload = json.loads(row["params_json"])
            except (TypeError, ValueError):
                params_payload = None
            if params_payload is not None:
                metrics_payload["candidate_params"] = params_payload
            history.append((metrics_payload, float(value)))
        return history


def init_db(path: str | Path) -> None:
    db_path = Path(path)
    db_path.parent.mkdir(parents=True, exist_ok=True)
    con = sqlite3.connect(str(db_path))
    try:
        con.executescript(SCHEMA)
        try:
            con.execute(
                "ALTER TABLE candidates ADD COLUMN design_hash TEXT NOT NULL DEFAULT ''"
            )
        except sqlite3.OperationalError:
            pass
        try:
            con.execute(
                "ALTER TABLE experiments ADD COLUMN constellaration_sha TEXT NOT NULL DEFAULT 'unknown'"
            )
        except sqlite3.OperationalError:
            pass
        try:
            con.execute("ALTER TABLE cycle_hv ADD COLUMN hv_delta REAL")
        except sqlite3.OperationalError:
            pass
        try:
            con.execute("ALTER TABLE cycle_hv ADD COLUMN hv_delta_moving_avg REAL")
        except sqlite3.OperationalError:
            pass
        con.commit()
    finally:
        con.close()


================================================================================
File: adapter.py
================================================================================

"""PEFT/LoRA adapter integration for Wave 7 adaptation hooks (docs/WAVE_7_ADAPTATION.md).

Set ``AI_SCIENTIST_PEFT=1`` to activate adapter loading before tool calls and queue updates
in ``reports/adapters/{tool}/{stage}/adapter.safetensors`` / ``reports/adapters/queue.jsonl``."""

from __future__ import annotations

import json
import logging
import os
from dataclasses import dataclass, field
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Callable, Mapping, Protocol

_LOGGER = logging.getLogger(__name__)
_ENV_VAR = "AI_SCIENTIST_PEFT"
_ADAPTERS_ROOT = Path("reports") / "adapters"
_ADAPTER_ROOT_ENV = "AI_SCIENTIST_ADAPTER_ROOT"
_QUEUE_PATH = _ADAPTERS_ROOT / "queue.jsonl"
_PERSIST_DIR_ENV = "AI_SCIENTIST_ADAPTER_PERSIST_DIR"


AdapterLoader = Callable[[Path, str, str], bool]
AdapterPersistHandler = Callable[[Path, str, str], bool]

_REGISTERED_LOADERS: list[tuple[str, AdapterLoader]] = []
_REGISTERED_PERSISTERS: list[tuple[str, AdapterPersistHandler]] = []


def register_adapter_loader(name: str, loader: AdapterLoader) -> None:
    """Register a callable that can load adapters for the runner (HF PEFT, ggml, etc.)."""

    _REGISTERED_LOADERS.append((name, loader))


def register_adapter_persist_handler(name: str, handler: AdapterPersistHandler) -> None:
    """Register a callable that persists adapters produced during a cycle run."""

    _REGISTERED_PERSISTERS.append((name, handler))


def _adapter_bundle_path(tool_name: str, stage: str) -> Path:
    normalized_stage = stage.lower().strip()
    root = Path(os.getenv(_ADAPTER_ROOT_ENV, _ADAPTERS_ROOT))
    return root / tool_name / normalized_stage / "adapter.safetensors"


def _ensure_adapter_directory(path: Path) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)


def _queue_entry(
    tool_name: str,
    stage: str,
    adapter_path: Path,
    backend: str | None,
    status: str,
    version: str | None = None,
) -> dict[str, Any]:
    return {
        "tool": tool_name,
        "stage": stage,
        "adapter_path": adapter_path.as_posix(),
        "backend": backend,
        "status": status,
        "timestamp": datetime.now(timezone.utc).replace(microsecond=0).isoformat(),
        "version": version,
    }


def _append_to_queue(entry: Mapping[str, Any]) -> None:
    queue_path = Path(os.getenv(_ADAPTER_ROOT_ENV, _ADAPTERS_ROOT)) / "queue.jsonl"
    _ensure_adapter_directory(queue_path)
    with queue_path.open("a", encoding="utf-8") as handle:
        handle.write(json.dumps(entry))
        handle.write("\n")


def record_adapter_refresh(
    tool_name: str,
    stage: str,
    *,
    backend: str | None = None,
    status: str = "refreshed",
    adapter_path: Path | None = None,
    version: str | None = None,
) -> None:
    """Expose queue logging so offline adapters can annotate backend refreshes."""

    bundle_path = adapter_path or _adapter_bundle_path(tool_name, stage)
    entry = _queue_entry(tool_name, stage, bundle_path, backend, status, version)
    _append_to_queue(entry)


def _try_load_adapter(adapter_path: Path, tool_name: str, stage: str) -> str | None:
    for backend_name, loader in _REGISTERED_LOADERS:
        if loader(adapter_path, tool_name, stage):
            return backend_name
    return None


def _try_persist_adapter(adapter_path: Path, tool_name: str, stage: str) -> str | None:
    for backend_name, handler in _REGISTERED_PERSISTERS:
        if handler(adapter_path, tool_name, stage):
            return backend_name
    return None


def _metadata_path(root: Path, version: str | None = None) -> Path:
    if version:
        return root / f"metadata_{version}.json"
    return root / "metadata.json"


def _load_metadata(root: Path, version: str | None = None) -> dict[str, Any]:
    for candidate in (_metadata_path(root, version), _metadata_path(root, None)):
        if candidate.exists():
            try:
                return json.loads(candidate.read_text(encoding="utf-8"))
            except json.JSONDecodeError:
                _LOGGER.warning("Failed to parse adapter metadata at %s", candidate)
    return {}


def _extract_version(adapter_path: Path) -> str:
    stem = adapter_path.stem
    if stem.startswith("adapter_"):
        return stem.replace("adapter_", "", 1)
    return "current"


def _latest_adapter_bundle(
    tool_name: str, stage: str
) -> tuple[Path | None, str | None, dict[str, Any]]:
    bundle_root = _adapter_bundle_path(tool_name, stage).parent
    candidates = [bundle_root / "adapter.safetensors", *sorted(bundle_root.glob("adapter_*.safetensors"))]
    existing = [path for path in candidates if path.exists()]
    if not existing:
        return None, None, {}
    newest = max(existing, key=lambda path: path.stat().st_mtime)
    version = _extract_version(newest)
    metadata = _load_metadata(bundle_root, version)
    return newest, version, metadata


def _json_metadata_loader(adapter_path: Path, tool_name: str, stage: str) -> bool:
    """Load JSON-based adapter bundles for inspection."""

    try:
        raw = adapter_path.read_text(encoding="utf-8").strip()
    except (FileNotFoundError, OSError) as exc:  # pragma: no cover - upstream guard
        _LOGGER.debug(
            "JSON adapter loader missing %s:%s (%s)",
            tool_name,
            stage,
            exc,
        )
        return False

    if not raw:
        _LOGGER.debug("JSON adapter %s:%s is empty", tool_name, stage)
        return True

    try:
        payload = json.loads(raw)
    except json.JSONDecodeError:
        _LOGGER.warning(
            "JSON adapter %s:%s failed to parse; treating as raw bytes",
            tool_name,
            stage,
        )
        return True

    _LOGGER.info(
        "Loaded JSON adapter %s:%s summary=%s",
        tool_name,
        stage,
        {
            "entry_count": payload.get("preference_pair_count"),
            "dataset_path": payload.get("dataset_path"),
        },
    )
    return True


def _staged_adapter_persist(adapter_path: Path, tool_name: str, stage: str) -> bool:
    """Promote staged adapter bundles when a staging directory is configured."""

    persist_root = os.getenv(_PERSIST_DIR_ENV)
    if not persist_root:
        return False

    normalized_stage = stage.lower().strip()
    staged_path = (
        Path(persist_root) / tool_name / normalized_stage / "adapter.safetensors"
    )
    if not staged_path.exists():
        return False

    _ensure_adapter_directory(adapter_path)
    staged_path.replace(adapter_path)
    _LOGGER.info(
        "Persisted staged adapter %s:%s from %s",
        tool_name,
        stage,
        staged_path,
    )
    return True


class ProblemEvaluator(Protocol):
    """Lightweight protocol that mirrors the evaluator interface used in runner.py."""

    def __call__(
        self,
        boundary_params: Mapping[str, Any],
        *,
        stage: str,
        use_cache: bool = True,
    ) -> dict[str, Any]: ...


@dataclass(frozen=True)
class AdapterState:
    """Tracks which LoRA weights were loaded/applied so Wave 7 can replay the stack."""

    loaded: dict[str, str] = field(default_factory=dict)
    updates: list[str] = field(default_factory=list)
    versions: dict[str, str] = field(default_factory=dict)

    def load_lora_weights(self, label: str, stage: str) -> None:
        """Record when a LoRA bundle is staged for the current tool/stage."""
        bundle_path, version, metadata = _latest_adapter_bundle(label, stage)
        key = f"{label}:{stage}"
        if bundle_path is None:
            self.loaded[key] = "missing"
            self.versions[key] = "missing"
            _LOGGER.debug(
                "Adapter bundle not found for %s:%s at %s",
                label,
                stage,
                _adapter_bundle_path(label, stage),
            )
            return

        backend_name = _try_load_adapter(bundle_path, label, stage)
        status = (
            f"{backend_name}:{bundle_path.as_posix()}"
            if backend_name
            else f"ready:{bundle_path.as_posix()}"
        )
        version_label = metadata.get("version") or version or "unknown"
        self.loaded[key] = status
        self.versions[key] = version_label
        _LOGGER.debug(
            "AdapterState.load_lora_weights label=%s stage=%s status=%s version=%s",
            label,
            stage,
            status,
            version_label,
        )

    def push_updates(self, label: str, stage: str) -> None:
        """Log when downstream adapters propagated updates."""
        bundle_path = _adapter_bundle_path(label, stage)
        persisted_backend = _try_persist_adapter(bundle_path, label, stage)
        status = f"persisted:{persisted_backend}" if persisted_backend else "queued"

        if not persisted_backend:
            entry = _queue_entry(
                label,
                stage,
                bundle_path,
                None,
                status,
                version=self.versions.get(f"{label}:{stage}"),
            )
            _append_to_queue(entry)

        self.updates.append(f"{label}:{stage}:{status}")
        _LOGGER.debug(
            "AdapterState.push_updates label=%s stage=%s status=%s path=%s",
            label,
            stage,
            status,
            bundle_path,
        )


adapter_state = AdapterState()


def is_peft_enabled() -> bool:
    """Return True when the Wave 7 PEFT toggle is set in the environment."""

    return os.getenv(_ENV_VAR, "0").lower() in {"1", "true", "yes"}


def current_adapter_version(tool_name: str, stage: str) -> str | None:
    """Return the last loaded adapter version for the given tool/stage, if any."""

    return adapter_state.versions.get(f"{tool_name}:{stage}")


def prepare_peft_hook(tool_name: str, stage: str) -> None:
    """Hook point that loads LoRA weights before a tool call."""

    if not is_peft_enabled():
        return
    _LOGGER.info("Preparing PEFT hook tool=%s stage=%s", tool_name, stage)
    adapter_state.load_lora_weights(tool_name, stage)


def apply_lora_updates(tool_name: str, stage: str) -> None:
    """Hook point that pushes LoRA updates after a tool call."""

    if not is_peft_enabled():
        return
    _LOGGER.info("Applying LoRA updates tool=%s stage=%s", tool_name, stage)
    adapter_state.push_updates(tool_name, stage)


def with_peft(evaluate_fn: ProblemEvaluator, tool_name: str) -> ProblemEvaluator:
    """Return either the evaluator unchanged or a PEFT-wrapped callable."""

    if not is_peft_enabled():
        return evaluate_fn

    def _wrapped(
        boundary_params: Mapping[str, Any],
        *,
        stage: str,
        use_cache: bool = True,
    ) -> dict[str, Any]:
        prepare_peft_hook(tool_name, stage)
        result = evaluate_fn(boundary_params, stage=stage, use_cache=use_cache)
        apply_lora_updates(tool_name, stage)
        return result

    return _wrapped


register_adapter_loader("json_metadata", _json_metadata_loader)
register_adapter_persist_handler("staged_adapter", _staged_adapter_persist)


================================================================================
File: tools.py
================================================================================

"""Physics tool wrappers for the ConStellaration AI Scientist."""

from __future__ import annotations

import hashlib
import json
import math
from collections import defaultdict
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Callable, Dict, Mapping, Sequence, Tuple

import numpy as np
from pymoo.indicators import hv as pymoo_hv

from ai_scientist import rag
from ai_scientist import memory
from constellaration import forward_model
from constellaration.geometry import surface_rz_fourier

_DEFAULT_RELATIVE_TOLERANCE = 1e-2
_CANONICAL_PRECISION = 1e-8
_DEFAULT_SCHEMA_VERSION = 1
_DEFAULT_ROUNDING = 1e-6
_EVALUATION_CACHE: Dict[Tuple[str, str], Dict[str, Any]] = {}
_CACHE_STATS: Dict[str, Dict[str, int]] = defaultdict(lambda: {"hits": 0, "misses": 0})
_P3_REFERENCE_POINT: Tuple[float, float] = (1.0, 20.0)


@dataclass(frozen=True)
class FlattenSchema:
    """Schema describing the intended Fourier truncation and hash version."""

    mpol: int
    ntor: int
    schema_version: int = _DEFAULT_SCHEMA_VERSION
    rounding: float = _DEFAULT_ROUNDING


@dataclass(frozen=True)
class BoundaryParams:
    """Container for surface parameters that may evolve in future waves."""

    params: Mapping[str, Any]


@dataclass(frozen=True)
class P3Summary:
    """Compact summary of the per-cycle P3 pareto front and hypervolume."""

    hv_score: float
    reference_point: Tuple[float, float]
    feasible_count: int
    archive_size: int
    pareto_entries: Tuple["ParetoEntry", ...]


@dataclass(frozen=True)
class ParetoEntry:
    design_hash: str
    seed: int
    stage: str
    gradient: float
    aspect_ratio: float
    objective: float
    feasibility: float

    def as_mapping(self) -> Mapping[str, float]:
        return {
            "seed": float(self.seed),
            "gradient": self.gradient,
            "aspect_ratio": self.aspect_ratio,
            "objective": self.objective,
            "feasibility": self.feasibility,
        }


def _quantize_float(value: float, *, precision: float = _CANONICAL_PRECISION) -> float:
    if precision <= 0.0:
        return float(value)
    return float(round(value / precision) * precision)


def _canonicalize_value(value: Any, *, precision: float = _CANONICAL_PRECISION) -> Any:
    if isinstance(value, Mapping):
        return {k: _canonicalize_value(v, precision=precision) for k, v in sorted(value.items())}
    if isinstance(value, np.ndarray):
        return _canonicalize_value(value.tolist(), precision=precision)
    if isinstance(value, (list, tuple)):
        return [_canonicalize_value(v, precision=precision) for v in value]
    if isinstance(value, float):
        return _quantize_float(value, precision=precision)
    if isinstance(value, (int, str, bool)) or value is None:
        return value
    return str(value)


def _hash_params(
    params: Mapping[str, Any], *, schema: FlattenSchema | None = None, rounding: float | None = None
) -> str:
    return design_hash(params, schema=schema, rounding=rounding)


def design_hash(
    params: Mapping[str, Any] | BoundaryParams,
    *,
    schema: FlattenSchema | None = None,
    rounding: float | None = None,
) -> str:
    params_map = _ensure_mapping(params)
    precision = rounding if rounding is not None else _CANONICAL_PRECISION
    payload: Mapping[str, Any]
    if schema is None:
        payload = params_map
    else:
        payload = {
            "schema_version": schema.schema_version,
            "mpol": schema.mpol,
            "ntor": schema.ntor,
            "rounding": schema.rounding,
            "params": params_map,
        }
        precision = rounding if rounding is not None else schema.rounding

    normalized = _canonicalize_value(payload, precision=precision)
    digest = json.dumps(normalized, sort_keys=True, separators=(",", ":"))
    return hashlib.sha256(digest.encode("utf-8")).hexdigest()


def _ensure_mapping(params: Mapping[str, Any] | BoundaryParams) -> Mapping[str, Any]:
    if isinstance(params, BoundaryParams):
        return params.params
    return params


def _derive_schema_from_params(
    params: Mapping[str, Any], *, schema_version: int = _DEFAULT_SCHEMA_VERSION, rounding: float = _DEFAULT_ROUNDING
) -> FlattenSchema:
    r_cos = np.asarray(params.get("r_cos", []), dtype=float)
    z_sin = np.asarray(params.get("z_sin", []), dtype=float)
    mpol_candidates = []
    ntor_candidates = []
    if r_cos.size:
        mpol_candidates.append(max(0, r_cos.shape[0] - 1))
        ntor_candidates.append(max(0, (r_cos.shape[1] - 1) // 2))
    if z_sin.size:
        mpol_candidates.append(max(0, z_sin.shape[0] - 1))
        ntor_candidates.append(max(0, (z_sin.shape[1] - 1) // 2))

    mpol = max(mpol_candidates) if mpol_candidates else 0
    ntor = max(ntor_candidates) if ntor_candidates else 0
    return FlattenSchema(mpol=mpol, ntor=ntor, schema_version=schema_version, rounding=rounding)


def _coefficient_from_matrix(matrix: np.ndarray, m: int, n: int, schema_ntor: int) -> float:
    if matrix.ndim != 2 or m < 0 or n < -schema_ntor or n > schema_ntor:
        return 0.0
    if m >= matrix.shape[0]:
        return 0.0

    matrix_ntor = max(0, (matrix.shape[1] - 1) // 2)
    column = n + matrix_ntor
    if column < 0 or column >= matrix.shape[1]:
        return 0.0
    return float(matrix[m, column])


def structured_flatten(
    params: Mapping[str, Any] | BoundaryParams,
    schema: FlattenSchema | None = None,
) -> tuple[np.ndarray, FlattenSchema]:
    """Flatten Fourier coefficients with deterministic ordering and schema metadata.

    The layout is `[r_cos modes..., z_sin modes...]` with n ranging from `-ntor`
    to `ntor` for each m in `[0, mpol]`. Missing coefficients are zero-padded and
    values are rounded to the schema precision to stabilize hashes and caches.
    """

    params_map = _ensure_mapping(params)
    active_schema = schema or _derive_schema_from_params(params_map)
    rounding = active_schema.rounding

    r_cos = np.asarray(params_map.get("r_cos", []), dtype=float)
    z_sin = np.asarray(params_map.get("z_sin", []), dtype=float)

    values: list[float] = []
    for matrix in (r_cos, z_sin):
        for m in range(active_schema.mpol + 1):
            for n in range(-active_schema.ntor, active_schema.ntor + 1):
                coefficient = _coefficient_from_matrix(matrix, m, n, active_schema.ntor)
                values.append(_quantize_float(coefficient, precision=rounding))

    return np.asarray(values, dtype=float), active_schema


def _evaluate_cached_stage(
    boundary_params: Mapping[str, Any] | BoundaryParams,
    *,
    stage: str,
    compute: Callable[[Mapping[str, Any]], Dict[str, Any]],
    maximize: bool,
    use_cache: bool = True,
) -> Dict[str, Any]:
    params_map = _ensure_mapping(boundary_params)
    stage_lower = stage.lower()
    schema = _derive_schema_from_params(params_map)
    cache_key = (stage_lower, _hash_params(params_map, schema=schema, rounding=schema.rounding))
    stats = _CACHE_STATS[stage_lower]
    if use_cache:
        cached = _EVALUATION_CACHE.get(cache_key)
        if cached is not None:
            stats["hits"] += 1
            return cached

    stats["misses"] += 1
    result = _safe_evaluate(lambda: compute(params_map), stage=stage_lower, maximize=maximize)
    if use_cache:
        _EVALUATION_CACHE[cache_key] = result
    return result


def _settings_for_stage(
    stage: str, *, skip_qi: bool = False
) -> forward_model.ConstellarationSettings:
    stage_lower = stage.lower()
    if stage_lower == "promote":
        settings = forward_model.ConstellarationSettings.default_high_fidelity_skip_qi()
    elif (
        stage_lower.startswith("p2")
        or stage_lower.startswith("p3")
        or stage_lower == "high_fidelity"
    ):
        settings = forward_model.ConstellarationSettings.default_high_fidelity()
    else:
        settings = forward_model.ConstellarationSettings()

    if skip_qi:
        return settings.model_copy(
            update={
                "boozer_preset_settings": None,
                "qi_settings": None,
            }
        )
    return settings


def _normalize_between_bounds(
    value: float, lower_bound: float, upper_bound: float
) -> float:
    assert lower_bound < upper_bound
    normalized = (value - lower_bound) / (upper_bound - lower_bound)
    return float(np.clip(normalized, 0.0, 1.0))


def _max_violation(margins: Mapping[str, float]) -> float:
    if not margins:
        return float("inf")
    return float(max(0.0, *[max(0.0, value) for value in margins.values()]))


def compute_constraint_margins(
    metrics: Mapping[str, Any] | forward_model.ConstellarationMetrics,
    problem: str,
) -> dict[str, float]:
    metrics_map = metrics.model_dump() if hasattr(metrics, "model_dump") else dict(metrics)
    problem_key = problem.lower()

    def _log10_margin(target: float) -> float:
        return _log10_or_large(metrics_map.get("qi")) - target

    margins: dict[str, float] = {}

    if problem_key.startswith("p1"):
        margins = {
            "aspect_ratio": float(metrics_map.get("aspect_ratio", float("nan"))) - 4.0,
            "average_triangularity": float(metrics_map.get("average_triangularity", float("nan"))) - (-0.5),
            "edge_rotational_transform": 0.3
            - float(metrics_map.get("edge_rotational_transform_over_n_field_periods", float("nan"))),
        }
    elif problem_key.startswith("p2"):
        margins = {
            "aspect_ratio": float(metrics_map.get("aspect_ratio", float("nan"))) - 10.0,
            "edge_rotational_transform": 0.25
            - float(metrics_map.get("edge_rotational_transform_over_n_field_periods", float("nan"))),
            "edge_magnetic_mirror_ratio": float(
                metrics_map.get("edge_magnetic_mirror_ratio", float("nan"))
            )
            - 0.2,
            "max_elongation": float(metrics_map.get("max_elongation", float("nan"))) - 5.0,
            "qi_log10": _log10_margin(-4.0),
        }
    else:
        flux_value = metrics_map.get("flux_compression_in_regions_of_bad_curvature")
        flux_margin = (
            float(flux_value) - 0.9
            if flux_value is not None
            else 0.0
        )
        margins = {
            "edge_rotational_transform": 0.25
            - float(metrics_map.get("edge_rotational_transform_over_n_field_periods", float("nan"))),
            "edge_magnetic_mirror_ratio": float(
                metrics_map.get("edge_magnetic_mirror_ratio", float("nan"))
            )
            - 0.25,
            "vacuum_well": -float(metrics_map.get("vacuum_well", float("nan"))),
            "flux_compression": flux_margin,
            "qi_log10": _log10_margin(-3.5),
        }

    return margins


def _log10_or_large(value: float | None) -> float:
    if value is None or value <= 0.0:
        return 10.0
    return float(math.log10(value))


def _contains_invalid_number(node: Any) -> bool:
    if isinstance(node, Mapping):
        return any(_contains_invalid_number(value) for value in node.values())
    if isinstance(node, (list, tuple)):
        return any(_contains_invalid_number(value) for value in node)
    if isinstance(node, np.ndarray):
        return not np.all(np.isfinite(node))
    if isinstance(node, float):
        return not math.isfinite(node)
    return False


def _replace_invalid_numbers(node: Any, replacement: float) -> Any:
    if isinstance(node, Mapping):
        return {key: _replace_invalid_numbers(value, replacement) for key, value in node.items()}
    if isinstance(node, (list, tuple)):
        return [_replace_invalid_numbers(value, replacement) for value in node]
    if isinstance(node, np.ndarray):
        sanitized = np.where(np.isfinite(node), node, replacement)
        return sanitized.tolist()
    if isinstance(node, float) and not math.isfinite(node):
        return float(replacement)
    return node


def _penalized_result(
    *, stage: str, maximize: bool, penalty: float, error: str | None = None
) -> Dict[str, Any]:
    return {
        "stage": stage,
        "objective": penalty,
        "minimize_objective": not maximize,
        "feasibility": float("inf"),
        "score": 0.0,
        "constraint_margins": {},
        "max_violation": float("inf"),
        "metrics": {},
        "error": error,
        "penalized": True,
    }


def _safe_evaluate(
    compute: Callable[[], Dict[str, Any]],
    stage: str,
    *,
    maximize: bool = False,
) -> Dict[str, Any]:
    penalty = -1e9 if maximize else 1e9
    try:
        result = compute()
    except Exception as exc:  # noqa: BLE001
        return _penalized_result(stage=stage, maximize=maximize, penalty=penalty, error=str(exc))

    invalid = _contains_invalid_number(result)
    if invalid:
        sanitized = _replace_invalid_numbers(result, penalty)
        sanitized["objective"] = penalty
        sanitized["feasibility"] = float("inf")
        sanitized["score"] = 0.0
        sanitized["constraint_margins"] = {}
        sanitized["max_violation"] = float("inf")
        sanitized.setdefault("metrics", {})
        sanitized["penalized"] = True
        sanitized["stage"] = stage
        sanitized["minimize_objective"] = not maximize
        return sanitized

    result.setdefault("stage", stage)
    result.setdefault("minimize_objective", not maximize)
    return result


def _gradient_score(metrics: forward_model.ConstellarationMetrics) -> float:
    gradient = float(metrics.minimum_normalized_magnetic_gradient_scale_length)
    aspect = float(metrics.aspect_ratio)
    return float(gradient / max(1.0, aspect))


def _p2_feasibility(metrics: forward_model.ConstellarationMetrics) -> float:
    margins = compute_constraint_margins(metrics, "p2")
    return _max_violation(margins)


def _p3_feasibility(metrics: forward_model.ConstellarationMetrics) -> float:
    margins = compute_constraint_margins(metrics, "p3")
    return _max_violation(margins)


def _objective_vector(metrics: Mapping[str, Any]) -> Tuple[float, float]:
    gradient = float(metrics["minimum_normalized_magnetic_gradient_scale_length"])
    aspect = float(metrics["aspect_ratio"])
    return -gradient, aspect


def _extract_p3_point(metrics: Mapping[str, Any]) -> Tuple[float, float]:
    vector = _objective_vector(metrics)
    return -vector[0], vector[1]


def _dominates(a: Tuple[float, float], b: Tuple[float, float]) -> bool:
    """Return True if objective a Pareto dominates b (higher gradient, lower aspect)."""

    higher_gradient = a[0] >= b[0]
    lower_aspect = a[1] <= b[1]
    strict = a[0] > b[0] or a[1] < b[1]
    return higher_gradient and lower_aspect and strict


def _hypervolume_minimization(
    vectors: Sequence[Tuple[float, float]],
    reference_point: Tuple[float, float],
) -> float:
    if not vectors:
        return 0.0
    indicator = pymoo_hv.Hypervolume(ref_point=np.asarray(reference_point, dtype=float))
    output = indicator(np.asarray(vectors, dtype=float))
    return float(output if output is not None else 0.0)


def summarize_p3_candidates(
    candidates: Sequence[Mapping[str, Any] | dict[str, Any]],
    *,
    reference_point: Tuple[float, float] = _P3_REFERENCE_POINT,
) -> P3Summary:
    """Produce the hypervolume score and all non-dominated seeds for a candidate batch."""

    @dataclass(frozen=True)
    class _P3Entry:
        gradient: float
        aspect: float
        seed: int
        evaluation: Mapping[str, Any]
        feasibility: float
        design_hash: str
        design_hash: str

    entries: list[_P3Entry] = []
    for candidate in candidates:
        design_id = candidate.get("design_hash")
        if design_id is None:
            design_id = design_hash(candidate.get("params", {}))
        design_id = str(design_id)
        eval_metrics = candidate["evaluation"]["metrics"]
        gradient, aspect = _extract_p3_point(eval_metrics)
        seed = int(candidate.get("seed", -1))
        feasibility = float(candidate["evaluation"]["feasibility"])
        entries.append(
            _P3Entry(
                design_hash=design_id,
                gradient=gradient,
                aspect=aspect,
                seed=seed,
                evaluation=candidate["evaluation"],
                feasibility=feasibility,
            )
        )

    hv_vectors: list[Tuple[float, float]] = []
    for entry in entries:
        if entry.feasibility > _DEFAULT_RELATIVE_TOLERANCE:
            continue
        hv_vectors.append((-entry.gradient, entry.aspect))

    pareto_entries: list[ParetoEntry] = []
    for current_index, entry in enumerate(entries):
        if entry.feasibility > _DEFAULT_RELATIVE_TOLERANCE:
            continue
        point = (entry.gradient, entry.aspect)
        dominated = False
        for other_index, other in enumerate(entries):
            if other_index == current_index:
                continue
            if other.feasibility > _DEFAULT_RELATIVE_TOLERANCE:
                continue
            if _dominates((other.gradient, other.aspect), point):
                dominated = True
                break
        if dominated:
            continue
        pareto_entries.append(
            ParetoEntry(
                design_hash=entry.design_hash,
                seed=entry.seed,
                stage=str(entry.evaluation.get("stage", "")),
                gradient=entry.gradient,
                aspect_ratio=entry.aspect,
                objective=float(entry.evaluation["objective"]),
                feasibility=entry.feasibility,
            )
        )

    pareto_entries.sort(key=lambda item: (-item.gradient, item.aspect_ratio))
    return P3Summary(
        hv_score=_hypervolume_minimization(hv_vectors, reference_point),
        reference_point=reference_point,
        feasible_count=sum(
            1 for entry in entries if entry.feasibility <= _DEFAULT_RELATIVE_TOLERANCE
        ),
        archive_size=len(pareto_entries),
        pareto_entries=tuple(pareto_entries),
    )


def retrieve_rag(
    query: str, *, k: int = 3, index_path: Path | str | None = None
) -> list[dict[str, str]]:
    """Expose RAG retrieval via the ai_scientist/rag_index.db index (Phase 3)."""

    index = Path(index_path) if index_path is not None else rag.DEFAULT_INDEX_PATH
    return rag.retrieve(query=query, k=k, index_path=index)


def make_boundary_from_params(
    params: Mapping[str, Any] | BoundaryParams,
) -> surface_rz_fourier.SurfaceRZFourier:
    """Construct a SurfaceRZFourier boundary from a simple parameter dictionary."""

    params_map = _ensure_mapping(params)
    payload: dict[str, Any] = {
        "r_cos": np.asarray(params_map["r_cos"], dtype=float),
        "z_sin": np.asarray(params_map["z_sin"], dtype=float),
        "is_stellarator_symmetric": bool(
            params_map.get("is_stellarator_symmetric", True)
        ),
        "n_field_periods": int(params_map.get("n_field_periods", 1)),
    }

    if "r_sin" in params_map:
        payload["r_sin"] = np.asarray(params_map["r_sin"], dtype=float)
    if "z_cos" in params_map:
        payload["z_cos"] = np.asarray(params_map["z_cos"], dtype=float)
    if "nfp" in params_map:
        payload.setdefault("n_field_periods", int(params_map["nfp"]))

    return surface_rz_fourier.SurfaceRZFourier(**payload)


def propose_boundary(
    params: Mapping[str, Any] | BoundaryParams,
    *,
    perturbation_scale: float = 0.05,
    seed: int | None = None,
) -> dict[str, Any]:
    """Perturb a given boundary parameter set with random noise."""
    params_map = _ensure_mapping(params)
    rng = np.random.default_rng(seed)
    new_params: dict[str, Any] = {}
    
    for key, value in params_map.items():
        if key in ("r_cos", "z_sin", "r_sin", "z_cos"):
            arr = np.asarray(value, dtype=float)
            noise = rng.normal(scale=perturbation_scale, size=arr.shape)
            new_params[key] = (arr + noise).tolist()
        else:
            new_params[key] = value
            
    # Ensure symmetry constraints if flag is present
    if new_params.get("is_stellarator_symmetric"):
        if "r_cos" in new_params:
            r_cos = np.asarray(new_params["r_cos"])
            if r_cos.ndim > 1:
                center_idx = r_cos.shape[1] // 2
                if center_idx > 0:
                    r_cos[0, :center_idx] = 0.0
                new_params["r_cos"] = r_cos.tolist()
        if "z_sin" in new_params:
            z_sin = np.asarray(new_params["z_sin"])
            z_sin[0, :] = 0.0
            new_params["z_sin"] = z_sin.tolist()
            
    return new_params


def evaluate_p1(
    boundary_params: Mapping[str, Any] | BoundaryParams,
    *,
    stage: str = "screen",
    use_cache: bool = True,
) -> Dict[str, Any]:
    """Run a P1-style evaluation and cache results by stage."""

    def compute(params_map: Mapping[str, Any]) -> Dict[str, Any]:
        boundary = make_boundary_from_params(params_map)
        settings = _settings_for_stage(stage, skip_qi=True)
        metrics, _ = forward_model.forward_model(boundary, settings=settings)
        constraint_margins = compute_constraint_margins(metrics, "p1")
        feasibility = _max_violation(constraint_margins)
        score = 0.0
        if feasibility <= _DEFAULT_RELATIVE_TOLERANCE:
            normalized = _normalize_between_bounds(
                value=metrics.max_elongation, lower_bound=1.0, upper_bound=10.0
            )
            score = 1.0 - normalized

        return {
            "stage": stage.lower(),
            "objective": float(metrics.max_elongation),
            "minimize_objective": True,
            "feasibility": feasibility,
            "score": score,
            "metrics": metrics.model_dump(),
            "settings": settings.model_dump(),
            "constraint_margins": constraint_margins,
            "max_violation": feasibility,
        }

    return _evaluate_cached_stage(
        boundary_params,
        stage=stage,
        compute=compute,
        maximize=False,
        use_cache=use_cache,
    )


def evaluate_p2(
    boundary_params: Mapping[str, Any] | BoundaryParams,
    *,
    stage: str = "p2",
    use_cache: bool = True,
) -> Dict[str, Any]:
    """Run a high-fidelity evaluator for the P2 (QI) problem."""

    def compute(params_map: Mapping[str, Any]) -> Dict[str, Any]:
        boundary = make_boundary_from_params(params_map)
        settings = _settings_for_stage(stage)
        metrics, _ = forward_model.forward_model(boundary, settings=settings)
        constraint_margins = compute_constraint_margins(metrics, "p2")
        feasibility = _max_violation(constraint_margins)
        gradient = float(metrics.minimum_normalized_magnetic_gradient_scale_length)
        score = _gradient_score(metrics)

        return {
            "stage": stage.lower(),
            "objective": gradient,
            "minimize_objective": False,
            "feasibility": feasibility,
            "score": score,
            "hv": float(max(0.0, gradient - 1.0)),
            "metrics": metrics.model_dump(),
            "settings": settings.model_dump(),
            "constraint_margins": constraint_margins,
            "max_violation": feasibility,
        }

    return _evaluate_cached_stage(
        boundary_params,
        stage=stage,
        compute=compute,
        maximize=True,
        use_cache=use_cache,
    )


def evaluate_p3(
    boundary_params: Mapping[str, Any] | BoundaryParams,
    *,
    stage: str = "p3",
    use_cache: bool = True,
) -> Dict[str, Any]:
    """Run a high-fidelity evaluator for the P3 (multi-objective) problem."""

    def compute(params_map: Mapping[str, Any]) -> Dict[str, Any]:
        boundary = make_boundary_from_params(params_map)
        settings = _settings_for_stage(stage)
        metrics, _ = forward_model.forward_model(boundary, settings=settings)
        constraint_margins = compute_constraint_margins(metrics, "p3")
        feasibility = _max_violation(constraint_margins)
        score = _gradient_score(metrics)

        return {
            "stage": stage.lower(),
            "objective": float(metrics.aspect_ratio),
            "minimize_objective": True,
            "feasibility": feasibility,
            "score": score,
            "hv": float(
                max(
                    0.0, metrics.minimum_normalized_magnetic_gradient_scale_length - 1.0
                )
            ),
            "metrics": metrics.model_dump(),
            "settings": settings.model_dump(),
            "constraint_margins": constraint_margins,
            "max_violation": feasibility,
        }

    return _evaluate_cached_stage(
        boundary_params,
        stage=stage,
        compute=compute,
        maximize=False,
        use_cache=use_cache,
    )


def evaluate_p3_set(
    boundary_specs: Sequence[Mapping[str, Any] | BoundaryParams],
    *,
    stage: str = "p3",
    reference_point: Tuple[float, float] = _P3_REFERENCE_POINT,
) -> Dict[str, Any]:
    """Evaluate a batch of P3 boundaries and compute the set-level hypervolume."""

    stage_lower = stage.lower()
    if not boundary_specs:
        return {
            "stage": stage_lower,
            "objectives": [],
            "feasibilities": [],
            "hv_score": 0.0,
            "metrics_list": [],
        }

    evaluations: list[Dict[str, Any]] = []
    hv_vectors: list[Tuple[float, float]] = []

    for candidate in boundary_specs:
        evaluation = evaluate_p3(candidate, stage=stage)
        metrics = evaluation["metrics"]
        feasibility = float(evaluation["feasibility"])

        if feasibility <= _DEFAULT_RELATIVE_TOLERANCE:
            hv_vectors.append(_objective_vector(metrics))

        evaluations.append(evaluation)

    hv_score = _hypervolume_minimization(hv_vectors, reference_point)
    return {
        "stage": stage_lower,
        "objectives": [
            {
                "aspect_ratio": float(eval_["metrics"]["aspect_ratio"]),
                "gradient": float(
                    eval_["metrics"][
                        "minimum_normalized_magnetic_gradient_scale_length"
                    ]
                ),
                "objective": eval_["objective"],
            }
            for eval_ in evaluations
        ],
        "feasibilities": [float(eval_["feasibility"]) for eval_ in evaluations],
        "hv_score": hv_score,
        "metrics_list": [eval_["metrics"] for eval_ in evaluations],
    }


def normalized_constraint_distance_sampler(
    base_designs: Sequence[Mapping[str, Sequence[float] | float]],
    *,
    normalized_distances: Sequence[float],
    proposal_count: int,
    jitter_scale: float = 0.01,
    rng: np.random.Generator | None = None,
    include_distances: bool = False,
) -> list[Mapping[str, float | Sequence[float]]]:
    """Constraint-aware sampler for Task X.6 (docs/TASKS_CODEX_MINI.md:233).

    Designs with smaller normalized constraint distances are preferred so the curriculum
    nudges proposals toward near-feasible regions.
    """

    if proposal_count <= 0:
        return []

    if rng is None:
        rng = np.random.default_rng()

    total_candidates = len(base_designs)
    if total_candidates == 0:
        return []

    distances = np.asarray(normalized_distances, dtype=float)
    if distances.shape[0] != total_candidates:
        raise ValueError("normalized_distances must align with base_designs")

    clipped = np.clip(distances, 0.0, 1.0)
    weights = (1.0 - clipped) + 1e-3
    weights_sum = float(np.sum(weights))
    if weights_sum <= 0.0:
        weights = np.ones_like(weights)
        weights_sum = float(weights.size)

    probabilities = (weights / weights_sum).astype(float)
    chosen_indices = rng.choice(total_candidates, size=proposal_count, p=probabilities)
    proposals: list[Mapping[str, float | Sequence[float]]] = []

    for idx in chosen_indices:
        candidate = base_designs[idx]
        perturbed: dict[str, float | Sequence[float]] = {}
        for key, value in candidate.items():
            array = np.asarray(value, dtype=float)
            jitter = rng.normal(scale=jitter_scale, size=array.shape)
            proposal_array = array + jitter
            if proposal_array.shape == ():
                perturbed[key] = float(proposal_array)
            else:
                perturbed[key] = proposal_array.tolist()
        if include_distances:
            proposals.append(
                {
                    "params": perturbed,
                    "normalized_constraint_distance": float(clipped[idx]),
                }
            )
        else:
            proposals.append(perturbed)

    return proposals


def get_cache_stats(stage: str) -> Mapping[str, int]:
    """Return hit/miss counts for a given stage."""

    return _CACHE_STATS[stage.lower()].copy()


def clear_evaluation_cache() -> None:
    """Reset the P1 evaluation cache and stats (useful for tests)."""

    _EVALUATION_CACHE.clear()
    _CACHE_STATS.clear()


def write_note(
    content: str,
    *,
    filename: str | None = None,
    out_dir: Path | str | None = None,
    world_model: Any | None = None,
    experiment_id: int,
    cycle: int,
    memory_db: str | Path | None = None,
) -> str:
    """Write a literature note to disk and, if context is provided, persist it in the world model."""

    target_dir = Path(out_dir) if out_dir else Path("reports/notes")
    target_dir.mkdir(parents=True, exist_ok=True)

    if not filename:
        digest = hashlib.sha256(content.encode("utf-8")).hexdigest()[:8]
        filename = f"note_{digest}.md"

    path = target_dir / filename
    path.write_text(content, encoding="utf-8")

    target_wm = world_model
    owned = False
    if target_wm is None and memory_db is not None:
        target_wm = memory.WorldModel(memory_db)
        owned = True
    if target_wm is None:
        raise ValueError("write_note requires world_model or memory_db for persistence.")
    try:
        target_wm.log_note(experiment_id=experiment_id, cycle=cycle, content=content)
    except Exception as exc:  # pragma: no cover - safety for agent path
        print(f"[write_note] failed to log note to world_model: {exc}")
    finally:
        if owned:
            target_wm.close()

    return f"Note saved to {path}: {content[:50]}..."


================================================================================
File: __init__.py
================================================================================

import os
import warnings

_suppress = os.getenv("AI_SCIENTIST_SUPPRESS_SIMSOPT_WARN", "1") != "0"
if _suppress:
    # Suppress noisy PendingDeprecationWarning from simsopt importing numpy.matlib.
    warnings.filterwarnings(
        "ignore",
        message="Importing from numpy.matlib is deprecated",
        category=PendingDeprecationWarning,
    )


================================================================================
File: prompts.py
================================================================================

"""Planner prompts grounded in live problem specs + governance docs."""

from __future__ import annotations

from dataclasses import dataclass
from typing import Mapping

from constellaration import problems as problem_module

_SOURCE_PATH = "constellaration/src/constellaration/problems.py"


@dataclass(frozen=True)
class ConstraintSpec:
    name: str
    operator: str
    value: float
    description: str


@dataclass(frozen=True)
class ObjectiveSpec:
    name: str
    direction: str  # "min" | "max"
    description: str


@dataclass(frozen=True)
class ProblemSpec:
    key: str
    constraints: tuple[ConstraintSpec, ...]
    objectives: tuple[ObjectiveSpec, ...]
    score_description: str
    source: str = _SOURCE_PATH

    def prompt_block(self) -> str:
        constraint_lines = "\n".join(
            f"- {c.name} {c.operator} {c.value:g} ({c.description})"
            for c in self.constraints
        )
        objective_lines = "\n".join(
            f"- {obj.direction.upper()} {obj.name}: {obj.description}"
            for obj in self.objectives
        )
        return (
            f"Constraints ({self.source}):\n{constraint_lines}\n"
            f"Objectives:\n{objective_lines}\n"
            f"Scoring: {self.score_description}\n"
        )


def _geometrical_spec() -> ProblemSpec:
    instance = problem_module.GeometricalProblem()
    return ProblemSpec(
        key="p1",
        constraints=(
            ConstraintSpec(
                "aspect_ratio",
                "<=",
                instance._aspect_ratio_upper_bound,
                "controls width vs. height",
            ),
            ConstraintSpec(
                "average_triangularity",
                "<=",
                instance._average_triangularity_upper_bound,
                "keeps indentations bounded toward circular shapes",
            ),
            ConstraintSpec(
                "edge_rotational_transform_over_n_field_periods",
                ">=",
                instance._edge_rotational_transform_over_n_field_periods_lower_bound,
                "ensures sufficient edge winding per field period",
            ),
        ),
        objectives=(
            ObjectiveSpec(
                "max_elongation",
                "min",
                "lower elongation yields more circular, feasible geometries",
            ),
        ),
        score_description=(
            "Normalized max elongation between 1.0 (ideal) and 10.0 (poor)."
        ),
    )


def _simple_qi_spec() -> ProblemSpec:
    instance = problem_module.SimpleToBuildQIStellarator()
    return ProblemSpec(
        key="p2",
        constraints=(
            ConstraintSpec(
                "aspect_ratio",
                "<=",
                instance._aspect_ratio_upper_bound,
                "limits width/height for coilability",
            ),
            ConstraintSpec(
                "edge_rotational_transform_over_n_field_periods",
                ">=",
                instance._edge_rotational_transform_over_n_field_periods_lower_bound,
                "preserves transform per field period",
            ),
            ConstraintSpec(
                "log10(QI residual)",
                "<=",
                instance._log10_qi_upper_bound,
                "bounds quasi-isodynamic residuals for transport",
            ),
            ConstraintSpec(
                "edge_magnetic_mirror_ratio",
                "<=",
                instance._edge_magnetic_mirror_ratio_upper_bound,
                "keeps boundary field variation manageable",
            ),
            ConstraintSpec(
                "max_elongation",
                "<=",
                instance._max_elongation_upper_bound,
                "prevents extreme vertical stretching",
            ),
        ),
        objectives=(
            ObjectiveSpec(
                "minimum_normalized_magnetic_gradient_scale_length",
                "max",
                "higher gradients correlate with easier-to-build coils",
            ),
        ),
        score_description=(
            "Linear score over the minimum normalized magnetic gradient scale length "
            "(0.0 poor → 1.0 optimal)."
        ),
    )


def _mhd_qi_spec() -> ProblemSpec:
    instance = problem_module.MHDStableQIStellarator()
    return ProblemSpec(
        key="p3",
        constraints=(
            ConstraintSpec(
                "edge_rotational_transform_over_n_field_periods",
                ">=",
                instance._edge_rotational_transform_over_n_field_periods_lower_bound,
                "ensures winding per period",
            ),
            ConstraintSpec(
                "log10(QI residual)",
                "<=",
                instance._log10_qi_upper_bound,
                "keeps QI residual small for confinement",
            ),
            ConstraintSpec(
                "edge_magnetic_mirror_ratio",
                "<=",
                instance._edge_magnetic_mirror_ratio_upper_bound,
                "limits mirror ratio at the edge",
            ),
            ConstraintSpec(
                "flux_compression_in_regions_of_bad_curvature",
                "<=",
                instance._flux_compression_in_regions_of_bad_curvature_upper_bound,
                "controls turbulent transport proxy",
            ),
            ConstraintSpec(
                "vacuum_well",
                ">=",
                instance._vacuum_well_lower_bound,
                "maintains ideal-MHD stability margin",
            ),
        ),
        objectives=(
            ObjectiveSpec(
                "minimum_normalized_magnetic_gradient_scale_length",
                "max",
                "larger gradients improve QI performance",
            ),
            ObjectiveSpec(
                "aspect_ratio",
                "min",
                "smaller aspect ratios favor compact machines",
            ),
        ),
        score_description=(
            "Hypervolume of feasible (-gradient, aspect_ratio) points relative to [1.0, 20.0]."
        ),
    )


_PROBLEM_SPECS: Mapping[str, ProblemSpec] = {
    "p1": _geometrical_spec(),
    "p2": _simple_qi_spec(),
    "p3": _mhd_qi_spec(),
}

PHASE_GUIDANCE = (
    "Phase 6 requires computing/archiving the Pareto front + hypervolume each cycle; "
    "Phase 9 acceptance demands feasible P1–P3 designs, reproducible logs, and cited claims "
    "(docs/MASTER_PLAN_AI_SCIENTIST.md)."
)

BUDGET_REMINDER = "Respect Wave B budget guardrails (docs/TASKS_CODEX_MINI.md) when narrating promotions."

TOOL_REMINDER = "Use ai_scientist.tools_api schemas (Wave 8) and cite repositories instead of line numbers."

REPRO_PROMPT = (
    "Each report needs deterministic reproduction steps: git SHAs (repo + constellaration), "
    "seed, fidelity, settings dump, and a rerun command for an archived design."
)

# New constant for physics heuristics
PHYSICS_HEURISTICS_GUIDANCE = (
    "PHYSICS HEURISTICS: Reduce high (m>2, n>2) modes for feasibility; "
    "increase m=1 to boost gradient (warn it raises aspect ratio); "
    "never change n_field_periods."
)

def get_problem_spec(problem: str) -> ProblemSpec:
    key = problem.lower()
    if key not in _PROBLEM_SPECS:
        raise KeyError(f"unknown problem spec '{problem}'")
    return _PROBLEM_SPECS[key]


def build_problem_prompt(problem: str, stage: str) -> str:
    spec = get_problem_spec(problem)
    return (
        f"Planning for {problem.upper()} at stage '{stage}'.\n"
        f"{spec.prompt_block()}"
        f"{PHASE_GUIDANCE}\n"
        f"{BUDGET_REMINDER}\n"
        f"{TOOL_REMINDER}\n"
        f"{REPRO_PROMPT}\n"
        f"{PHYSICS_HEURISTICS_GUIDANCE}\n"
    )


def annotate_solution_summary(summary: str) -> str:
    return (
        f"Summary (tie back to constraints/objectives + cite {_SOURCE_PATH}): {summary}"
    )


================================================================================
File: model_provider.py
================================================================================

"""Helpers for building OpenAI-style calls that match OpenRouter, Moonshot, and StreamLake APIs."""

from __future__ import annotations

import json
import logging
import os
from dataclasses import dataclass
from typing import Any, Mapping, Sequence
from urllib.error import HTTPError, URLError
from urllib.request import Request, urlopen

from ai_scientist.config import ProviderConfig

_LOGGER = logging.getLogger(__name__)
_DEFAULT_TIMEOUT_SECONDS = 60.0


@dataclass(frozen=True)
class ChatRequest:
    path: str
    headers: Mapping[str, str]
    body: Mapping[str, Any]


@dataclass(frozen=True)
class ChatResponse:
    status_code: int
    body: Mapping[str, Any]


def _resolve_auth_header(provider: ProviderConfig) -> str:
    token = os.getenv(provider.auth_env)
    if not token:
        placeholder = provider.auth_env or provider.name.upper()
        token = f"LOCAL-{placeholder}"
    return f"Bearer {token}"


def build_chat_request(
    provider: ProviderConfig,
    tool_call: Mapping[str, Any],
    *,
    messages: Sequence[Mapping[str, str]] | None = None,
    model: str | None = None,
) -> ChatRequest:
    """Return the per-provider path, headers, and body for a chat completion call."""

    headers: dict[str, str] = {
        "Authorization": _resolve_auth_header(provider),
        "Content-Type": "application/json",
    }
    for key, value in provider.extra_headers:
        headers[key] = value
    payload = {
        "model": model or provider.default_model,
        "messages": list(messages)
        if messages
        else [{"role": "user", "content": "tool request"}],
        "tool_call": tool_call,
    }
    _LOGGER.info(
        "Built chat request provider=%s path=%s model=%s tool=%s",
        provider.name,
        provider.chat_path,
        payload["model"],
        tool_call.get("name"),
    )
    return ChatRequest(path=provider.chat_path, headers=headers, body=payload)


def invoke_chat_completion(
    provider: ProviderConfig,
    tool_call: Mapping[str, Any],
    *,
    messages: Sequence[Mapping[str, str]] | None = None,
    model: str | None = None,
    base_url_override: str | None = None,
    timeout: float | None = None,
) -> ChatResponse:
    """Send a chat completion request to the configured provider and return the decoded response."""

    chat_request = build_chat_request(provider, tool_call, messages=messages, model=model)
    base_url = (base_url_override or provider.base_url or "").rstrip("/")
    if not base_url:
        raise ValueError(f"Provider '{provider.name}' is missing a base_url")
    url = f"{base_url}{chat_request.path}"
    data = json.dumps(chat_request.body, separators=(",", ":")).encode("utf-8")
    request = Request(url, data=data, headers=dict(chat_request.headers), method="POST")
    request_timeout = timeout or _DEFAULT_TIMEOUT_SECONDS
    try:
        with urlopen(request, timeout=request_timeout) as response:
            payload = response.read()
            status = getattr(response, "status", response.getcode())
    except HTTPError as exc:  # pragma: no cover - exercised with live providers
        error_body = exc.read().decode("utf-8", "replace") if exc.fp else ""
        message = (
            f"Provider '{provider.name}' returned HTTP {exc.code} for {url}: {error_body}"
        )
        raise RuntimeError(message) from exc
    except URLError as exc:  # pragma: no cover - exercised with live providers
        raise RuntimeError(f"Failed to reach provider '{provider.name}' at {url}: {exc}") from exc

    body_text = payload.decode("utf-8") if payload else "{}"
    try:
        parsed = json.loads(body_text) if body_text.strip() else {}
    except json.JSONDecodeError as exc:  # pragma: no cover - malformed upstream payload
        raise RuntimeError(f"Provider '{provider.name}' returned invalid JSON: {body_text}") from exc
    _LOGGER.info(
        "provider=%s status=%s finish_reason=%s",
        provider.name,
        status,
        parsed.get("choices", [{}])[0].get("finish_reason"),
    )
    return ChatResponse(status_code=status, body=parsed)


__all__ = ["ChatRequest", "ChatResponse", "build_chat_request", "invoke_chat_completion"]


================================================================================
File: rag.py
================================================================================

"""Local retrieval helpers for AI Scientist Wave 3 (RAG)."""

from __future__ import annotations

import os
import re
import sqlite3
from collections import defaultdict
from dataclasses import dataclass
from difflib import SequenceMatcher
from pathlib import Path
from typing import Iterable, List, Sequence

DEFAULT_INDEX_PATH = Path("ai_scientist/rag_index.db")
DEFAULT_INDEX_SOURCES = (
    "docs/papers/2506.19583v1.md",
    "docs/papers/2511.02824v2.md",
    "ConStellaration Fusion Challenge_ Benchmarks and Solution Strategies.md",
    "docs/MASTER_PLAN_AI_SCIENTIST.md",
    "docs/AI_SCIENTIST_PRODUCTION_PLAN.md",
    "docs/AI_SCIENTIST_UPDATED_PLAN.md",
    "docs/AI_SCIENTIST_UNIFIED_ROADMAP.md",
)
INDEX_TABLE_NAME = "rag_references"
META_TABLE_NAME = "rag_index_meta"


@dataclass(frozen=True)
class DocumentChunk:
    source: str
    anchor: str
    chunk: str
    start_line: int
    end_line: int


@dataclass(frozen=True)
class IndexSummary:
    index_path: str
    chunks_indexed: int


@dataclass(frozen=True)
class SourceMeta:
    source: str
    mtime: float
    chunk_count: int


def _tokenize(text: str) -> List[str]:
    tokens = re.findall(r"\b\w+\b", text.lower())
    return [token for token in tokens if len(token) > 1]


def _file_mtime(path: Path) -> float | None:
    try:
        return path.stat().st_mtime
    except OSError:
        return None


def _write_metadata(
    conn: sqlite3.Connection,
    sources: Sequence[str],
    chunk_counts: dict[str, int],
) -> None:
    conn.execute(f"DELETE FROM {META_TABLE_NAME}")
    report_lines = []
    unique_sources = dict.fromkeys(sources)
    for source in unique_sources:
        mtime = _file_mtime(Path(source)) or 0.0
        conn.execute(
            f"INSERT INTO {META_TABLE_NAME} (source, mtime, chunk_count) VALUES (?, ?, ?)",
            (source, mtime, chunk_counts.get(source, 0)),
        )
        report_lines.append(
            f"{source}: {chunk_counts.get(source, 0)} chunks (mtime={mtime:.0f})"
        )
    print(
        "[rag] indexed sources:\n" + "\n".join(f"  - {line}" for line in report_lines)
    )


def _ensure_index(conn: sqlite3.Connection) -> None:
    conn.execute(
        f"""
        CREATE TABLE IF NOT EXISTS {INDEX_TABLE_NAME} (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            source TEXT NOT NULL,
            anchor TEXT,
            chunk TEXT NOT NULL,
            start_line INTEGER NOT NULL,
            end_line INTEGER NOT NULL,
            tokens TEXT NOT NULL
        )
        """
    )
    conn.execute(
        f"""
        CREATE TABLE IF NOT EXISTS {META_TABLE_NAME} (
            source TEXT PRIMARY KEY,
            mtime REAL NOT NULL,
            chunk_count INTEGER NOT NULL
        )
        """
    )


def _iter_document_chunks(source: str, lines: Sequence[str]) -> Iterable[DocumentChunk]:
    anchor = ""
    chunk: List[str] = []
    chunk_start = 1

    def flush(end_line: int) -> DocumentChunk | None:
        if not chunk:
            return None
        text = " ".join(line.strip() for line in chunk if line.strip())
        if not text:
            return None
        return DocumentChunk(
            source=source,
            anchor=anchor,
            chunk=text,
            start_line=chunk_start,
            end_line=end_line,
        )

    for line_number, raw in enumerate(lines, start=1):
        stripped = raw.strip()
        if stripped.startswith("#"):
            anchor = stripped.lstrip("#").strip()
            if chunk:
                flushed = flush(line_number - 1)
                if flushed:
                    chunk.clear()
                    chunk_start = line_number
                    yield flushed
        if not chunk:
            chunk_start = line_number
        chunk.append(raw)
        if stripped == "":
            flushed = flush(line_number)
            if flushed:
                chunk.clear()
                chunk_start = line_number + 1
                yield flushed
    flushed = flush(len(lines))
    if flushed:
        yield flushed


def ensure_index(
    sources: Sequence[str] | None = None,
    index_path: Path | str | None = None,
    *,
    force_rebuild: bool = False,
) -> IndexSummary:
    """Ensure an index exists and reuse it unless a rebuild is requested."""

    index_path = Path(index_path) if index_path is not None else DEFAULT_INDEX_PATH
    sources = tuple(sources or DEFAULT_INDEX_SOURCES)
    env_force = os.environ.get("AI_SCIENTIST_RAG_FORCE_REBUILD", "").lower() in {
        "1",
        "true",
        "yes",
    }
    should_rebuild = force_rebuild or env_force or not index_path.exists()

    if not should_rebuild and index_path.exists():
        conn = sqlite3.connect(index_path)
        conn.row_factory = sqlite3.Row
        try:
            _ensure_index(conn)
            existing_meta = {
                row["source"]: SourceMeta(
                    source=row["source"],
                    mtime=row["mtime"],
                    chunk_count=int(row["chunk_count"]),
                )
                for row in conn.execute(
                    f"SELECT source, mtime, chunk_count FROM {META_TABLE_NAME}"
                )
            }
            needs_rebuild = False
            current_total = 0
            for source in sources:
                path = Path(source)
                meta = existing_meta.get(source)
                if path.exists():
                    current_mtime = _file_mtime(path)
                    if (
                        meta is None
                        or current_mtime is None
                        or meta.mtime != current_mtime
                    ):
                        needs_rebuild = True
                        break
                    current_total += meta.chunk_count
                else:
                    if meta is not None and meta.chunk_count > 0:
                        needs_rebuild = True
                        break
            if not needs_rebuild and current_total > 0:
                print(f"[rag] reusing index {index_path} ({current_total} chunks)")
                return IndexSummary(str(index_path), current_total)
            print("[rag] source changes detected; rebuilding index")
            should_rebuild = True
        finally:
            conn.close()
    if should_rebuild:
        return build_index(sources, index_path)
    return IndexSummary(str(index_path), 0)


def build_index(
    sources: Sequence[str],
    index_path: Path | str | None = None,
) -> IndexSummary:
    """Persist a simple sqlite-backed index derived from Markdown sources."""

    index_path = Path(index_path) if index_path is not None else DEFAULT_INDEX_PATH
    index_path.parent.mkdir(parents=True, exist_ok=True)
    conn = sqlite3.connect(index_path)
    conn.row_factory = sqlite3.Row
    try:
        _ensure_index(conn)
        conn.execute(f"DELETE FROM {INDEX_TABLE_NAME}")
        total_chunks = 0
        chunk_counts: dict[str, int] = defaultdict(int)
        for source in sources:
            path = Path(source)
            if not path.exists():
                continue
            lines = path.read_text(encoding="utf-8").splitlines()
            for chunk in _iter_document_chunks(source, lines):
                tokens = " ".join(_tokenize(chunk.chunk))
                conn.execute(
                    f"INSERT INTO {INDEX_TABLE_NAME} (source, anchor, chunk, start_line, end_line, tokens) VALUES (?, ?, ?, ?, ?, ?)",
                    (
                        chunk.source,
                        chunk.anchor,
                        chunk.chunk,
                        chunk.start_line,
                        chunk.end_line,
                        tokens,
                    ),
                )
                chunk_counts[source] += 1
                total_chunks += 1
        _write_metadata(conn, sources, chunk_counts)
        conn.commit()
    finally:
        conn.close()
    return IndexSummary(str(index_path), total_chunks)


def _fuzzy_similarity(a: str, b: str) -> float:
    if not a or not b:
        return 0.0
    return SequenceMatcher(None, a.lower(), b.lower()).ratio()


def retrieve(
    query: str,
    k: int = 3,
    index_path: Path | str | None = None,
    similarity_weight: float = 0.5,
) -> List[dict[str, str]]:
    """Retrieve the top-k chunks using token overlap + fuzzy matching."""

    index_path = Path(index_path) if index_path is not None else DEFAULT_INDEX_PATH
    if not index_path.exists():
        return []
    tokens = set(_tokenize(query))
    if not tokens:
        return []
    conn = sqlite3.connect(index_path)
    conn.row_factory = sqlite3.Row
    try:
        rows = conn.execute(f"SELECT * FROM {INDEX_TABLE_NAME}").fetchall()
    finally:
        conn.close()
    scored: List[tuple[float, sqlite3.Row]] = []
    for row in rows:
        row_tokens = set(str(row["tokens"]).split())
        overlap_score = len(tokens & row_tokens)
        if overlap_score == 0 and similarity_weight <= 0:
            continue
        similarity_score = _fuzzy_similarity(query, row["chunk"])
        score = overlap_score + similarity_weight * similarity_score
        if score == 0:
            continue
        scored.append((score, row))
    scored.sort(key=lambda item: (-item[0], item[1]["source"], item[1]["start_line"]))
    results: List[dict[str, str]] = []
    for _, row in scored[:k]:
        results.append(
            {
                "source": row["source"],
                "anchor": row["anchor"],
                "chunk": row["chunk"],
                "start_line": str(row["start_line"]),
                "end_line": str(row["end_line"]),
            }
        )
    return results


================================================================================
File: agent.py
================================================================================

"""Client gating helpers so K2-Instruct/K2-Thinking emit valid tool calls (docs/TASKS_CODEX_MINI.md:157-190)."""

from __future__ import annotations

import logging
from dataclasses import dataclass
from typing import Tuple

from ai_scientist.config import ModelConfig, load_model_config
from ai_scientist.tools_api import TOOL_SCHEMA_BY_NAME

_LOGGER = logging.getLogger(__name__)

_ROLE_ALIAS_MAP = {
    "screen": lambda cfg: cfg.instruct_model,
    "short_loop": lambda cfg: cfg.instruct_model,
    "prompt": lambda cfg: cfg.instruct_model,
    "planning": lambda cfg: cfg.thinking_model,
    "report": lambda cfg: cfg.thinking_model,
    "verification": lambda cfg: cfg.thinking_model,
    "literature": lambda cfg: cfg.role_map.get("literature", cfg.thinking_model),
    "analysis": lambda cfg: cfg.role_map.get("analysis", cfg.thinking_model),
}


@dataclass(frozen=True)
class AgentGate:
    model_alias: str
    allowed_tools: Tuple[str, ...]
    system_prompt: str
    provider_model: str

    def allows(self, tool_name: str) -> bool:
        return tool_name in self.allowed_tools


def gates_from_config(config: ModelConfig) -> tuple[AgentGate, ...]:
    return tuple(
        AgentGate(
            model_alias=gate.model_alias,
            allowed_tools=tuple(gate.allowed_tools),
            system_prompt=gate.system_prompt or "",
            provider_model=gate.provider_model or gate.model_alias,
        )
        for gate in config.agent_gates
    )


def gate_for_model(config: ModelConfig, model_alias: str) -> AgentGate | None:
    for gate in gates_from_config(config):
        if gate.model_alias == model_alias:
            return gate
    return None


def _resolve_alias_for_role(role: str | None, config: ModelConfig) -> str:
    normalized = (role or "short_loop").lower()
    resolver = _ROLE_ALIAS_MAP.get(normalized)
    if resolver:
        return resolver(config)
    return config.instruct_model


def provision_model_tier(
    role: str | None = None, *, config: ModelConfig | None = None
) -> AgentGate:
    """Return the AgentGate backing the K2 tier that best fits the requested role."""

    resolved = config or load_model_config()
    alias = _resolve_alias_for_role(role, resolved)
    gate = gate_for_model(resolved, alias)
    if gate is None:
        raise ValueError(
            f"Configured model '{alias}' is not declared in configs/model.yaml"
        )
    _LOGGER.info(
        "Provisioned %s tier for role=%s (base_url=%s, tools=%s)",
        alias,
        (role or "short_loop").lower(),
        resolved.base_url,
        gate.allowed_tools,
    )
    return gate


def validate_tool_call(config: ModelConfig, model_alias: str, tool_name: str) -> None:
    gate = gate_for_model(config, model_alias)
    if gate is None:
        raise ValueError(f"Unknown agent model '{model_alias}'")
    if tool_name not in gate.allowed_tools:
        raise ValueError(
            f"Tool '{tool_name}' is not permitted for {model_alias}; allowed {gate.allowed_tools}"
        )
    if tool_name not in TOOL_SCHEMA_BY_NAME:
        raise ValueError(f"No schema registered for tool '{tool_name}'")


================================================================================
File: tools_api_smoke.py
================================================================================

"""Exercise the OpenAI tool schemas listed in ai_scientist/tools_api.py (Wave 8 checklist in docs/TASKS_CODEX_MINI.md:157-190)."""

from __future__ import annotations

import logging
from typing import Any, Mapping

from ai_scientist.tools_api import list_tool_schemas

_LOGGER = logging.getLogger(__name__)
SAMPLE_PARAMS: Mapping[str, Any] = {
    "r_cos": [[1.5, 0.0], [0.0, 0.05]],
    "z_sin": [[0.0, 0.0], [0.0, 0.05]],
    "n_field_periods": 1,
}


def _build_sample_payload(tool_name: str) -> Mapping[str, Any]:
    if tool_name == "make_boundary":
        return {"params": SAMPLE_PARAMS}
    if tool_name in {"evaluate_p1", "evaluate_p2", "evaluate_p3"}:
        return {
            "params": SAMPLE_PARAMS,
            "problem": tool_name.replace("evaluate_", ""),
            "stage": "screen",
        }
    if tool_name == "log_citation":
        return {
            "source_path": "docs/MASTER_PLAN_AI_SCIENTIST.md",
            "anchor": "Phase 1",
            "quote": "Tiered K2 gate ensures deterministic tooling.",
        }
    if tool_name == "write_report":
        return {
            "title": "Smoke Report",
            "sections": [
                {"heading": "Summary", "body": "This report confirms tool schemas."},
                {
                    "heading": "Next Steps",
                    "body": "Log this output and proceed with Phase 9.",
                },
            ],
            "references": [
                "docs/TASKS_CODEX_MINI.md:157-190",
                "docs/MASTER_PLAN_AI_SCIENTIST.md:247-368",
            ],
        }
    if tool_name == "retrieve_rag":
        return {"query": "Phase 3 planning guidance", "k": 2}
    if tool_name == "write_note":
        return {
            "content": "Smoke note content for tool schema validation.",
            "experiment_id": 0,
            "cycle": 0,
        }
    raise ValueError(f"No smoke sample defined for tool '{tool_name}'")


def _validate_schema(schema: Mapping[str, Any], payload: Mapping[str, Any]) -> None:
    parameters = schema.get("parameters", {})
    required = parameters.get("required", [])
    for field in required:
        if field not in payload:
            raise AssertionError(
                f"Payload for {schema['name']} is missing required field '{field}'"
            )


def run_smoke() -> None:
    """Ensure every tool schema exposes a minimal payload (Phase 1 smoke test)."""

    for schema in list_tool_schemas():
        name = schema["name"]
        sample = _build_sample_payload(name)
        _validate_schema(schema, sample)
        _LOGGER.info("Tool schema '%s' accepts sample payload %s", name, sample)


def smoke_entrypoint() -> None:
    logging.basicConfig(level=logging.INFO)
    run_smoke()


if __name__ == "__main__":
    smoke_entrypoint()


================================================================================
File: test_helpers.py
================================================================================

"""Shared fixtures and helpers reused between ai_scientist tests."""

from constellaration.forward_model import ConstellarationMetrics


def base_params() -> dict[str, list[list[float]] | int | bool]:
    return {
        "r_cos": [[0.0, 0.0, 1.5, 0.0, 0.2], [0.0, 0.0, 0.05, 0.0, 0.1]],
        "z_sin": [[0.0, 0.0, 0.0, 0.05, 0.0], [0.0, 0.0, 0.02, 0.0, 0.0]],
        "n_field_periods": 1,
        "is_stellarator_symmetric": True,
    }


def dummy_metrics() -> ConstellarationMetrics:
    return ConstellarationMetrics(
        aspect_ratio=3.0,
        aspect_ratio_over_edge_rotational_transform=2.0,
        max_elongation=1.2,
        axis_rotational_transform_over_n_field_periods=0.35,
        edge_rotational_transform_over_n_field_periods=0.4,
        axis_magnetic_mirror_ratio=0.7,
        edge_magnetic_mirror_ratio=0.6,
        average_triangularity=-0.3,
        vacuum_well=0.1,
        minimum_normalized_magnetic_gradient_scale_length=0.2,
        qi=1e-5,
        flux_compression_in_regions_of_bad_curvature=0.1,
    )


def dummy_metrics_with(**overrides: float) -> ConstellarationMetrics:
    data = dummy_metrics().model_dump()
    data.update(overrides)
    return ConstellarationMetrics(**data)


================================================================================
File: reporting.py
================================================================================

"""Deterministic Markdown reporting with citations, Pareto figures, and statements (Phase 8/X guidance)."""

from __future__ import annotations

import json
import logging
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Mapping, Sequence

from ai_scientist import rag
from ai_scientist.rag import DEFAULT_INDEX_SOURCES
from ai_scientist.memory import StageHistoryEntry

_LOGGER = logging.getLogger(__name__)
_ALLOWED_REFERENCE_PREFIXES = (
    "docs/",
    "constellaration/",
    "Jr.AI-Scientist/",
    "reports/",
    "tests/",
)
_POSITIONING_SOURCES = DEFAULT_INDEX_SOURCES + (
    "docs/MASTER_PLAN_AI_SCIENTIST.md",
    "docs/TASKS_CODEX_MINI.md",
)


def write_report(title: str, content: str, out_dir: str | Path = "reports") -> Path:
    ts = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
    safe_name = title.replace(" ", "_")
    out_path = Path(out_dir) / f"{ts}_{safe_name}.md"
    out_path.parent.mkdir(parents=True, exist_ok=True)
    header = f"# {title}\n\nGenerated: {ts} UTC\n\n"
    out_path.write_text(header + content)
    return out_path


def validate_references(references: Sequence[str]) -> None:
    if not references:
        raise ValueError(
            "Deterministic reports must cite at least one anchor (docs/ or constellaration/)."
        )
    invalid = [
        ref for ref in references if not ref.startswith(_ALLOWED_REFERENCE_PREFIXES)
    ]
    if invalid:
        raise ValueError(
            "Reference anchors must point to repo docs or known guides, got: %s"
            % invalid
        )


def _statement_value(statement: Any, key: str) -> Any:
    if isinstance(statement, Mapping):
        return statement.get(key)
    return getattr(statement, key)


def _format_statements_table(statements: Sequence[Mapping[str, Any] | Any]) -> str:
    if not statements:
        return "No statements tracked for this cycle."
    lines = [
        "| Stage | Statement | Status | Tool | Seed | Created |",
        "| --- | --- | --- | --- | --- | --- |",
    ]
    for statement in statements:
        stage = _statement_value(statement, "stage") or "unknown"
        text = _statement_value(statement, "text") or ""
        status = _statement_value(statement, "status") or "pending"
        tool_name = _statement_value(statement, "tool_name") or "n/a"
        seed = _statement_value(statement, "seed")
        created_at = _statement_value(statement, "created_at") or "unknown"
        safe_text = str(text).replace("|", "\u007c").replace("\n", " ")
        lines.append(
            f"| {stage.upper()} | {safe_text} | {status} | {tool_name} | {seed or '-'} | {created_at} |"
        )
    return "\n".join(lines)


def _relative_path(path: Path, base_dir: Path) -> str:
    try:
        rel = path.relative_to(base_dir)
    except ValueError:
        rel = path
    return rel.as_posix()


def _format_stage_history_table(stage_history: Sequence[StageHistoryEntry]) -> str:
    if not stage_history:
        return "No governance stage history recorded yet."
    lines = [
        "| Cycle | Stage | Selected At |",
        "| --- | --- | --- |",
    ]
    for entry in stage_history:
        lines.append(f"| {entry.cycle} | {entry.stage.upper()} | {entry.selected_at} |")
    return "\n".join(lines)


def _format_reference_table(references: Sequence[str]) -> str:
    lines = [
        "| Reference |",
        "| --- |",
        *[f"| {reference} |" for reference in references],
    ]
    return "\n".join(lines)


def _format_artifact_table(
    artifact_entries: Sequence[tuple[str, Path]],
    out_dir: Path,
) -> str:
    if not artifact_entries:
        return "No artifacts logged this cycle."
    lines = [
        "| Kind | Path |",
        "| --- | --- |",
    ]
    for kind, path in artifact_entries:
        lines.append(f"| {kind} | {_relative_path(path, out_dir)} |")
    return "\n".join(lines)


def _format_adaptation_figures(
    figures: Sequence[Path],
    out_dir: Path,
) -> list[str]:
    if not figures:
        return ["- No adaptation figures captured for this cycle."]
    lines: list[str] = []
    for figure in figures:
        lines.append(f"- ![{figure.name}]({_relative_path(figure, out_dir)})")
    return lines


def _normalize_quote_text(text: str) -> str:
    return " ".join(text.strip().split())


def _truncate_quote_text(text: str, max_words: int = 25) -> str:
    words = text.split()
    if len(words) <= max_words:
        return text
    return " ".join(words[:max_words]) + " ..."


def _collect_positioning_quotes(
    *,
    min_quotes: int = 3,
    queries: Sequence[str] | None = None,
) -> list[dict[str, str]]:
    queries = queries or (
        "hypervolume baseline acceptance",
        "pareto archives vs baseline story",
        "Task X.4 related work rewrite positioning",
    )
    rag.ensure_index(sources=_POSITIONING_SOURCES)
    seen: set[tuple[str, str, str]] = set()
    quotes: list[dict[str, str]] = []
    for query in queries:
        for chunk in rag.retrieve(query, k=3):
            key = (chunk["source"], chunk["start_line"], chunk["end_line"])
            if key in seen:
                continue
            seen.add(key)
            text = _normalize_quote_text(chunk["chunk"])
            if not text:
                continue
            final_text = _truncate_quote_text(text)
            quotes.append(
                {
                    "text": final_text,
                    "source": chunk["source"],
                    "start_line": chunk["start_line"],
                    "end_line": chunk["end_line"],
                }
            )
            if len(quotes) >= min_quotes:
                return quotes[:min_quotes]
    return quotes


def _format_property_graph_section(
    summary: Mapping[str, Any] | None,
    rag_citations: Sequence[Mapping[str, Any]] | None = None,
) -> list[str]:
    if summary is None:
        return ["- PropertyGraph summary unavailable (no experiment context)."]
    lines = [
        f"- Nodes: {summary.get('node_count', 0)}",
        f"- Edges: {summary.get('edge_count', 0)}",
        f"- Citations tracked: {summary.get('citation_count', 0)}",
    ]
    citations = rag_citations or summary.get("citations") or []
    if citations:
        lines.append("- RAG citations:")
        for citation in citations:
            source = citation.get("source_path") or "unknown"
            anchor = citation.get("anchor") or ""
            quote = citation.get("quote") or ""
            anchor_display = f"{source}:{anchor}" if anchor else source
            lines.append(f"  - {anchor_display} — {quote}")
    return lines


def _build_positioning_section(
    p3_summary: Mapping[str, Any],
    *,
    positioning_artifacts: Mapping[str, str] | None = None,
) -> list[str]:
    hv_score = p3_summary.get("hv_score")
    archive_size = p3_summary.get("archive_size")
    positioning_lines: list[str] = ["### Positioning vs baselines"]
    hv_line = (
        f"- Current HV: {hv_score:.6f}, tracking the positive-delta expectation from the master plan."
        if hv_score is not None
        else "- Current HV: n/a"
    )
    archive_line = (
        f"- Pareto archive size: {archive_size} entries, so we can document the front the master plan wants us to guard."
        if archive_size is not None
        else "- Pareto archive: unknown count"
    )
    positioning_lines.extend([hv_line, archive_line])
    artifact_lines: list[str] = []
    if positioning_artifacts:
        anchors = ", ".join(
            anchor
            for anchor in (
                positioning_artifacts.get("preference_pairs"),
                positioning_artifacts.get("p3_summary"),
                positioning_artifacts.get("trajectory"),
            )
            if anchor
        )
        if anchors:
            artifact_lines.append(
                f"- RLAIF evidence anchored at {anchors}; the master plan calls this linkage out in docs/MASTER_PLAN_AI_SCIENTIST.md:226-247 "
                "and docs/TASKS_CODEX_MINI.md:238 so reviewers can trace the HV claim."
            )
    if artifact_lines:
        positioning_lines.extend(artifact_lines)
    quotes = _collect_positioning_quotes()
    if not quotes:
        positioning_lines.append(
            "- Baseline quotes could not be retrieved; ensure the RAG index is built and rerun the report."
        )
        return positioning_lines
    for quote in quotes:
        citation = f"{quote['source']}:{quote['start_line']}-{quote['end_line']}"
        positioning_lines.append(f"> {quote['text']} [{citation}]")
    return positioning_lines


def save_pareto_figure(
    pareto_entries: Sequence[Any],
    out_dir: str | Path,
    *,
    title: str,
    cycle_index: int,
) -> Path | None:
    if not pareto_entries:
        return None
    try:
        import matplotlib.pyplot as plt
    except ImportError:  # pragma: no cover - optional dependency
        _LOGGER.warning(
            "matplotlib not available, skipping Pareto figure for cycle %s",
            cycle_index + 1,
        )
        return None
    gradients: list[float] = [entry.gradient for entry in pareto_entries]
    aspects: list[float] = [entry.aspect_ratio for entry in pareto_entries]
    fig, ax = plt.subplots(figsize=(5, 4))
    ax.scatter(gradients, aspects, c="tab:purple", edgecolor="black")
    ax.set_xlabel("Minimum normalized gradient")
    ax.set_ylabel("Aspect ratio")
    ax.set_title(f"Pareto cycle {cycle_index + 1}: {title}")
    ax.grid(True, linestyle="--", linewidth=0.5)
    figure_dir = Path(out_dir) / "figures"
    figure_dir.mkdir(parents=True, exist_ok=True)
    safe_title = "".join(c if c.isalnum() else "_" for c in title)[:32]
    file_name = f"pareto_cycle_{cycle_index + 1}_{safe_title}.png"
    out_path = figure_dir / file_name
    fig.tight_layout()
    fig.savefig(out_path, dpi=120)
    plt.close(fig)
    return out_path


def build_cycle_report(
    *,
    cycle_index: int,
    problem: str,
    screened: int,
    promoted: int,
    governance_stage: str,
    best_metrics: Mapping[str, Any],
    config_snapshot: Mapping[str, Any],
    reproduction_steps: Sequence[str],
    reproduction_snippet: str,
    environment_block: str,
    pareto_lines: str,
    p3_summary: Mapping[str, Any],
    positioning_artifacts: Mapping[str, str] | None = None,
    statements: Sequence[Mapping[str, Any] | Any],
    references: Sequence[str],
    figure_paths: Sequence[Path],
    stage_history: Sequence[StageHistoryEntry],
    artifact_entries: Sequence[tuple[str, Path]],
    adaptation_figures: Sequence[Path],
    property_graph_summary: Mapping[str, Any] | None = None,
    rag_citations: Sequence[Mapping[str, Any]] | None = None,
    out_dir: str | Path = "reports",
) -> str:
    validate_references(references)
    base_dir = Path(out_dir)
    summary_lines = [
        f"- Screened: {screened}",
        f"- Promoted: {promoted}",
        f"- Governance stage: {governance_stage.upper()}",
    ]
    reproduction_lines = "\n".join(
        f"{idx + 1}. {step}" for idx, step in enumerate(reproduction_steps)
    )
    config_block = json.dumps(config_snapshot, indent=2)
    best_metrics_block = json.dumps(best_metrics, indent=2)
    pareto_block = f"#### Non-dominated front\n{pareto_lines}"
    figure_section = []
    for figure in figure_paths:
        try:
            rel = figure.relative_to(Path(out_dir))
        except ValueError:
            rel = figure
        figure_section.append(f"- ![{figure.name}]({rel.as_posix()})")
    if not figure_section:
        figure_section.append("- No Pareto figures generated for this cycle.")
    statement_table = _format_statements_table(statements)
    stage_table = _format_stage_history_table(stage_history)
    artifact_table = _format_artifact_table(artifact_entries, base_dir)
    adaptation_lines = _format_adaptation_figures(adaptation_figures, base_dir)
    citation_table = _format_reference_table(references)
    citation_status = "Citation validation: PASS (all anchors resolve to repo docs)."
    hv_score = p3_summary.get("hv_score")
    reference_point = p3_summary.get("reference_point")
    hv_lines = [
        f"- Hypervolume: {hv_score:.6f}"
        if hv_score is not None
        else "- Hypervolume: n/a",
        f"- Reference point: {reference_point}"
        if reference_point
        else "- Reference point: unknown",
        f"- Feasible evaluations: {p3_summary.get('feasible_count', 0)}",
        f"- Pareto archive size: {p3_summary.get('archive_size', 0)}",
    ]
    positioning_section = _build_positioning_section(
        {
            "hv_score": hv_score,
            "archive_size": p3_summary.get("archive_size"),
        },
        positioning_artifacts=positioning_artifacts,
    )
    graph_lines = _format_property_graph_section(
        property_graph_summary, rag_citations=rag_citations
    )
    document = [
        f"## Cycle {cycle_index + 1}",
        f"- Problem: {problem}",
        *summary_lines,
        "",  # blank line
        "### Best candidate metrics",
        f"```json\n{best_metrics_block}\n```",
        "### Config snapshot",
        f"```json\n{config_block}\n```",
        "### Reproduction",
        reproduction_lines,
        reproduction_snippet,
        "### Environment",
        environment_block,
        "### Stage history",
        stage_table,
        "### PropertyGraph",
        *graph_lines,
        "### Phase 6 / P3 summary",
        *hv_lines,
        *positioning_section,
        pareto_block,
        "### Pareto figures",
        *figure_section,
        "### Adaptation figures",
        *adaptation_lines,
        "### Artifacts",
        artifact_table,
        "### Statements",
        statement_table,
        "### Citations",
        citation_status,
        citation_table,
        "### References for governance",
        "- docs/TASKS_CODEX_MINI.md:200-248",
        "- docs/MASTER_PLAN_AI_SCIENTIST.md:247-368",
    ]
    return "\n".join(str(line) for line in document)


def collect_adaptation_figures(out_dir: str | Path = "reports") -> list[Path]:
    base_dir = Path(out_dir)
    figure_dir = base_dir / "adaptation" / "figures"
    if not figure_dir.exists():
        return []
    collected: list[Path] = []
    for pattern in ("*.png", "*.svg", "*.jpg", "*.jpeg"):
        collected.extend(sorted(figure_dir.glob(pattern)))
    return sorted(collected)


def export_metrics_to_prometheus_textfile(metrics: Mapping[str, Any], file_path: Path) -> None:
    try:
        from prometheus_client import CollectorRegistry, Gauge, write_to_textfile
    except ImportError:
        _LOGGER.warning("prometheus_client not installed, skipping metrics export")
        return

    registry = CollectorRegistry()
    for key, value in metrics.items():
        if isinstance(value, (int, float)):
            safe_key = key.replace(" ", "_").replace("-", "_").lower()
            g = Gauge(f"ai_scientist_{safe_key}", f"Metric: {key}", registry=registry)
            g.set(value)
    
    write_to_textfile(str(file_path), registry)


================================================================================
File: optim/__init__.py
================================================================================

"""Optimization helpers (skeleton)."""

from . import search as _search
from . import surrogate as _surrogate

search = _search
surrogate = _surrogate

__all__ = ["search", "surrogate"]


================================================================================
File: optim/surrogate.py
================================================================================

"""Surrogate helpers for candidate screening and feasibility-aware ranking.

Per the unified roadmap (docs/AI_SCIENTIST_UNIFIED_ROADMAP.md) the production
surrogate is a bundled vectorizer + scaler + RF classifier/regressor that
estimates feasibility and objective jointly. The legacy linear ranker remains
for backward compatibility, but SurrogateBundle is the default path going
forward.
"""

from __future__ import annotations

import logging
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
from typing import Any, Mapping, Sequence

import numpy as np
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.preprocessing import StandardScaler

from ai_scientist import memory, tools


def _flatten_boundary_params(params: Mapping[str, Any]) -> np.ndarray:
    values: list[np.ndarray] = []
    for key in ("r_cos", "z_sin"):
        payload = params.get(key)
        if payload is None:
            continue
        arr = np.asarray(payload, dtype=float).ravel()
        if arr.size:
            values.append(arr)
    if not values:
        return np.zeros((0,), dtype=float)
    return np.concatenate(values)


def _params_feature_vector(params: Mapping[str, Any]) -> np.ndarray:
    flattened = _flatten_boundary_params(params)
    if flattened.size == 0:
        return np.zeros((2,), dtype=float)
    return np.array([float(np.sum(flattened)), float(flattened.size)], dtype=float)


@dataclass(frozen=True)
class SurrogateRank:
    """Surrogate score + reference for a candidate."""

    score: float
    metrics: Mapping[str, Any]


class SimpleSurrogateRanker:
    """Surrogate ranker that uses a ridge-learned model instead of heuristics."""

    def __init__(self, *, alpha: float = 1e-2) -> None:
        self._alpha = float(alpha)
        self._feature_weights: np.ndarray | None = None
        self._bias: float = 0.0

    def _feature_vector(self, metrics: Mapping[str, Any]) -> np.ndarray:
        params = metrics.get("candidate_params")
        if isinstance(params, Mapping):
            return _params_feature_vector(params)
        gradient = float(
            metrics.get("minimum_normalized_magnetic_gradient_scale_length", 0.0)
        )
        aspect = float(metrics.get("aspect_ratio", 0.0))
        hv = float(metrics.get("hv", gradient - aspect))
        return np.array([gradient, aspect, hv, gradient - aspect], dtype=float)

    def fit(
        self,
        metrics_list: Sequence[Mapping[str, Any]],
        target_values: Sequence[float],
    ) -> None:
        if not metrics_list:
            raise ValueError("training data cannot be empty")
        if len(metrics_list) != len(target_values):
            raise ValueError("metrics and targets must be the same length")
        features = np.vstack([self._feature_vector(m) for m in metrics_list])
        targets = np.asarray(target_values, dtype=float)
        intercept = np.ones((features.shape[0], 1), dtype=float)
        design = np.hstack([features, intercept])
        reg = np.eye(design.shape[1], dtype=float)
        reg[-1, -1] = 0.0
        xtx = design.T @ design + self._alpha * reg
        xty = design.T @ targets
        weights = np.linalg.solve(xtx, xty)
        self._feature_weights = weights[:-1]
        self._bias = float(weights[-1])

    def fit_from_world_model(
        self,
        world_model: memory.WorldModel,
        *,
        target_column: str = "hv",
        problem: str | None = None,
    ) -> None:
        history = world_model.surrogate_training_data(
            target=target_column, problem=problem
        )
        if not history:
            raise ValueError("insufficient history for surrogate training")
        metrics_list, targets = zip(*history)
        self.fit(metrics_list, targets)

    def _ensure_trained(self) -> None:
        if self._feature_weights is None:
            raise RuntimeError("surrogate ranker has not been trained")

    def rank(self, metrics_list: Sequence[Mapping[str, Any]]) -> list[SurrogateRank]:
        self._ensure_trained()
        assert self._feature_weights is not None
        ranked: list[SurrogateRank] = []
        weights = self._feature_weights
        for metrics in metrics_list:
            features = self._feature_vector(metrics)
            score = float(np.dot(features, weights) + self._bias)
            ranked.append(SurrogateRank(score=score, metrics=metrics))
        return sorted(ranked, key=lambda item: item.score, reverse=True)


@dataclass(frozen=True)
class SurrogatePrediction:
    """Bundle of surrogate scores for a single candidate."""

    expected_value: float
    prob_feasible: float
    predicted_objective: float
    minimize_objective: bool
    metadata: Mapping[str, Any]


class SurrogateBundle:
    """Feasibility-first surrogate bundle with RF heads and structured features.

    - Vectorizes boundary params with tools.structured_flatten using a persisted
      schema (mpol, ntor, schema_version, rounding).
    - Standard-scales the flattened vector.
    - Fits a RF classifier for feasibility probability and a RF regressor for
      objective/HV. PyTorch ensemble is intentionally kept behind the
      `enable_torch` flag but not initialized here.
    - Training policy: fit only when >= `min_samples`; otherwise emit a cold
      start log and keep caller order. The regressor trains on feasible-only
      points when there are >= `min_feasible_for_regressor` feasible rows.
    - Ranking uses E[value] = P(feasible) * corrected_objective where the sign
      is inverted for minimize problems so that higher is always better.
    - Fit/predict run inside a timeout guard to avoid surprises in CI.
    - Retrain cadence: either `points_cadence` new rows or `cycle_cadence`
      elapsed cycles since the last fit.
    """

    def __init__(
        self,
        *,
        min_samples: int = 8,
        min_feasible_for_regressor: int = 4,
        feasibility_tolerance: float = tools._DEFAULT_RELATIVE_TOLERANCE,
        timeout_seconds: float = 1.0,
        points_cadence: int = 16,
        cycle_cadence: int = 1,
        enable_torch: bool = False,
    ) -> None:
        self._min_samples = int(min_samples)
        self._min_feasible_for_regressor = int(min_feasible_for_regressor)
        self._feasibility_tolerance = float(feasibility_tolerance)
        self._timeout_seconds = float(timeout_seconds)
        self._points_cadence = int(points_cadence)
        self._cycle_cadence = int(cycle_cadence)
        self._enable_torch = bool(enable_torch)

        self._scaler: StandardScaler | None = None
        self._classifier: RandomForestClassifier | None = None
        self._regressor: RandomForestRegressor | None = None
        self._schema: tools.FlattenSchema | None = None
        self._trained = False
        self._last_fit_count = 0
        self._last_fit_cycle = 0

    def _with_timeout(self, func):
        with ThreadPoolExecutor(max_workers=1) as pool:
            future = pool.submit(func)
            return future.result(timeout=self._timeout_seconds)

    def _vectorize(self, params: Mapping[str, Any]) -> np.ndarray:
        vector, schema = tools.structured_flatten(
            params, schema=self._schema
        )
        if self._schema is None:
            self._schema = schema
        return vector

    def _feature_matrix(self, metrics_list: Sequence[Mapping[str, Any]]) -> np.ndarray:
        vectors: list[np.ndarray] = []
        for metrics in metrics_list:
            params = metrics.get("candidate_params") or metrics.get("params")
            if not isinstance(params, Mapping):
                params = {}
            vectors.append(self._vectorize(params))
        if not vectors:
            return np.zeros((0, 0), dtype=float)
        return np.vstack(vectors)

    def _label_feasibility(self, metrics_list: Sequence[Mapping[str, Any]]) -> np.ndarray:
        labels: list[int] = []
        for metrics in metrics_list:
            feasibility = metrics.get("feasibility")
            if feasibility is None:
                feasibility = metrics.get("max_violation", float("inf"))
            labels.append(int(float(feasibility) <= self._feasibility_tolerance))
        return np.asarray(labels, dtype=int)

    def fit(
        self,
        metrics_list: Sequence[Mapping[str, Any]],
        target_values: Sequence[float],
        *,
        minimize_objective: bool,
        cycle: int | None = None,
    ) -> None:
        sample_count = len(metrics_list)
        self._last_fit_count = sample_count
        if cycle is not None:
            self._last_fit_cycle = int(cycle)

        if sample_count < self._min_samples:
            logging.info("[surrogate] cold start: %d samples (< %d)", sample_count, self._min_samples)
            self._trained = False
            return

        features = self._feature_matrix(metrics_list)
        if features.size == 0:
            self._trained = False
            return

        feasibility_labels = self._label_feasibility(metrics_list)
        regression_targets = np.asarray(target_values, dtype=float)

        feasible_mask = feasibility_labels == 1
        if np.count_nonzero(feasible_mask) >= self._min_feasible_for_regressor:
            features_reg = features[feasible_mask]
            regression_targets = regression_targets[feasible_mask]
        else:
            features_reg = features

        # Align classifier features to all rows; regressor uses features_reg.
        def _train_bundle() -> None:
            scaler = StandardScaler()
            scaled_class = scaler.fit_transform(features)
            clf = RandomForestClassifier(
                n_estimators=12,
                max_depth=6,
                random_state=0,
                n_jobs=1,
            )
            clf.fit(scaled_class, feasibility_labels)

            scaled_reg = scaler.transform(features_reg)
            reg = RandomForestRegressor(
                n_estimators=12,
                max_depth=6,
                random_state=0,
                n_jobs=1,
            )
            reg.fit(scaled_reg, regression_targets)

            self._scaler = scaler
            self._classifier = clf
            self._regressor = reg
            self._trained = True

        self._with_timeout(_train_bundle)

    def should_retrain(self, sample_count: int, cycle: int | None = None) -> bool:
        if not self._trained:
            return True
        delta_points = sample_count - self._last_fit_count
        if delta_points >= self._points_cadence:
            return True
        if cycle is None:
            return False
        return (cycle - self._last_fit_cycle) >= self._cycle_cadence

    def _predict_batch(self, feature_matrix: np.ndarray) -> tuple[np.ndarray, np.ndarray]:
        assert self._classifier is not None
        assert self._regressor is not None
        assert self._scaler is not None

        def _predict() -> tuple[np.ndarray, np.ndarray]:
            scaled = self._scaler.transform(feature_matrix)
            prob = self._classifier.predict_proba(scaled)[:, 1]
            preds = self._regressor.predict(scaled)
            return prob, preds

        return self._with_timeout(_predict)

    def _expected_value(
        self, prob_feasible: float, predicted_objective: float, minimize_objective: bool
    ) -> float:
        oriented = -float(predicted_objective) if minimize_objective else float(predicted_objective)
        return float(prob_feasible) * oriented

    def rank_candidates(
        self,
        candidates: Sequence[Mapping[str, Any]],
        *,
        minimize_objective: bool,
        exploration_ratio: float = 0.0,
    ) -> list[SurrogatePrediction]:
        if not candidates:
            return []

        exploration_weight = max(0.0, float(exploration_ratio)) * 0.1

        if not self._trained:
            logging.info("[surrogate] cold start ranking; using heuristic features")
            cold_ranks: list[SurrogatePrediction] = []
            for candidate in candidates:
                params = candidate.get("candidate_params") or candidate.get("params", {})
                features = _params_feature_vector(params)
                base_score = float(features[0]) if features.size else 0.0
                score = -base_score if minimize_objective else base_score
                cold_ranks.append(
                    SurrogatePrediction(
                        expected_value=score,
                        prob_feasible=0.0,
                        predicted_objective=base_score,
                        minimize_objective=minimize_objective,
                        metadata=candidate,
                    )
                )
            return sorted(cold_ranks, key=lambda item: item.expected_value, reverse=True)

        metrics_list: list[Mapping[str, Any]] = []
        for candidate in candidates:
            params = candidate.get("candidate_params")
            if params is None:
                params = candidate.get("params", {})
            metrics_list.append({"candidate_params": params})
        feature_matrix = self._feature_matrix(metrics_list)
        prob, preds = self._predict_batch(feature_matrix)

        ranked: list[SurrogatePrediction] = []
        for candidate, pf, obj in zip(candidates, prob, preds):
            constraint_distance = float(candidate.get("constraint_distance", 0.0))
            constraint_distance = max(0.0, constraint_distance)
            uncertainty = float(pf * (1.0 - pf))
            base_score = self._expected_value(pf, obj, minimize_objective)
            score = (float(pf) - constraint_distance) + exploration_weight * uncertainty
            # Preserve expected_value semantics for downstream consumers.
            score = score if self._trained else base_score
            ranked.append(
                SurrogatePrediction(
                    expected_value=score,
                    prob_feasible=float(pf),
                    predicted_objective=float(obj),
                    minimize_objective=minimize_objective,
                    metadata=candidate,
                )
            )

        return sorted(ranked, key=lambda item: item.expected_value, reverse=True)


================================================================================
File: optim/search.py
================================================================================

"""Wave 7 search wrappers for P3 candidate generation and ranking.

These helpers follow the gating notes in docs/MASTER_PLAN_AI_SCIENTIST.md:205 and
docs/TASKS_CODEX_MINI.md:145, where the Wave 7 DoD requires a structured
``search wrapper`` kernel (Nelder–Mead or CMA-ES) that still calls
`tools.evaluate_p3_set` for HV-aware scoring.
"""

from __future__ import annotations

from dataclasses import dataclass
from itertools import cycle
from typing import Any, Literal, Mapping, Sequence

import numpy as np

from ai_scientist import tools

_Method = Literal["nelder_mead", "cma_es", "nsga2", "ngopt"]


@dataclass(frozen=True)
class BatchSummary:
    """Minimal record of a candidate batch tied to P3 evaluations."""

    stage: str
    hv_score: float
    objectives: Sequence[Mapping[str, Any]]


@dataclass(frozen=True)
class _ParamSlot:
    path: tuple[str, ...]
    shape: tuple[int, ...]
    length: int


class _ParameterVectorizer:
    """Simple flatten/unflatten helper for the active parameter dict."""

    def __init__(self, prototype: Mapping[str, Any]) -> None:
        self._slots: list[_ParamSlot] = []
        self._dim = 0
        self._collect_slots(prototype, ())
        self._reference = self.flatten(prototype)

    def _collect_slots(self, node: Any, path: tuple[str, ...]) -> None:
        if isinstance(node, Mapping):
            for key, value in node.items():
                self._collect_slots(value, path + (key,))
            return
        array = np.asarray(node, dtype=float)
        shape = () if array.ndim == 0 else array.shape
        length = int(array.size)
        self._slots.append(_ParamSlot(path, shape, length))
        self._dim += length

    def _get_by_path(self, params: Mapping[str, Any], path: tuple[str, ...]) -> Any:
        node: Any = params
        for key in path:
            node = node[key]
        return node

    def _set_by_path(
        self, target: dict[str, Any], path: tuple[str, ...], value: Any
    ) -> None:
        node = target
        for key in path[:-1]:
            if key not in node or not isinstance(node[key], dict):
                node[key] = {}
            node = node[key]
        node[path[-1]] = value

    @property
    def dim(self) -> int:
        return self._dim

    @property
    def reference(self) -> np.ndarray:
        return self._reference.copy()

    def flatten(self, params: Mapping[str, Any]) -> np.ndarray:
        if self._dim == 0:
            return np.zeros(0, dtype=float)
        vector = np.empty(self._dim, dtype=float)
        offset = 0
        for slot in self._slots:
            raw_value = self._get_by_path(params, slot.path)
            array = np.asarray(raw_value, dtype=float)
            if slot.shape:
                array = array.reshape(slot.shape)
            else:
                array = array.reshape(())
            length = slot.length
            vector[offset : offset + length] = array.reshape(-1)
            offset += length
        return vector

    def unflatten(self, vector: Sequence[float] | np.ndarray) -> dict[str, Any]:
        if self._dim == 0:
            return {}
        params: dict[str, Any] = {}
        offset = 0
        arr = np.asarray(vector, dtype=float)
        for slot in self._slots:
            segment = arr[offset : offset + slot.length]
            offset += slot.length
            value: Any
            if slot.shape:
                value = segment.reshape(slot.shape).tolist()
            else:
                value = float(segment[0])
            self._set_by_path(params, slot.path, value)
        return params


class P3SearchWrapper:
    """Generate and score P3 proposals using the HV-aware set evaluator.

    The wrapper now follows the Wave 7 optimizer kernel mandate in
    docs/MASTER_PLAN_AI_SCIENTIST.md:331 and docs/TASKS_CODEX_MINI.md:145 by
    flattening the parameter dictionary and emitting either a Nelder–Mead
    simplex or CMA-ES-style batch before the HV surrogate ranking step.
    """

    def __init__(
        self,
        base_params: Mapping[str, Any],
        *,
        perturbation_scale: float = 0.05,
        method: _Method = "nelder_mead",
    ) -> None:
        self._vectorizer = _ParameterVectorizer(base_params)
        self._method = method
        self._scale = float(perturbation_scale)
        self._dim = self._vectorizer.dim
        self._mean = self._vectorizer.flatten(base_params)
        self._simplex = self._build_simplex(self._mean)
        self._sigma = max(self._scale, 1e-4)
        self._best_score = float("-inf")

    def _build_simplex(self, center: np.ndarray) -> list[np.ndarray]:
        if self._dim == 0:
            return [center.copy()]
        simplex: list[np.ndarray] = [center.copy()]
        for axis in range(self._dim):
            direction = np.zeros(self._dim, dtype=float)
            direction[axis] = self._scale
            simplex.append(center + direction)
        return simplex

    def propose_candidates(
        self, batch_size: int, seed: int | None = None
    ) -> list[Mapping[str, Any]]:
        """Generate a structured batch of proposals anchored on the current mean."""

        if batch_size <= 0:
            return []
        rng = np.random.default_rng(seed)
        proposals: list[Mapping[str, Any]] = []
        if self._dim == 0:
            base = self._vectorizer.unflatten(self._mean)
            return [base for _ in range(batch_size)]

        if self._method == "cma_es":
            for _ in range(batch_size):
                delta = rng.normal(scale=self._sigma, size=self._dim)
                proposals.append(self._vectorizer.unflatten(self._mean + delta))
            return proposals

        simplex_iter = cycle(self._simplex)
        while len(proposals) < batch_size:
            vertex = next(simplex_iter)
            if len(proposals) < len(self._simplex):
                proposals.append(self._vectorizer.unflatten(vertex))
            else:
                jitter = rng.uniform(-self._scale, self._scale, size=self._dim)
                proposals.append(self._vectorizer.unflatten(self._mean + jitter))
        return proposals[:batch_size]

    def evaluate_batch(self, candidates: Sequence[Mapping[str, Any]]) -> BatchSummary:
        """Score the proposals using the P3 HV-aware set evaluator."""

        evaluation = tools.evaluate_p3_set(candidates)
        return BatchSummary(
            stage=evaluation["stage"],
            hv_score=float(evaluation["hv_score"]),
            objectives=tuple(evaluation["objectives"]),
        )

    def _score_metrics(self, metrics: Mapping[str, Any]) -> float:
        gradient = float(metrics["minimum_normalized_magnetic_gradient_scale_length"])
        aspect = float(metrics["aspect_ratio"])
        return gradient - aspect

    def _crowding_distance(
        self, fronts: list[list[int]], objectives: list[tuple[float, float]]
    ) -> list[float]:
        distances = [0.0 for _ in objectives]
        for front in fronts:
            if len(front) <= 2:
                for idx in front:
                    distances[idx] = float("inf")
                continue
            grad_sorted = sorted(front, key=lambda idx: objectives[idx][0], reverse=True)
            aspect_sorted = sorted(front, key=lambda idx: objectives[idx][1])
            grad_values = [objectives[idx][0] for idx in grad_sorted]
            aspect_values = [objectives[idx][1] for idx in aspect_sorted]
            grad_span = max(max(grad_values) - min(grad_values), 1e-9)
            aspect_span = max(max(aspect_values) - min(aspect_values), 1e-9)
            distances[grad_sorted[0]] = float("inf")
            distances[grad_sorted[-1]] = float("inf")
            distances[aspect_sorted[0]] = float("inf")
            distances[aspect_sorted[-1]] = float("inf")
            for pos in range(1, len(grad_sorted) - 1):
                prev_val = objectives[grad_sorted[pos - 1]][0]
                next_val = objectives[grad_sorted[pos + 1]][0]
                distances[grad_sorted[pos]] += abs(next_val - prev_val) / grad_span
            for pos in range(1, len(aspect_sorted) - 1):
                prev_val = objectives[aspect_sorted[pos - 1]][1]
                next_val = objectives[aspect_sorted[pos + 1]][1]
                distances[aspect_sorted[pos]] += abs(next_val - prev_val) / aspect_span
        return distances

    def _nondominated_fronts(
        self, objectives: list[tuple[float, float]], feasibilities: list[float]
    ) -> list[list[int]]:
        feasible_idx = [
            idx
            for idx, feas in enumerate(feasibilities)
            if feas <= tools._DEFAULT_RELATIVE_TOLERANCE
        ]
        if not feasible_idx:
            return [list(range(len(objectives)))]
        fronts: list[list[int]] = []
        remaining = set(feasible_idx)
        while remaining:
            front: list[int] = []
            for idx in list(remaining):
                dominates_any = False
                dominated = False
                for other in list(remaining):
                    if idx == other:
                        continue
                    grad, aspect = objectives[idx]
                    o_grad, o_aspect = objectives[other]
                    if (o_grad >= grad and o_aspect <= aspect) and (
                        o_grad > grad or o_aspect < aspect
                    ):
                        dominated = True
                        break
                    if (grad >= o_grad and aspect <= o_aspect) and (
                        grad > o_grad or aspect < o_aspect
                    ):
                        dominates_any = True
                if not dominated:
                    front.append(idx)
            if not front:
                break
            fronts.append(front)
            remaining -= set(front)
        return fronts if fronts else [list(range(len(objectives)))]

    def _update_state(
        self, best_vector: np.ndarray, hv_score: float, improved: bool
    ) -> None:
        if self._dim == 0:
            return
        if self._method == "nelder_mead":
            self._mean = best_vector
            self._simplex = self._build_simplex(best_vector)
            return
        self._best_score = max(self._best_score, hv_score)
        if improved:
            self._mean = best_vector
            self._sigma = max(self._sigma * 0.9, 1e-6)
        else:
            self._sigma = min(self._sigma * 1.1, 1.0)

    def rank_candidates(
        self, candidates: Sequence[Mapping[str, Any]]
    ) -> list[tuple[Mapping[str, Any], float]]:
        """Return proposals ordered by the HV proxy (gradient minus aspect)."""

        evaluation = tools.evaluate_p3_set(candidates)
        metrics_seq = evaluation.get("metrics_list", [])
        if not metrics_seq:
            return []
        if self._method in {"nsga2", "ngopt"}:
            # Phase 5 HV/Objective (docs/AI_SCIENTIST_UNIFIED_ROADMAP.md §5):
            # emulate NSGA-II style ordering with feasibility-first fronts.
            objectives = [
                (
                    float(m["minimum_normalized_magnetic_gradient_scale_length"]),
                    float(m["aspect_ratio"]),
                )
                for m in metrics_seq
            ]
            feasibilities = [float(f) for f in evaluation.get("feasibilities", [])]
            fronts = self._nondominated_fronts(objectives, feasibilities)
            distances = self._crowding_distance(fronts, objectives)
            ordering: list[int] = []
            for front in fronts:
                ordering.extend(
                    sorted(
                        front,
                        key=lambda idx: (
                            -distances[idx],
                            -self._score_metrics(metrics_seq[idx]),
                        ),
                    )
                )
            scored = [
                (candidates[idx], self._score_metrics(metrics_seq[idx]))
                for idx in ordering
            ]
        else:
            scored = sorted(
                [
                    (candidate, self._score_metrics(metrics))
                    for candidate, metrics in zip(candidates, metrics_seq)
                ],
                key=lambda item: item[1],
                reverse=True,
            )
        best_idx = max(range(len(scored)), key=lambda idx: scored[idx][1])
        best_vector = self._vectorizer.flatten(candidates[best_idx])
        improved = evaluation["hv_score"] >= self._best_score
        self._update_state(best_vector, float(evaluation["hv_score"]), improved)
        return scored


================================================================================
File: optim/samplers.py
================================================================================

"""Sampler implementations for Phase 1/2 candidate generation."""

from __future__ import annotations

import logging
from typing import Any, Mapping, Sequence

import numpy as np

from ai_scientist import config as ai_config
from ai_scientist import tools
from constellaration.initial_guess import generate_nae
from constellaration.geometry import surface_rz_fourier

_LOGGER = logging.getLogger(__name__)


def _surface_to_params(surface: surface_rz_fourier.SurfaceRZFourier) -> dict[str, Any]:
    return {
        "r_cos": np.asarray(surface.r_cos).tolist(),
        "z_sin": np.asarray(surface.z_sin).tolist(),
        "n_field_periods": int(surface.n_field_periods),
        "is_stellarator_symmetric": bool(surface.is_stellarator_symmetric),
    }


class NearAxisSampler:
    """Generates candidates using Near-Axis Expansion (NAE) via Constellaration."""

    def __init__(
        self,
        template: ai_config.BoundaryTemplateConfig,
    ) -> None:
        self._template = template

    def generate(self, seeds: Sequence[int]) -> list[Mapping[str, Any]]:
        candidates: list[Mapping[str, Any]] = []
        for seed in seeds:
            rng = np.random.default_rng(seed)
            
            # Sample NAE parameters around reasonable defaults or template values
            # We assume template provides n_field_periods and mode limits.
            
            # Aspect ratio: Sample around 4.0 to 8.0 (typical for stellarators)
            aspect_ratio = rng.uniform(4.0, 8.0)
            
            # Elongation: Sample around 1.5 to 2.5
            max_elongation = rng.uniform(1.5, 2.5)
            
            # Rotational transform (iota): Sample around 0.4 to 1.2
            rotational_transform = rng.uniform(0.4, 1.2)
            
            # Mirror ratio: Sample around 1.05 to 1.2
            mirror_ratio = rng.uniform(1.05, 1.2)

            try:
                surface = generate_nae(
                    aspect_ratio=aspect_ratio,
                    max_elongation=max_elongation,
                    rotational_transform=rotational_transform,
                    mirror_ratio=mirror_ratio,
                    n_field_periods=self._template.n_field_periods,
                    max_poloidal_mode=self._template.n_poloidal_modes,
                    max_toroidal_mode=self._template.n_toroidal_modes,
                )
                params = _surface_to_params(surface)
                candidates.append(
                    {
                        "seed": seed,
                        "params": params,
                        "source": "near_axis_sampler",
                        "design_hash": tools.design_hash(params),
                        "constraint_distance": 0.0, # NAE designs are "theoretically" near feasible
                    }
                )
            except Exception as exc:
                _LOGGER.warning(f"Near-axis generation failed for seed {seed}: {exc}")
                continue
        
        return candidates
